{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® 2 CLASS CLASSIFICATION MANUAL AND CLASS WEIGHTS STRATEGY‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® LOADING THE SPLIT DATA ARRAYS ‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">üìÅ reminder to change the folder path to your numpy array folder üìÅ</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\data\\numpy\\cwtnormalized\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">converting to 2 class</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(labels_fold_1)):\n",
    "    if (labels_fold_1[i] == 2):\n",
    "        labels_fold_1[i] = 1\n",
    "\n",
    "for i in range (len(labels_fold_2)):\n",
    "    if (labels_fold_2[i] == 2):\n",
    "        labels_fold_2[i] = 1\n",
    "\n",
    "for i in range (len(labels_fold_3)):\n",
    "    if (labels_fold_3[i] == 2):\n",
    "        labels_fold_3[i] = 1\n",
    "\n",
    "for i in range (len(labels_fold_4)):\n",
    "    if (labels_fold_4[i] == 2):\n",
    "        labels_fold_4[i] = 1\n",
    "\n",
    "for i in range (len(labels_fold_5)):\n",
    "    if (labels_fold_5[i] == 2):\n",
    "        labels_fold_5[i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">data balancer & early stopping</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Result plotting</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_metrics(labels, predictions, n_classes=2):\n",
    "    \"\"\"\n",
    "    Computes and visualizes classification metrics.\n",
    "\n",
    "    Args:\n",
    "        labels (array-like): True class labels.\n",
    "        predictions (array-like): Probabilities or class predictions.\n",
    "        n_classes (int): Number of classes (default: 2 for binary classification).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert probability predictions to class labels if needed\n",
    "    if predictions.ndim > 1:\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predicted_classes = predictions\n",
    "\n",
    "    # Compute Confusion Matrix\n",
    "    cm = confusion_matrix(labels, predicted_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise Accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    class_accuracies = []\n",
    "    for d in range(n_classes):\n",
    "        correct_preds = cm[d][d]\n",
    "        total_true_samples = sum(cm[d])\n",
    "        accuracy = correct_preds / total_true_samples if total_true_samples > 0 else 0\n",
    "        class_accuracies.append(accuracy)\n",
    "        print(f'Class {d}: {correct_preds}/{total_true_samples} ({accuracy:.2%})')\n",
    "\n",
    "    # Precision-Recall Curve & AUPRC\n",
    "    print(\"\\nPrecision-Recall Curve:\")\n",
    "    plt.figure()\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        # Binary classification\n",
    "        precision, recall, _ = precision_recall_curve(labels, predictions[:, 1])\n",
    "        auprc = auc(recall, precision)\n",
    "        plt.plot(recall, precision, label=f'AUPRC = {auprc:.2f}')\n",
    "        avg_auprc = auprc\n",
    "    else:\n",
    "        # Multi-class case\n",
    "        labels_binarized = label_binarize(labels, classes=np.arange(n_classes))\n",
    "        auprcs = []\n",
    "        for class_idx in range(n_classes):\n",
    "            precision, recall, _ = precision_recall_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "            auprc = auc(recall, precision)\n",
    "            auprcs.append(auprc)\n",
    "            plt.plot(recall, precision, label=f'Class {class_idx} (AUPRC = {auprc:.2f})')\n",
    "        avg_auprc = np.mean(auprcs)\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve & AUC\n",
    "    print(\"\\nROC Curve:\")\n",
    "    plt.figure()\n",
    "    \n",
    "    if n_classes == 2:\n",
    "        # Binary classification\n",
    "        fpr, tpr, _ = roc_curve(labels, predictions[:, 1])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "    else:\n",
    "        # Multi-class case\n",
    "        for class_idx in range(n_classes):\n",
    "            fpr, tpr, _ = roc_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f'Class {class_idx} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Compute Metrics\n",
    "    f1 = f1_score(labels, predicted_classes, average='macro')\n",
    "    balanced_acc = balanced_accuracy_score(labels, predicted_classes)\n",
    "\n",
    "    print(f\"\\nMean F1 Score: {f1:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"Average AUPRC: {avg_auprc:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predicted_classes))\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"class_wise_accuracy\": class_accuracies,\n",
    "        \"mean_f1_score\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">MODEL GOES HERE</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "debug_mode_flag = False\n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens ‚Üí 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Test with demo data & model Summary</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = TRANS_CNN(input_shape=(3,224,224),num_classes=2,num_transformer_blocks=4,embed_dim=128) # declare model here\n",
    "randomdata = torch.randn((1,3,224,224))\n",
    "output = model(randomdata)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "‚îú‚îÄCustomCnn: 1-1                                             --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-1                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                                      448\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2                                 32\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                                      4,640\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5                                 64\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-7                                   --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-2                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-8                                      18,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-9                                 128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-10                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-11                                     73,856\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-12                                256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-13                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-14                                  --\n",
       "‚îÇ    ‚îî‚îÄAdaptiveAvgPool2d: 2-3                                --\n",
       "‚îÇ    ‚îî‚îÄFlatten: 2-4                                          --\n",
       "‚îú‚îÄLinear: 1-2                                                65,664\n",
       "‚îú‚îÄModuleList: 1-3                                            --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-5                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-15                         66,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-16                                 16,576\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-17                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-18                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-19                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-20                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-6                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-21                         66,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-22                                 16,576\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-23                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-24                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-25                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-26                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-7                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-27                         66,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-28                                 16,576\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-29                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-30                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-31                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-32                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-8                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-33                         66,048\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-34                                 16,576\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-35                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-36                                  256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-37                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-38                                    --\n",
       "‚îú‚îÄAdaptiveAvgPool1d: 1-4                                     --\n",
       "‚îú‚îÄDropout: 1-5                                               --\n",
       "‚îú‚îÄLinear: 1-6                                                16,512\n",
       "‚îú‚îÄLinear: 1-7                                                16,512\n",
       "‚îú‚îÄLinear: 1-8                                                4,128\n",
       "‚îú‚îÄLinear: 1-9                                                66\n",
       "=====================================================================================\n",
       "Total params: 533,346\n",
       "Trainable params: 533,346\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Training Code</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________\n",
      "__________________________________________________\n",
      "__________________________________________________\n",
      "params set: 1\n",
      "__________________________________________________\n",
      "__________________________________________________\n",
      "__________________________________________________\n",
      "Running with params: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.782228330205127e-05, 'optimizer': 'Adam', 'weight_decay': 1.89223276202094e-06, 'batch_size': 10, 'label_smoothing': 0.17}\n",
      "__________________________________________________\n",
      "__________________________________________________\n",
      "__________________________________________________\n",
      "Fold No: 1\n",
      "Epoch [1/100], Loss: 0.6909, Accuracy: 0.5362\n",
      "Validation Loss: 0.7481, Validation Accuracy: 0.5889\n",
      "Epoch [2/100], Loss: 0.6535, Accuracy: 0.6749\n",
      "Validation Loss: 0.6654, Validation Accuracy: 0.6288\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "# Fold indices and other configurations\n",
    "num_folds = 5\n",
    "fold_indices = np.random.permutation(np.arange(num_folds))\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "test_folds_chosen = []\n",
    "val_folds_chosen = []\n",
    "fold_confusion_matrices = []  # To store confusion matrices for each fold\n",
    "fold_accuracies = []  # To store balanced accuracy for each fold\n",
    "fold_auprcs = []  # To store AUPRC for each fold\n",
    "\n",
    "# Model and training configurations\n",
    "num_classes = 2\n",
    "epochs = 100\n",
    "input_shape = (3,224,224)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "torch.random.manual_seed(42)  # For reproducibility\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "all_params = [\n",
    "    {\n",
    "        \"num_heads\": 2,\n",
    "        \"num_transformer_blocks\": 2,\n",
    "        \"learning_rate\": 2.782228330205127e-05,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"weight_decay\": 1.89223276202094e-06,\n",
    "        \"batch_size\": 10,\n",
    "        \"label_smoothing\": 0.17\n",
    "    },  # Trial 86, Accuracy = 0.5554 \n",
    "    {\n",
    "        \"num_heads\": 2,\n",
    "        \"num_transformer_blocks\": 4,\n",
    "        \"learning_rate\": 1.2292958406478385e-05,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"weight_decay\": 5.221031733062504e-06,\n",
    "        \"batch_size\": 32,\n",
    "        \"label_smoothing\": 0.16\n",
    "    },  # Trial 69, Accuracy = 0.5460\n",
    "    {\n",
    "        \"num_heads\": 32,\n",
    "        \"num_transformer_blocks\": 8,\n",
    "        \"learning_rate\": 1.4338089312657102e-05,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"weight_decay\": 0.0006154361290634533,\n",
    "        \"batch_size\": 32,\n",
    "        \"label_smoothing\": 0.17\n",
    "    },  # Trial 48, Accuracy = 0.5408\n",
    "    {\n",
    "        \"num_heads\": 16,\n",
    "        \"num_transformer_blocks\": 2,\n",
    "        \"learning_rate\": 0.00012771315175564576,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"weight_decay\": 1.3852738619108427e-06,\n",
    "        \"batch_size\": 10,\n",
    "        \"label_smoothing\": 0.14\n",
    "    },  # Trial 133, Accuracy = 0.5405\n",
    "    {\n",
    "        \"num_heads\": 32,\n",
    "        \"num_transformer_blocks\": 8,\n",
    "        \"learning_rate\": 2.5198448093373934e-05,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"weight_decay\": 0.0004641556896534395,\n",
    "        \"batch_size\": 32,\n",
    "        \"label_smoothing\": 0.16\n",
    "    }  # Trial 53, Accuracy = 0.5403\n",
    "]\n",
    "\n",
    "p_indx = 1\n",
    "\n",
    "for params in all_params:\n",
    "    \n",
    "    print(\"_\"*50)\n",
    "    print(\"_\"*50)\n",
    "    print(\"_\"*50)\n",
    "    print(\"params set:\",p_indx)\n",
    "    print(\"_\"*50)\n",
    "    print(\"_\"*50)\n",
    "    print(\"_\"*50)\n",
    "    \n",
    "    num_heads = params[\"num_heads\"]\n",
    "    num_transformer_blocks = params[\"num_transformer_blocks\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    optimizer_name = params[\"optimizer\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    label_smoothing = params[\"label_smoothing\"]\n",
    "\n",
    "    print(f\"Running with params: {params}\")\n",
    "    \n",
    "    def save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_info, csv_path=r\"G:\\CODING\\py\\newnotebooks\\results.csv\"):\n",
    "        \"\"\"\n",
    "        Save all results from the current experiment to a CSV file\n",
    "        \n",
    "        Parameters:\n",
    "        - fold_accuracies: list of balanced accuracy scores for each fold\n",
    "        - fold_auprcs: list of AUPRC scores for each fold\n",
    "        - fold_confusion_matrices: list of confusion matrices for each fold\n",
    "        - model_info: string with model architecture description\n",
    "        - csv_path: path to the CSV file to save results\n",
    "        \"\"\"\n",
    "        # Current time for experiment identification\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Prepare data for the DataFrame\n",
    "        data = {\n",
    "            \"timestamp\": timestamp,\n",
    "            \"model_info\": model_info,\n",
    "            \"num_classes\": num_classes,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"epochs\": epochs,\n",
    "            \"mean_balanced_accuracy\": np.mean(fold_accuracies),\n",
    "            \"std_balanced_accuracy\": np.std(fold_accuracies),\n",
    "            \"mean_auprc\": np.mean(fold_auprcs),\n",
    "            \"std_auprc\": np.std(fold_auprcs),\n",
    "        }\n",
    "        \n",
    "        # Add individual fold results\n",
    "        for i, (acc, auprc) in enumerate(zip(fold_accuracies, fold_auprcs)):\n",
    "            data[f\"fold_{i+1}_accuracy\"] = acc\n",
    "            data[f\"fold_{i+1}_auprc\"] = auprc\n",
    "        \n",
    "        # Add confusion matrix info\n",
    "        for i, cm in enumerate(fold_confusion_matrices):\n",
    "            data[f\"fold_{i+1}_confusion_matrix\"] = str(cm)\n",
    "        \n",
    "        # Create DataFrame and append to CSV\n",
    "        df = pd.DataFrame([data])\n",
    "        \n",
    "        # Check if file exists\n",
    "        file_exists = os.path.isfile(csv_path)\n",
    "        \n",
    "        # Save to CSV\n",
    "        if file_exists:\n",
    "            df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "        print(f\"Results saved to {csv_path}\") \n",
    "\n",
    "    # Training loop for cross-validation\n",
    "    for fold_idx in range(num_folds):\n",
    "        print(\"_\"*50)\n",
    "        print(\"_\"*50)\n",
    "        print(\"_\"*50)\n",
    "        \n",
    "        print(f'Fold No: {fold_idx + 1}')\n",
    "        \n",
    "        # Initialize model, loss, and optimizer\n",
    "        fold_model = TRANS_CNN(input_shape=input_shape,num_classes=num_classes,embed_dim=128,num_heads=num_heads,num_transformer_blocks=num_transformer_blocks) # Initialize your model here\n",
    "        fold_model.to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "        \n",
    "        optimizer = optim.Adam(fold_model.parameters(), lr=learning_rate,weight_decay=weight_decay)\n",
    "        \n",
    "        # Split data into train, validation, and test sets\n",
    "        test_fold = fold_indices[fold_idx]\n",
    "        val_fold = val_fold_indices[fold_idx]\n",
    "        train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "        train_data = train_data.transpose(0, 3, 1, 2)  # Transpose to match PyTorch input format\n",
    "\n",
    "        test_folds_chosen.append(test_fold)\n",
    "        val_folds_chosen.append(val_fold)\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=1)\n",
    "\n",
    "            train_dataset = TensorDataset(\n",
    "                torch.tensor(balanced_train_data, dtype=torch.float32).to(device),\n",
    "                torch.tensor(balanced_train_labels, dtype=torch.long).to(device)\n",
    "            )\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "            fold_model.train()\n",
    "            running_loss = 0.0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            \n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = fold_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "            # Validation loop\n",
    "            val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "            val_labels = labels_folds[val_fold]\n",
    "            val_dataset = TensorDataset(\n",
    "                torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "                torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "            )\n",
    "            val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "            fold_model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_preds = []\n",
    "            val_labels_list = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_outputs = fold_model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                    val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                    val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "            val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "            print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "            early_stopping(val_loss, fold_model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        # Load the best model\n",
    "        early_stopping.load_best_model(fold_model)\n",
    "\n",
    "        # Test loop\n",
    "        test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "        test_labels = labels_folds[test_fold]\n",
    "        test_dataset = TensorDataset(\n",
    "            torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        test_probs = []\n",
    "        test_preds = []\n",
    "        test_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for test_inputs, test_labels in test_loader:\n",
    "                test_outputs = fold_model(test_inputs)\n",
    "                probabilities = torch.softmax(test_outputs, dim=1)\n",
    "                test_probs.extend(probabilities.cpu().numpy())\n",
    "                _, preds = torch.max(probabilities, 1)\n",
    "                test_preds.extend(preds.cpu().numpy())\n",
    "                test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "        # Compute metrics\n",
    "        test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "        fold_accuracies.append(test_acc)\n",
    "\n",
    "        cm = confusion_matrix(test_labels_list, test_preds)\n",
    "        fold_confusion_matrices.append(cm)\n",
    "\n",
    "        test_labels_binarized = test_labels_list\n",
    "        test_auprcs = []\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_list, np.array(test_probs)[:, 1])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "        mean_test_auprc = np.mean(test_auprcs)\n",
    "        fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "        print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "        # Use the plot_metrics function to visualize metrics\n",
    "        plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "    # Final metrics across all folds\n",
    "    average_auprc = np.mean(fold_auprcs)\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "    print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "    print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "    print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "        \n",
    "    # Save all results to CSV\n",
    "    model_description = f\"TRANS_CNN {p_indx} MANUAL APPROACH 2 CLASS\"\n",
    "    save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_description,csv_path=r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\newnotebooks\\results.csv\")\n",
    "    p_indx += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
