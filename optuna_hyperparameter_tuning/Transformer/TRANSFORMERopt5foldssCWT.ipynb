{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\data\\numpy\\optunadata\\cwt\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=64, num_heads=2, ff_dim=64, num_transformer_blocks=2,dropout_rate=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.reshape = nn.Flatten(start_dim=2)  # Reshaping as in TensorFlow's Reshape\n",
    "        self.embedding = nn.Linear(input_shape[1] * input_shape[2], embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(embed_dim, num_heads, ff_dim,dropout_rate=dropout_rate) for _ in range(num_transformer_blocks)]\n",
    "        )\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.reshape(x)\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # PyTorch uses (seq_len, batch, embed_dim) format for transformers\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)\n",
    "        x = x.permute(1, 2, 0)  # Back to (batch, embed_dim, seq_len)\n",
    "        x = self.global_avg_pool(x).squeeze(-1)\n",
    "        x = self.dropout(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = TransformerModel(input_shape=(224,224,3),num_classes=3) # declare model here\n",
    "randomdata = torch.randn((1,224,224,3))\n",
    "output = model(randomdata)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (3,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-09 09:58:56,937] A new study created in memory with name: no-name-b6ceb15b-e5dc-483f-bf4d-77d06a5910a5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=0.00014957638446925025, optimizer=Adam, weight_decay=0.00025478724688789584, batch_size=16,factor=1\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=2.5298034370526554e-06, optimizer=Adam, weight_decay=6.459712811892587e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 1, Fold 1: Test Accuracy = 0.5025\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 0, Fold 1: Test Accuracy = 0.4458\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 1, Fold 2: Test Accuracy = 0.3847\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 0, Fold 2: Test Accuracy = 0.5008\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 1, Fold 3: Test Accuracy = 0.4453\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 0, Fold 3: Test Accuracy = 0.3948\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 1, Fold 4: Test Accuracy = 0.4889\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 0, Fold 4: Test Accuracy = 0.4958\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:07:26,293] Trial 1 finished with value: 0.4535005421391043 and parameters: {'ff_dim': 32, 'dropout_rate': 0.26356084463848456, 'embed_dim': 2048, 'learning_rate': 2.5298034370526554e-06, 'optimizer': 'Adam', 'weight_decay': 6.459712811892587e-05, 'batch_size': 32, 'num_heads': 8, 'num_transformer_blocks': 8}. Best is trial 1 with value: 0.4535005421391043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 5: Test Accuracy = 0.4460\n",
      "Trial 1: Mean Accuracy = 0.4535, Fold Accuracies = [np.float64(0.5025108225108225), np.float64(0.38473674555127496), np.float64(0.44534530382698817), np.float64(0.4888715659475424), np.float64(0.4460382728588936)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=2, learning_rate=1.6494616695368336e-06, optimizer=Adam, weight_decay=7.963158112269603e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:07:41,272] Trial 0 finished with value: 0.46277505180999323 and parameters: {'ff_dim': 2048, 'dropout_rate': 0.48264534964809236, 'embed_dim': 32, 'learning_rate': 0.00014957638446925025, 'optimizer': 'Adam', 'weight_decay': 0.00025478724688789584, 'batch_size': 16, 'num_heads': 2, 'num_transformer_blocks': 4}. Best is trial 0 with value: 0.46277505180999323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 5: Test Accuracy = 0.4766\n",
      "Trial 0: Mean Accuracy = 0.4628, Fold Accuracies = [np.float64(0.4458441558441559), np.float64(0.500773558368495), np.float64(0.39479110888730884), np.float64(0.4958274127037739), np.float64(0.4766390232462328)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=8, learning_rate=0.008181184577648146, optimizer=AdamW, weight_decay=0.0002713712347068007, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 2, Fold 1: Test Accuracy = 0.4726\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 3, Fold 1: Test Accuracy = 0.3333\n",
      "Trial 2, Fold 2: Test Accuracy = 0.4368\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 3, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 3, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 2, Fold 3: Test Accuracy = 0.4440\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 2, Fold 4: Test Accuracy = 0.5698\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 3, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:18:15,071] Trial 3 finished with value: 0.3333333333333333 and parameters: {'ff_dim': 64, 'dropout_rate': 0.1576328981576447, 'embed_dim': 1024, 'learning_rate': 0.008181184577648146, 'optimizer': 'AdamW', 'weight_decay': 0.0002713712347068007, 'batch_size': 10, 'num_heads': 1, 'num_transformer_blocks': 8}. Best is trial 0 with value: 0.46277505180999323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 3: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=6.21695516850386e-07, optimizer=Adam, weight_decay=0.00019274008015460436, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:18:36,801] Trial 2 finished with value: 0.4790788580852764 and parameters: {'ff_dim': 32, 'dropout_rate': 0.3084362790543471, 'embed_dim': 1024, 'learning_rate': 1.6494616695368336e-06, 'optimizer': 'Adam', 'weight_decay': 7.963158112269603e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Fold 5: Test Accuracy = 0.4722\n",
      "Trial 2: Mean Accuracy = 0.4791, Fold Accuracies = [np.float64(0.4726406926406927), np.float64(0.4367577814468293), np.float64(0.4439802988673238), np.float64(0.5698181529312188), np.float64(0.47219736454031747)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=8, learning_rate=0.0005038557175037136, optimizer=AdamW, weight_decay=0.00048332242665265424, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 4, Fold 1: Test Accuracy = 0.4816\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 5, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 5, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 4, Fold 2: Test Accuracy = 0.4050\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 5, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 4, Fold 3: Test Accuracy = 0.4122\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 5, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 5, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 5: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:24:38,828] Trial 5 finished with value: 0.3333333333333333 and parameters: {'ff_dim': 1024, 'dropout_rate': 0.318036894094731, 'embed_dim': 128, 'learning_rate': 0.0005038557175037136, 'optimizer': 'AdamW', 'weight_decay': 0.00048332242665265424, 'batch_size': 32, 'num_heads': 1, 'num_transformer_blocks': 8}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=16, learning_rate=0.000261747437563711, optimizer=AdamW, weight_decay=0.0005767542423787947, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 4, Fold 4: Test Accuracy = 0.5441\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 6, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:26:57,437] Trial 4 finished with value: 0.4598989784129907 and parameters: {'ff_dim': 1024, 'dropout_rate': 0.17178480536415433, 'embed_dim': 512, 'learning_rate': 6.21695516850386e-07, 'optimizer': 'Adam', 'weight_decay': 0.00019274008015460436, 'batch_size': 10, 'num_heads': 2, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 5: Test Accuracy = 0.4565\n",
      "Trial 4: Mean Accuracy = 0.4599, Fold Accuracies = [np.float64(0.48164502164502165), np.float64(0.4049685073075277), np.float64(0.41218943576868944), np.float64(0.5441475604720196), np.float64(0.4565443668716953)]\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=0.0002726124111685776, optimizer=AdamW, weight_decay=0.00017221689262781232, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 6, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 7, Fold 1: Test Accuracy = 0.4143\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 6, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 7, Fold 2: Test Accuracy = 0.4558\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 6, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 7, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:30:26,519] Trial 6 finished with value: 0.3333333333333333 and parameters: {'ff_dim': 256, 'dropout_rate': 0.3443305692063453, 'embed_dim': 128, 'learning_rate': 0.000261747437563711, 'optimizer': 'AdamW', 'weight_decay': 0.0005767542423787947, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 16}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 6: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=2.078646893834765e-05, optimizer=AdamW, weight_decay=0.00038616137444888866, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 8, Fold 1: Test Accuracy = 0.5228\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 7, Fold 4: Test Accuracy = 0.4301\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 8, Fold 2: Test Accuracy = 0.4013\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:33:31,662] Trial 7 finished with value: 0.4140562085291487 and parameters: {'ff_dim': 32, 'dropout_rate': 0.1877489990471375, 'embed_dim': 256, 'learning_rate': 0.0002726124111685776, 'optimizer': 'AdamW', 'weight_decay': 0.00017221689262781232, 'batch_size': 10, 'num_heads': 16, 'num_transformer_blocks': 4}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 5: Test Accuracy = 0.4367\n",
      "Trial 7: Mean Accuracy = 0.4141, Fold Accuracies = [np.float64(0.4143290043290044), np.float64(0.4558093316211093), np.float64(0.3333333333333333), np.float64(0.4301133935102343), np.float64(0.43669597985206215)]\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=1, learning_rate=1.0012843231898847e-06, optimizer=Adam, weight_decay=0.0007013399484079567, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 8, Fold 3: Test Accuracy = 0.4521\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 9, Fold 1: Test Accuracy = 0.4492\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 8, Fold 4: Test Accuracy = 0.4770\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 9, Fold 2: Test Accuracy = 0.4146\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:37:37,046] Trial 8 finished with value: 0.46119027151107117 and parameters: {'ff_dim': 256, 'dropout_rate': 0.19087439544191687, 'embed_dim': 1024, 'learning_rate': 2.078646893834765e-05, 'optimizer': 'AdamW', 'weight_decay': 0.00038616137444888866, 'batch_size': 32, 'num_heads': 2, 'num_transformer_blocks': 4}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 5: Test Accuracy = 0.4527\n",
      "Trial 8: Mean Accuracy = 0.4612, Fold Accuracies = [np.float64(0.5228138528138527), np.float64(0.4013055708432704), np.float64(0.45210741842888363), np.float64(0.4770411654173892), np.float64(0.45268335005196)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=2, learning_rate=5.053460592896154e-05, optimizer=Adam, weight_decay=4.19549040097764e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 10, Fold 1: Test Accuracy = 0.4892\n",
      "Trial 9, Fold 3: Test Accuracy = 0.4411\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 10, Fold 2: Test Accuracy = 0.4215\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 9, Fold 4: Test Accuracy = 0.5668\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 10, Fold 3: Test Accuracy = 0.4387\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:42:31,092] Trial 9 finished with value: 0.4672441208724365 and parameters: {'ff_dim': 128, 'dropout_rate': 0.4758574795088818, 'embed_dim': 512, 'learning_rate': 1.0012843231898847e-06, 'optimizer': 'Adam', 'weight_decay': 0.0007013399484079567, 'batch_size': 10, 'num_heads': 2, 'num_transformer_blocks': 1}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Fold 5: Test Accuracy = 0.4644\n",
      "Trial 9: Mean Accuracy = 0.4672, Fold Accuracies = [np.float64(0.4491774891774892), np.float64(0.41464868831407076), np.float64(0.4411471454864129), np.float64(0.5668025768256256), np.float64(0.4644447045585842)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=2, learning_rate=1.1012962877549561e-07, optimizer=Adam, weight_decay=3.1941828635411935e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 10, Fold 4: Test Accuracy = 0.5512\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 11, Fold 1: Test Accuracy = 0.3841\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:43:35,503] Trial 10 finished with value: 0.4680196465796394 and parameters: {'ff_dim': 512, 'dropout_rate': 0.2644773455231292, 'embed_dim': 64, 'learning_rate': 5.053460592896154e-05, 'optimizer': 'Adam', 'weight_decay': 4.19549040097764e-05, 'batch_size': 32, 'num_heads': 1, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10, Fold 5: Test Accuracy = 0.4396\n",
      "Trial 10: Mean Accuracy = 0.4680, Fold Accuracies = [np.float64(0.4891774891774892), np.float64(0.42145783648260265), np.float64(0.438655243345418), np.float64(0.5511845608904246), np.float64(0.43962310300226254)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=2, learning_rate=1.3501085446174475e-07, optimizer=Adam, weight_decay=5.051918032771868e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 11, Fold 2: Test Accuracy = 0.3373\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 1: Test Accuracy = 0.4008\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 11, Fold 3: Test Accuracy = 0.2935\n",
      "Trial 12, Fold 2: Test Accuracy = 0.4278\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 11, Fold 4: Test Accuracy = 0.3201\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 3: Test Accuracy = 0.3198\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 12, Fold 4: Test Accuracy = 0.3747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:48:30,498] Trial 11 finished with value: 0.33589274914683465 and parameters: {'ff_dim': 512, 'dropout_rate': 0.39565860943132314, 'embed_dim': 64, 'learning_rate': 1.1012962877549561e-07, 'optimizer': 'Adam', 'weight_decay': 3.1941828635411935e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11, Fold 5: Test Accuracy = 0.3444\n",
      "Trial 11: Mean Accuracy = 0.3359, Fold Accuracies = [np.float64(0.3841125541125541), np.float64(0.33734177215189876), np.float64(0.2935415368229473), np.float64(0.32008934776903314), np.float64(0.34437853487774)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=32, learning_rate=1.2729669410645611e-05, optimizer=Adam, weight_decay=9.087140908090127e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:49:14,105] Trial 12 finished with value: 0.3810049486497511 and parameters: {'ff_dim': 512, 'dropout_rate': 0.385912351177575, 'embed_dim': 64, 'learning_rate': 1.3501085446174475e-07, 'optimizer': 'Adam', 'weight_decay': 5.051918032771868e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12, Fold 5: Test Accuracy = 0.3820\n",
      "Trial 12: Mean Accuracy = 0.3810, Fold Accuracies = [np.float64(0.40077922077922074), np.float64(0.42777166269186084), np.float64(0.31975460634310343), np.float64(0.3747181450127042), np.float64(0.38200110842186646)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=32, learning_rate=1.5843128923788224e-05, optimizer=Adam, weight_decay=1.3583942375529081e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 14, Fold 1: Test Accuracy = 0.4167\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 13, Fold 1: Test Accuracy = 0.4679\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 14, Fold 2: Test Accuracy = 0.4209\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 14, Fold 3: Test Accuracy = 0.4673\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 13, Fold 2: Test Accuracy = 0.4410\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 14, Fold 4: Test Accuracy = 0.4977\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 13, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 13, Fold 4: Test Accuracy = 0.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:56:26,677] Trial 14 finished with value: 0.46516144179555907 and parameters: {'ff_dim': 512, 'dropout_rate': 0.25282405514490625, 'embed_dim': 64, 'learning_rate': 1.5843128923788224e-05, 'optimizer': 'Adam', 'weight_decay': 1.3583942375529081e-05, 'batch_size': 32, 'num_heads': 1, 'num_transformer_blocks': 32}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14, Fold 5: Test Accuracy = 0.5233\n",
      "Trial 14: Mean Accuracy = 0.4652, Fold Accuracies = [np.float64(0.4166666666666667), np.float64(0.42088913349232554), np.float64(0.4673057172201301), np.float64(0.4976933754290968), np.float64(0.523252316169576)]\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.0402392651089812e-05, optimizer=Adam, weight_decay=2.290251058153077e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 15, Fold 1: Test Accuracy = 0.4821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 10:58:41,737] Trial 13 finished with value: 0.4251594760628711 and parameters: {'ff_dim': 512, 'dropout_rate': 0.25736604321647205, 'embed_dim': 64, 'learning_rate': 1.2729669410645611e-05, 'optimizer': 'Adam', 'weight_decay': 9.087140908090127e-06, 'batch_size': 32, 'num_heads': 4, 'num_transformer_blocks': 32}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13, Fold 5: Test Accuracy = 0.4802\n",
      "Trial 13: Mean Accuracy = 0.4252, Fold Accuracies = [np.float64(0.46787878787878784), np.float64(0.4410383415886993), np.float64(0.3333333333333333), np.float64(0.4033447500914884), np.float64(0.48020216742204647)]\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.004704006366373762, optimizer=Adam, weight_decay=1.3819737793145035e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 15, Fold 2: Test Accuracy = 0.4214\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 16, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 16, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 15, Fold 3: Test Accuracy = 0.4260\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 16, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 16, Fold 4: Test Accuracy = 0.3333\n",
      "Trial 15, Fold 4: Test Accuracy = 0.4313\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:04:49,516] Trial 16 finished with value: 0.3333333333333333 and parameters: {'ff_dim': 32, 'dropout_rate': 0.3662518157820155, 'embed_dim': 1024, 'learning_rate': 0.004704006366373762, 'optimizer': 'Adam', 'weight_decay': 1.3819737793145035e-06, 'batch_size': 16, 'num_heads': 16, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 16: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.049617384989039774, optimizer=Adam, weight_decay=2.9941601771210973e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 17, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:06:09,032] Trial 15 finished with value: 0.45160004085513217 and parameters: {'ff_dim': 32, 'dropout_rate': 0.2518826648859174, 'embed_dim': 1024, 'learning_rate': 1.0402392651089812e-05, 'optimizer': 'Adam', 'weight_decay': 2.290251058153077e-05, 'batch_size': 32, 'num_heads': 16, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15, Fold 5: Test Accuracy = 0.4972\n",
      "Trial 15: Mean Accuracy = 0.4516, Fold Accuracies = [np.float64(0.4821212121212121), np.float64(0.421396685623433), np.float64(0.42597444968208403), np.float64(0.43130555223765343), np.float64(0.4972023046112783)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=2.5821821760623193e-06, optimizer=Adam, weight_decay=6.253454290522161e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 17, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 18, Fold 1: Test Accuracy = 0.4813\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 17, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 17, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 18, Fold 2: Test Accuracy = 0.4536\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:09:18,021] Trial 17 finished with value: 0.3333333333333333 and parameters: {'ff_dim': 64, 'dropout_rate': 0.10410635927777931, 'embed_dim': 256, 'learning_rate': 0.049617384989039774, 'optimizer': 'Adam', 'weight_decay': 2.9941601771210973e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 2}. Best is trial 2 with value: 0.4790788580852764.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 17: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=16, learning_rate=5.264168273349705e-05, optimizer=Adam, weight_decay=8.697275800811252e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 19, Fold 1: Test Accuracy = 0.3364\n",
      "Trial 18, Fold 3: Test Accuracy = 0.4400\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 19, Fold 2: Test Accuracy = 0.4761\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 18, Fold 4: Test Accuracy = 0.5843\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 19, Fold 3: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:14:42,586] Trial 18 finished with value: 0.4904910822876266 and parameters: {'ff_dim': 128, 'dropout_rate': 0.1223412597644731, 'embed_dim': 256, 'learning_rate': 2.5821821760623193e-06, 'optimizer': 'Adam', 'weight_decay': 6.253454290522161e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18, Fold 5: Test Accuracy = 0.4933\n",
      "Trial 18: Mean Accuracy = 0.4905, Fold Accuracies = [np.float64(0.48125541125541127), np.float64(0.4536231884057971), np.float64(0.4399895247207812), np.float64(0.5842506141928866), np.float64(0.4933366728632567)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=2.1792141362193585e-06, optimizer=Adam, weight_decay=9.89985337818974e-05, batch_size=16,factor=1\n",
      "Trial 19, Fold 4: Test Accuracy = 0.5184\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 20, Fold 1: Test Accuracy = 0.4511\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:16:01,639] Trial 19 finished with value: 0.4192748892716941 and parameters: {'ff_dim': 2048, 'dropout_rate': 0.3006166734252251, 'embed_dim': 32, 'learning_rate': 5.264168273349705e-05, 'optimizer': 'Adam', 'weight_decay': 8.697275800811252e-05, 'batch_size': 32, 'num_heads': 1, 'num_transformer_blocks': 16}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19, Fold 5: Test Accuracy = 0.4321\n",
      "Trial 19: Mean Accuracy = 0.4193, Fold Accuracies = [np.float64(0.33640692640692643), np.float64(0.47611141686540703), np.float64(0.3333333333333333), np.float64(0.5184362641806975), np.float64(0.43208650557210637)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=3.372141513375443e-06, optimizer=Adam, weight_decay=1.152522486061992e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 20, Fold 2: Test Accuracy = 0.4551\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 21, Fold 1: Test Accuracy = 0.4416\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 21, Fold 2: Test Accuracy = 0.4537\n",
      "Trial 20, Fold 3: Test Accuracy = 0.4308\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 21, Fold 3: Test Accuracy = 0.4351\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 20, Fold 4: Test Accuracy = 0.5246\n",
      "Trial 21, Fold 4: Test Accuracy = 0.5498\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:22:52,770] Trial 20 finished with value: 0.4676155184503437 and parameters: {'ff_dim': 128, 'dropout_rate': 0.4280678515488287, 'embed_dim': 256, 'learning_rate': 2.1792141362193585e-06, 'optimizer': 'Adam', 'weight_decay': 9.89985337818974e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20, Fold 5: Test Accuracy = 0.4765\n",
      "Trial 20: Mean Accuracy = 0.4676, Fold Accuracies = [np.float64(0.4511255411255411), np.float64(0.4550938665688253), np.float64(0.4307578191522046), np.float64(0.5245993704685633), np.float64(0.47650099493658454)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=3.762077255854489e-06, optimizer=Adam, weight_decay=4.697875170756365e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 22, Fold 1: Test Accuracy = 0.5245\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:24:34,215] Trial 21 finished with value: 0.4724319063880692 and parameters: {'ff_dim': 128, 'dropout_rate': 0.10119271787101194, 'embed_dim': 256, 'learning_rate': 3.372141513375443e-06, 'optimizer': 'Adam', 'weight_decay': 1.152522486061992e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21, Fold 5: Test Accuracy = 0.4820\n",
      "Trial 21: Mean Accuracy = 0.4724, Fold Accuracies = [np.float64(0.44155844155844154), np.float64(0.4536598789212989), np.float64(0.4351319065885995), np.float64(0.5498396199163786), np.float64(0.4819696849556275)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=4.540174557803003e-07, optimizer=Adam, weight_decay=1.2484761205052558e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 22, Fold 2: Test Accuracy = 0.3662\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 23, Fold 1: Test Accuracy = 0.4664\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 22, Fold 3: Test Accuracy = 0.4617\n",
      "Trial 23, Fold 2: Test Accuracy = 0.3940\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 22, Fold 4: Test Accuracy = 0.5267\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:31:33,290] Trial 22 finished with value: 0.47670782286972396 and parameters: {'ff_dim': 128, 'dropout_rate': 0.10589302667414588, 'embed_dim': 2048, 'learning_rate': 3.762077255854489e-06, 'optimizer': 'Adam', 'weight_decay': 4.697875170756365e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22, Fold 5: Test Accuracy = 0.5044\n",
      "Trial 22: Mean Accuracy = 0.4767, Fold Accuracies = [np.float64(0.5245021645021645), np.float64(0.36622638048064576), np.float64(0.4617209742594885), np.float64(0.5266896805244482), np.float64(0.5043999145818728)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=3.4574917812589834e-07, optimizer=Adam, weight_decay=1.360877349977649e-05, batch_size=16,factor=1\n",
      "Trial 23, Fold 3: Test Accuracy = 0.4048\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 24, Fold 1: Test Accuracy = 0.4822\n",
      "Trial 23, Fold 4: Test Accuracy = 0.5008\n",
      "Trial 24, Fold 2: Test Accuracy = 0.4703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:37:35,419] Trial 23 finished with value: 0.44899479419186294 and parameters: {'ff_dim': 128, 'dropout_rate': 0.10092988496406471, 'embed_dim': 256, 'learning_rate': 4.540174557803003e-07, 'optimizer': 'Adam', 'weight_decay': 1.2484761205052558e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23, Fold 5: Test Accuracy = 0.4790\n",
      "Trial 23: Mean Accuracy = 0.4490, Fold Accuracies = [np.float64(0.4663636363636363), np.float64(0.39397664037179725), np.float64(0.4048424757942047), np.float64(0.5007809611222522), np.float64(0.47901025730742425)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=5.225017888974928e-06, optimizer=Adam, weight_decay=1.344520581061046e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 25, Fold 1: Test Accuracy = 0.4735\n",
      "Trial 24, Fold 3: Test Accuracy = 0.4087\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 25, Fold 2: Test Accuracy = 0.3573\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 25, Fold 3: Test Accuracy = 0.4395\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 24, Fold 4: Test Accuracy = 0.5382\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 25, Fold 4: Test Accuracy = 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:45:54,430] Trial 24 finished with value: 0.48310536183137637 and parameters: {'ff_dim': 128, 'dropout_rate': 0.14350736709418738, 'embed_dim': 2048, 'learning_rate': 3.4574917812589834e-07, 'optimizer': 'Adam', 'weight_decay': 1.360877349977649e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24, Fold 5: Test Accuracy = 0.5162\n",
      "Trial 24: Mean Accuracy = 0.4831, Fold Accuracies = [np.float64(0.48220779220779225), np.float64(0.47026233718583743), np.float64(0.40865805235883973), np.float64(0.5382306456929535), np.float64(0.5161679817114587)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=2.1086500827838022e-07, optimizer=Adam, weight_decay=1.6643539326686226e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:47:42,100] Trial 25 finished with value: 0.4438296093299153 and parameters: {'ff_dim': 128, 'dropout_rate': 0.1437650104284779, 'embed_dim': 2048, 'learning_rate': 5.225017888974928e-06, 'optimizer': 'Adam', 'weight_decay': 1.344520581061046e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25, Fold 5: Test Accuracy = 0.4623\n",
      "Trial 25: Mean Accuracy = 0.4438, Fold Accuracies = [np.float64(0.4735064935064935), np.float64(0.35732587292851464), np.float64(0.43949319253153557), np.float64(0.48657113267715707), np.float64(0.4622513550058756)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=1, learning_rate=3.1440113618731716e-07, optimizer=AdamW, weight_decay=5.208859919798618e-06, batch_size=16,factor=1\n",
      "Trial 26, Fold 1: Test Accuracy = 0.4821\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 27, Fold 1: Test Accuracy = 0.4832\n",
      "Trial 26, Fold 2: Test Accuracy = 0.4025\n",
      "Trial 27, Fold 2: Test Accuracy = 0.4540\n",
      "Trial 26, Fold 3: Test Accuracy = 0.4445\n",
      "Trial 27, Fold 3: Test Accuracy = 0.4005\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 27, Fold 4: Test Accuracy = 0.4001\n",
      "Trial 26, Fold 4: Test Accuracy = 0.5010\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 11:59:39,990] Trial 27 finished with value: 0.4401310756733947 and parameters: {'ff_dim': 128, 'dropout_rate': 0.21532851174778705, 'embed_dim': 2048, 'learning_rate': 3.1440113618731716e-07, 'optimizer': 'AdamW', 'weight_decay': 5.208859919798618e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27, Fold 5: Test Accuracy = 0.4630\n",
      "Trial 27: Mean Accuracy = 0.4401, Fold Accuracies = [np.float64(0.4831601731601731), np.float64(0.45397174830306364), np.float64(0.4004833843930044), np.float64(0.4000885070221731), np.float64(0.4629515654885594)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=1.0796649749079528e-06, optimizer=Adam, weight_decay=1.7975839086129747e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 28, Fold 1: Test Accuracy = 0.5403\n",
      "Trial 26, Fold 5: Test Accuracy = 0.4910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:02:25,617] Trial 26 finished with value: 0.4642115987336375 and parameters: {'ff_dim': 128, 'dropout_rate': 0.2057416981284149, 'embed_dim': 2048, 'learning_rate': 2.1086500827838022e-07, 'optimizer': 'Adam', 'weight_decay': 1.6643539326686226e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26: Mean Accuracy = 0.4642, Fold Accuracies = [np.float64(0.4820779220779221), np.float64(0.4024857824252431), np.float64(0.44451174372434216), np.float64(0.5009806067512608), np.float64(0.49100193868941916)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=32, learning_rate=1.1530312363994997e-06, optimizer=Adam, weight_decay=2.88760585811824e-06, batch_size=16,factor=1\n",
      "Trial 28, Fold 2: Test Accuracy = 0.4433\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 29, Fold 1: Test Accuracy = 0.4963\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 28, Fold 3: Test Accuracy = 0.4140\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 29, Fold 2: Test Accuracy = 0.4139\n",
      "Trial 28, Fold 4: Test Accuracy = 0.5272\n",
      "Trial 28, Fold 5: Test Accuracy = 0.5068\n",
      "Trial 28: Mean Accuracy = 0.4863, Fold Accuracies = [np.float64(0.5402597402597403), np.float64(0.44325200269063786), np.float64(0.4140423605076122), np.float64(0.5272370549907502), np.float64(0.5067635969422324)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:13:53,940] Trial 28 finished with value: 0.4863109510781946 and parameters: {'ff_dim': 32, 'dropout_rate': 0.14076357351667868, 'embed_dim': 2048, 'learning_rate': 1.0796649749079528e-06, 'optimizer': 'Adam', 'weight_decay': 1.7975839086129747e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=8.499508418306741e-07, optimizer=Adam, weight_decay=2.4953589373673907e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 29, Fold 3: Test Accuracy = 0.4764\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 30, Fold 1: Test Accuracy = 0.3200\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 29, Fold 4: Test Accuracy = 0.5328\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 30, Fold 2: Test Accuracy = 0.3808\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:18:29,140] Trial 29 finished with value: 0.47835704971082815 and parameters: {'ff_dim': 32, 'dropout_rate': 0.15180952857197247, 'embed_dim': 1024, 'learning_rate': 1.1530312363994997e-06, 'optimizer': 'Adam', 'weight_decay': 2.88760585811824e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 32}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29, Fold 5: Test Accuracy = 0.4724\n",
      "Trial 29: Mean Accuracy = 0.4784, Fold Accuracies = [np.float64(0.49627705627705626), np.float64(0.4138598422307833), np.float64(0.47641050512498645), np.float64(0.5328214810568747), np.float64(0.4724163638644403)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=4.2826590912178575e-07, optimizer=Adam, weight_decay=2.5138296991874028e-05, batch_size=16,factor=1\n",
      "Trial 30, Fold 3: Test Accuracy = 0.3395\n",
      "Trial 31, Fold 1: Test Accuracy = 0.3816\n",
      "Trial 30, Fold 4: Test Accuracy = 0.3838\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:21:26,471] Trial 30 finished with value: 0.36307948024019854 and parameters: {'ff_dim': 2048, 'dropout_rate': 0.13523754036379418, 'embed_dim': 32, 'learning_rate': 8.499508418306741e-07, 'optimizer': 'Adam', 'weight_decay': 2.4953589373673907e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30, Fold 5: Test Accuracy = 0.3914\n",
      "Trial 30: Mean Accuracy = 0.3631, Fold Accuracies = [np.float64(0.31995670995670994), np.float64(0.3807588821622944), np.float64(0.33946341991532), np.float64(0.3838217062932125), np.float64(0.3913966828734557)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=3.191952377546225e-07, optimizer=Adam, weight_decay=9.650670910772898e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 31, Fold 2: Test Accuracy = 0.2794\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 32, Fold 1: Test Accuracy = 0.4794\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 31, Fold 3: Test Accuracy = 0.3572\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 31, Fold 4: Test Accuracy = 0.3741\n",
      "Trial 32, Fold 2: Test Accuracy = 0.4357\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:27:08,291] Trial 31 finished with value: 0.35400645381584683 and parameters: {'ff_dim': 2048, 'dropout_rate': 0.1339406016137002, 'embed_dim': 32, 'learning_rate': 4.2826590912178575e-07, 'optimizer': 'Adam', 'weight_decay': 2.5138296991874028e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31, Fold 5: Test Accuracy = 0.3777\n",
      "Trial 31: Mean Accuracy = 0.3540, Fold Accuracies = [np.float64(0.38160173160173166), np.float64(0.27944413869014856), np.float64(0.35716495931319625), np.float64(0.3741206051370776), np.float64(0.37770083433708024)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=2.4511558646364287e-06, optimizer=Adam, weight_decay=7.92277168064745e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 33, Fold 1: Test Accuracy = 0.5635\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 32, Fold 3: Test Accuracy = 0.4163\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 33, Fold 2: Test Accuracy = 0.4273\n",
      "Trial 32, Fold 4: Test Accuracy = 0.4890\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 33, Fold 3: Test Accuracy = 0.4713\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 33, Fold 4: Test Accuracy = 0.5645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:36:04,688] Trial 32 finished with value: 0.46346550406855336 and parameters: {'ff_dim': 32, 'dropout_rate': 0.22568076836902123, 'embed_dim': 2048, 'learning_rate': 3.191952377546225e-07, 'optimizer': 'Adam', 'weight_decay': 9.650670910772898e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 18 with value: 0.4904910822876266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32, Fold 5: Test Accuracy = 0.4969\n",
      "Trial 32: Mean Accuracy = 0.4635, Fold Accuracies = [np.float64(0.4794372294372294), np.float64(0.43565706598177706), np.float64(0.41628847397419805), np.float64(0.4889971064718903), np.float64(0.496947644477672)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=1.4860802510083972e-06, optimizer=Adam, weight_decay=6.379098644840817e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 33, Fold 5: Test Accuracy = 0.4511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 12:43:44,298] Trial 33 finished with value: 0.49553429258193216 and parameters: {'ff_dim': 32, 'dropout_rate': 0.22443843372647287, 'embed_dim': 2048, 'learning_rate': 2.4511558646364287e-06, 'optimizer': 'Adam', 'weight_decay': 7.92277168064745e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33: Mean Accuracy = 0.4955, Fold Accuracies = [np.float64(0.5634632034632036), np.float64(0.4273466642206323), np.float64(0.4712862501719058), np.float64(0.5644740763262258), np.float64(0.4511012687276936)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=5.631863378465107e-06, optimizer=Adam, weight_decay=5.662088870631402e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 35, Fold 1: Test Accuracy = 0.4888\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 34, Fold 1: Test Accuracy = 0.4455\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 35, Fold 2: Test Accuracy = 0.3442\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 35, Fold 3: Test Accuracy = 0.4882\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 35, Fold 4: Test Accuracy = 0.5247\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 34, Fold 2: Test Accuracy = 0.3491\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 35, Fold 5: Test Accuracy = 0.4968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 14:26:08,446] Trial 35 finished with value: 0.46853575294407124 and parameters: {'ff_dim': 32, 'dropout_rate': 0.17463684617618194, 'embed_dim': 2048, 'learning_rate': 5.631863378465107e-06, 'optimizer': 'Adam', 'weight_decay': 5.662088870631402e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35: Mean Accuracy = 0.4685, Fold Accuracies = [np.float64(0.48883116883116884), np.float64(0.34418455329297376), np.float64(0.48823433024049256), np.float64(0.5246656650094937), np.float64(0.49676304734622745)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=1.6884657931296817e-06, optimizer=Adam, weight_decay=5.6121112348405454e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 34, Fold 3: Test Accuracy = 0.4255\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 36, Fold 1: Test Accuracy = 0.5056\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 34, Fold 4: Test Accuracy = 0.4892\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 36, Fold 2: Test Accuracy = 0.3735\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 34, Fold 5: Test Accuracy = 0.4957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 15:29:32,201] Trial 34 finished with value: 0.44101633435431997 and parameters: {'ff_dim': 32, 'dropout_rate': 0.173291427362385, 'embed_dim': 2048, 'learning_rate': 1.4860802510083972e-06, 'optimizer': 'Adam', 'weight_decay': 6.379098644840817e-06, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 16}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34: Mean Accuracy = 0.4410, Fold Accuracies = [np.float64(0.4455411255411255), np.float64(0.3491316577997921), np.float64(0.42546129267286403), np.float64(0.48921745326981), np.float64(0.4957301424880085)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=3.1066660371825837e-05, optimizer=Adam, weight_decay=5.80783158014497e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 36, Fold 3: Test Accuracy = 0.4045\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 37, Fold 1: Test Accuracy = 0.4284\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 36, Fold 4: Test Accuracy = 0.5362\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 37, Fold 2: Test Accuracy = 0.3751\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 36, Fold 5: Test Accuracy = 0.4696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 16:29:56,457] Trial 36 finished with value: 0.4579076480443212 and parameters: {'ff_dim': 64, 'dropout_rate': 0.22918456129606093, 'embed_dim': 2048, 'learning_rate': 1.6884657931296817e-06, 'optimizer': 'Adam', 'weight_decay': 5.6121112348405454e-05, 'batch_size': 10, 'num_heads': 8, 'num_transformer_blocks': 8}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36: Mean Accuracy = 0.4579, Fold Accuracies = [np.float64(0.5056277056277055), np.float64(0.37353696569436795), np.float64(0.4045118315060116), np.float64(0.5362348879085679), np.float64(0.4696268494849531)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=0.0016859252856168277, optimizer=AdamW, weight_decay=1.8161949291791775e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 37, Fold 3: Test Accuracy = 0.4103\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 38, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 37, Fold 4: Test Accuracy = 0.4114\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 38, Fold 2: Test Accuracy = 0.3389\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 38, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 16:48:01,532] Trial 37 finished with value: 0.40698020152627806 and parameters: {'ff_dim': 64, 'dropout_rate': 0.12708128544620198, 'embed_dim': 2048, 'learning_rate': 3.1066660371825837e-05, 'optimizer': 'Adam', 'weight_decay': 5.80783158014497e-05, 'batch_size': 10, 'num_heads': 8, 'num_transformer_blocks': 8}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37, Fold 5: Test Accuracy = 0.4097\n",
      "Trial 37: Mean Accuracy = 0.4070, Fold Accuracies = [np.float64(0.4283549783549783), np.float64(0.3751268880327769), np.float64(0.4103200519667483), np.float64(0.41141134791089334), np.float64(0.4096877413659936)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=8.133767072493931e-06, optimizer=AdamW, weight_decay=1.9278438288657877e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 38, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 39, Fold 1: Test Accuracy = 0.5210\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 16:50:48,353] Trial 38 finished with value: 0.33444444444444443 and parameters: {'ff_dim': 1024, 'dropout_rate': 0.1291849227640748, 'embed_dim': 2048, 'learning_rate': 0.0016859252856168277, 'optimizer': 'AdamW', 'weight_decay': 1.8161949291791775e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 38: Mean Accuracy = 0.3344, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.33888888888888885), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=6.9989140330022756e-06, optimizer=AdamW, weight_decay=0.00010563264250406262, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 40, Fold 1: Test Accuracy = 0.4705\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 39, Fold 2: Test Accuracy = 0.3968\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 39, Fold 3: Test Accuracy = 0.4522\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 40, Fold 2: Test Accuracy = 0.3787\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 39, Fold 4: Test Accuracy = 0.5153\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 40, Fold 3: Test Accuracy = 0.4085\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 16:55:24,377] Trial 39 finished with value: 0.4758549662912116 and parameters: {'ff_dim': 1024, 'dropout_rate': 0.16253836627339444, 'embed_dim': 128, 'learning_rate': 8.133767072493931e-06, 'optimizer': 'AdamW', 'weight_decay': 1.9278438288657877e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39, Fold 5: Test Accuracy = 0.4939\n",
      "Trial 39: Mean Accuracy = 0.4759, Fold Accuracies = [np.float64(0.5210389610389611), np.float64(0.39683544303797474), np.float64(0.45220068645265493), np.float64(0.5153151502139682), np.float64(0.49388459071249874)]\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=0.00012065756696969491, optimizer=Adam, weight_decay=0.00011012823925233242, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 40, Fold 4: Test Accuracy = 0.5243\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 16:56:43,147] Trial 40 finished with value: 0.46294739952361896 and parameters: {'ff_dim': 256, 'dropout_rate': 0.16722003836812155, 'embed_dim': 128, 'learning_rate': 6.9989140330022756e-06, 'optimizer': 'AdamW', 'weight_decay': 0.00010563264250406262, 'batch_size': 16, 'num_heads': 2, 'num_transformer_blocks': 4}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40, Fold 5: Test Accuracy = 0.5328\n",
      "Trial 40: Mean Accuracy = 0.4629, Fold Accuracies = [np.float64(0.47051948051948056), np.float64(0.37865529260686115), np.float64(0.4084839520478001), np.float64(0.5243213016996612), np.float64(0.5327569707442915)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=1, learning_rate=7.499031147531724e-07, optimizer=Adam, weight_decay=9.017426418174215e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 41, Fold 1: Test Accuracy = 0.4643\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 41, Fold 2: Test Accuracy = 0.3920\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 42, Fold 1: Test Accuracy = 0.4567\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 41, Fold 3: Test Accuracy = 0.3854\n",
      "Trial 42, Fold 2: Test Accuracy = 0.3967\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 41, Fold 4: Test Accuracy = 0.5038\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:01:43,335] Trial 41 finished with value: 0.4231403072706307 and parameters: {'ff_dim': 256, 'dropout_rate': 0.19660193233270404, 'embed_dim': 512, 'learning_rate': 0.00012065756696969491, 'optimizer': 'Adam', 'weight_decay': 0.00011012823925233242, 'batch_size': 16, 'num_heads': 2, 'num_transformer_blocks': 4}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41, Fold 5: Test Accuracy = 0.3702\n",
      "Trial 41: Mean Accuracy = 0.4231, Fold Accuracies = [np.float64(0.46428571428571425), np.float64(0.39199229499174465), np.float64(0.3854354117106599), np.float64(0.5037647234682612), np.float64(0.3702233918967733)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=1, learning_rate=6.686685908069584e-07, optimizer=Adam, weight_decay=8.953824798122134e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 42, Fold 3: Test Accuracy = 0.4984\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 43, Fold 1: Test Accuracy = 0.4978\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 42, Fold 4: Test Accuracy = 0.4601\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 42, Fold 5: Test Accuracy = 0.4489\n",
      "Trial 42: Mean Accuracy = 0.4522, Fold Accuracies = [np.float64(0.4566666666666667), np.float64(0.3966947960618847), np.float64(0.49835116764250625), np.float64(0.46014736057238864), np.float64(0.44892763750493475)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:04:44,836] Trial 42 finished with value: 0.4521575256896762 and parameters: {'ff_dim': 32, 'dropout_rate': 0.32861590057841755, 'embed_dim': 512, 'learning_rate': 7.499031147531724e-07, 'optimizer': 'Adam', 'weight_decay': 9.017426418174215e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=16, learning_rate=2.2925363607107718e-07, optimizer=Adam, weight_decay=0.000998602277859039, batch_size=10,factor=1\n",
      "Trial 43, Fold 2: Test Accuracy = 0.4144\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 44, Fold 1: Test Accuracy = 0.4532\n",
      "Trial 43, Fold 3: Test Accuracy = 0.4799\n",
      "Trial 43, Fold 4: Test Accuracy = 0.5451\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 44, Fold 2: Test Accuracy = 0.4243\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:11:34,492] Trial 43 finished with value: 0.49366651826489943 and parameters: {'ff_dim': 32, 'dropout_rate': 0.3252336776552044, 'embed_dim': 1024, 'learning_rate': 6.686685908069584e-07, 'optimizer': 'Adam', 'weight_decay': 8.953824798122134e-06, 'batch_size': 16, 'num_heads': 4, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43, Fold 5: Test Accuracy = 0.5311\n",
      "Trial 43: Mean Accuracy = 0.4937, Fold Accuracies = [np.float64(0.4978354978354978), np.float64(0.4143765669907662), np.float64(0.4798628850323476), np.float64(0.5451129588491878), np.float64(0.5311446826166979)]\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=1, learning_rate=1.786641506484801e-07, optimizer=Adam, weight_decay=3.652335371521148e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 45, Fold 1: Test Accuracy = 0.4332\n",
      "Trial 44, Fold 3: Test Accuracy = 0.4507\n",
      "Trial 45, Fold 2: Test Accuracy = 0.3439\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 45, Fold 3: Test Accuracy = 0.3339\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 44, Fold 4: Test Accuracy = 0.4887\n",
      "Trial 45, Fold 4: Test Accuracy = 0.4333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:21:38,307] Trial 44 finished with value: 0.4646237417852799 and parameters: {'ff_dim': 32, 'dropout_rate': 0.2749878218353121, 'embed_dim': 1024, 'learning_rate': 2.2925363607107718e-07, 'optimizer': 'Adam', 'weight_decay': 0.000998602277859039, 'batch_size': 10, 'num_heads': 4, 'num_transformer_blocks': 16}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44, Fold 5: Test Accuracy = 0.5062\n",
      "Trial 44: Mean Accuracy = 0.4646, Fold Accuracies = [np.float64(0.45324675324675323), np.float64(0.42430746651990464), np.float64(0.4507051062597109), np.float64(0.48865293366361223), np.float64(0.5062064492364181)]\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=1, learning_rate=6.599523880369536e-07, optimizer=Adam, weight_decay=3.615405676468489e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:21:53,898] Trial 45 finished with value: 0.39792912814698433 and parameters: {'ff_dim': 32, 'dropout_rate': 0.33448527393014604, 'embed_dim': 1024, 'learning_rate': 1.786641506484801e-07, 'optimizer': 'Adam', 'weight_decay': 3.652335371521148e-05, 'batch_size': 16, 'num_heads': 16, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45, Fold 5: Test Accuracy = 0.4454\n",
      "Trial 45: Mean Accuracy = 0.3979, Fold Accuracies = [np.float64(0.4332034632034632), np.float64(0.3438849140830429), np.float64(0.3338519766968928), np.float64(0.43328079935468233), np.float64(0.44542448739684054)]\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=1, learning_rate=2.3693772558805398e-06, optimizer=Adam, weight_decay=1.2318124589003632e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 47, Fold 1: Test Accuracy = 0.4650\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 46, Fold 1: Test Accuracy = 0.4838\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 47, Fold 2: Test Accuracy = 0.4590\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 46, Fold 2: Test Accuracy = 0.4157\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 47, Fold 3: Test Accuracy = 0.4547\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 46, Fold 3: Test Accuracy = 0.3707\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 47, Fold 4: Test Accuracy = 0.5179\n",
      "Trial 46, Fold 4: Test Accuracy = 0.4786\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:28:57,197] Trial 47 finished with value: 0.4711857566339397 and parameters: {'ff_dim': 32, 'dropout_rate': 0.3563802126968223, 'embed_dim': 256, 'learning_rate': 2.3693772558805398e-06, 'optimizer': 'Adam', 'weight_decay': 1.2318124589003632e-05, 'batch_size': 16, 'num_heads': 8, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47, Fold 5: Test Accuracy = 0.4593\n",
      "Trial 47: Mean Accuracy = 0.4712, Fold Accuracies = [np.float64(0.464978354978355), np.float64(0.45902280927047023), np.float64(0.4547280670183786), np.float64(0.5179055268501463), np.float64(0.459294025052348)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=1, learning_rate=5.613351560740463e-07, optimizer=Adam, weight_decay=7.2396418156082805e-06, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:31:03,593] Trial 46 finished with value: 0.45357642075850285 and parameters: {'ff_dim': 32, 'dropout_rate': 0.28595219717421083, 'embed_dim': 256, 'learning_rate': 6.599523880369536e-07, 'optimizer': 'Adam', 'weight_decay': 3.615405676468489e-05, 'batch_size': 16, 'num_heads': 16, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46, Fold 5: Test Accuracy = 0.5191\n",
      "Trial 46: Mean Accuracy = 0.4536, Fold Accuracies = [np.float64(0.4837662337662338), np.float64(0.4157280009784137), np.float64(0.3706562703909502), np.float64(0.4786440454960892), np.float64(0.5190875531608273)]\n",
      "Hyperparameters: num_heads=1, num_transformer_blocks=1, learning_rate=1.2326901874618455e-07, optimizer=Adam, weight_decay=3.528539287404821e-06, batch_size=16,factor=1\n",
      "Trial 48, Fold 1: Test Accuracy = 0.5003\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 49, Fold 1: Test Accuracy = 0.4082\n",
      "Trial 48, Fold 2: Test Accuracy = 0.4134\n",
      "Trial 49, Fold 2: Test Accuracy = 0.4029\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 49, Fold 3: Test Accuracy = 0.4522\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 48, Fold 3: Test Accuracy = 0.4353\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 48, Fold 4: Test Accuracy = 0.5091\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 49, Fold 4: Test Accuracy = 0.4872\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 49, Fold 5: Test Accuracy = 0.4433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:41:54,116] Trial 49 finished with value: 0.43876912678135954 and parameters: {'ff_dim': 128, 'dropout_rate': 0.23689546053481753, 'embed_dim': 2048, 'learning_rate': 1.2326901874618455e-07, 'optimizer': 'Adam', 'weight_decay': 3.528539287404821e-06, 'batch_size': 16, 'num_heads': 1, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49: Mean Accuracy = 0.4388, Fold Accuracies = [np.float64(0.4082251082251082), np.float64(0.40287714792392837), np.float64(0.4522445772873709), np.float64(0.48715228941910554), np.float64(0.44334651105128486)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=1.2228910383354978e-06, optimizer=AdamW, weight_decay=0.00018664586420108893, batch_size=10,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 17:42:17,227] Trial 48 finished with value: 0.4726977719808613 and parameters: {'ff_dim': 128, 'dropout_rate': 0.293794253803453, 'embed_dim': 2048, 'learning_rate': 5.613351560740463e-07, 'optimizer': 'Adam', 'weight_decay': 7.2396418156082805e-06, 'batch_size': 16, 'num_heads': 1, 'num_transformer_blocks': 1}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48, Fold 5: Test Accuracy = 0.5054\n",
      "Trial 48: Mean Accuracy = 0.4727, Fold Accuracies = [np.float64(0.5002597402597403), np.float64(0.41340426832997007), np.float64(0.43529064510748866), np.float64(0.5091101970846531), np.float64(0.5054240091224546)]\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=32, learning_rate=1.4553714073281512e-06, optimizer=AdamW, weight_decay=4.214118031709585e-06, batch_size=10,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-09 17:46:14,150] Trial 50 failed with parameters: {'ff_dim': 32, 'dropout_rate': 0.12066243087829602, 'embed_dim': 2048, 'learning_rate': 1.2228910383354978e-06, 'optimizer': 'AdamW', 'weight_decay': 0.00018664586420108893, 'batch_size': 10, 'num_heads': 4, 'num_transformer_blocks': 8} because of the following error: RuntimeError('DataLoader worker (pid(s) 16516) exited unexpectedly').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1251, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\queue.py\", line 212, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_2068\\2990928662.py\", line 93, in objective\n",
      "    for val_inputs, val_labels in val_loader:\n",
      "                                  ^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1458, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1410, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1264, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "        f\"DataLoader worker (pid(s) {pids_str}) exited unexpectedly\"\n",
      "    ) from e\n",
      "RuntimeError: DataLoader worker (pid(s) 16516) exited unexpectedly\n",
      "[W 2025-05-09 17:46:14,162] Trial 50 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 51, Fold 1: Test Accuracy = 0.5072\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 51, Fold 2: Test Accuracy = 0.4144\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 51, Fold 3: Test Accuracy = 0.4546\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 51, Fold 4: Test Accuracy = 0.5070\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-09 18:02:56,365] Trial 51 finished with value: 0.4651167621314281 and parameters: {'ff_dim': 32, 'dropout_rate': 0.12382811746676455, 'embed_dim': 1024, 'learning_rate': 1.4553714073281512e-06, 'optimizer': 'AdamW', 'weight_decay': 4.214118031709585e-06, 'batch_size': 10, 'num_heads': 4, 'num_transformer_blocks': 32}. Best is trial 33 with value: 0.49553429258193216.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51, Fold 5: Test Accuracy = 0.4425\n",
      "Trial 51: Mean Accuracy = 0.4651, Fold Accuracies = [np.float64(0.5071861471861472), np.float64(0.4143826820766832), np.float64(0.45459017664597945), np.float64(0.5069542909935373), np.float64(0.44247051375479374)]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 16516) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\queue.py:212\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 144\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# Start Optuna Study\u001b[39;00m\n\u001b[0;32m    138\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    139\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    140\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(),\n\u001b[0;32m    141\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_startup_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_min_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, interval_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m    142\u001b[0m )\n\u001b[1;32m--> 144\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# Best result\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:100\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[38;5;66;03m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n\u001b[1;32m--> 100\u001b[0m                         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m                 futures\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    103\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    104\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m                     )\n\u001b[0;32m    116\u001b[0m                 )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 93\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     91\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 93\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mval_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 16516) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "np.random.seed(42)  # Set random seed for reproducibility\n",
    "torch.manual_seed(42)  # Set random seed for reproducibility\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    #embed_dim=64, num_heads=2, ff_dim=64, num_transformer_blocks=2\n",
    "    ff_dim = trial.suggest_categorical(\"ff_dim\", [32, 64, 128, 256,512,1024,2048])\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    embed_dim = trial.suggest_categorical(\"embed_dim\", [32, 64, 128, 256,512,1024,2048])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-7, 1e-1, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [1, 2, 4, 8, 16])\n",
    "\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [1, 2, 4,8,16,32])\n",
    "    label_smoothing = 0.3\n",
    "\n",
    "    factor = 1\n",
    "    \n",
    "\n",
    "    print(f\"Hyperparameters: num_heads={num_heads}, num_transformer_blocks={num_transformer_blocks}, \"\n",
    "          f\"learning_rate={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size},factor={factor}\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "\n",
    "    for test_fold_idx in range(5):\n",
    "        test_fold = fold_indices[test_fold_idx]\n",
    "        remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "        val_fold_idx = test_fold_idx % 4\n",
    "        val_fold = remaining_folds[val_fold_idx]\n",
    "        train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "\n",
    "        test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "        test_labels = labels_folds[test_fold]\n",
    "\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                      torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                    torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), \n",
    "                                     torch.tensor(test_labels, dtype=torch.long))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TransformerModel(input_shape=input_shape, num_classes=num_classes, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim,dropout_rate=dropout_rate, num_transformer_blocks=num_transformer_blocks).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "        optimizer = optimizer_cls[optimizer_name](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Trial {trial.number}: Mean Accuracy = {mean_accuracy:.4f}, Fold Accuracies = {fold_accuracies}\")\n",
    "    \n",
    "    trial.set_user_attr(\"fold_accuracies\", fold_accuracies)\n",
    "    trial.report(mean_accuracy, step=0)  # Single report after all folds\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5, n_min_trials=5, interval_steps=1),\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=200, n_jobs=2)\n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
