{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"G:\\CODING\\py\\data\\numpy\\melunfiltered\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens → 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "├─CustomCnn: 1-1                                             --\n",
       "│    └─Sequential: 2-1                                       --\n",
       "│    │    └─Conv2d: 3-1                                      448\n",
       "│    │    └─BatchNorm2d: 3-2                                 32\n",
       "│    │    └─ReLU: 3-3                                        --\n",
       "│    │    └─Conv2d: 3-4                                      4,640\n",
       "│    │    └─BatchNorm2d: 3-5                                 64\n",
       "│    │    └─ReLU: 3-6                                        --\n",
       "│    │    └─MaxPool2d: 3-7                                   --\n",
       "│    └─Sequential: 2-2                                       --\n",
       "│    │    └─Conv2d: 3-8                                      18,496\n",
       "│    │    └─BatchNorm2d: 3-9                                 128\n",
       "│    │    └─ReLU: 3-10                                       --\n",
       "│    │    └─Conv2d: 3-11                                     73,856\n",
       "│    │    └─BatchNorm2d: 3-12                                256\n",
       "│    │    └─ReLU: 3-13                                       --\n",
       "│    │    └─MaxPool2d: 3-14                                  --\n",
       "│    └─AdaptiveAvgPool2d: 2-3                                --\n",
       "│    └─Flatten: 2-4                                          --\n",
       "├─Linear: 1-2                                                164,160\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─TransformerEncoder: 2-5                               --\n",
       "│    │    └─MultiheadAttention: 3-15                         410,880\n",
       "│    │    └─Sequential: 3-16                                 41,344\n",
       "│    │    └─LayerNorm: 3-17                                  640\n",
       "│    │    └─LayerNorm: 3-18                                  640\n",
       "│    │    └─Dropout: 3-19                                    --\n",
       "│    │    └─Dropout: 3-20                                    --\n",
       "│    └─TransformerEncoder: 2-6                               --\n",
       "│    │    └─MultiheadAttention: 3-21                         410,880\n",
       "│    │    └─Sequential: 3-22                                 41,344\n",
       "│    │    └─LayerNorm: 3-23                                  640\n",
       "│    │    └─LayerNorm: 3-24                                  640\n",
       "│    │    └─Dropout: 3-25                                    --\n",
       "│    │    └─Dropout: 3-26                                    --\n",
       "│    └─TransformerEncoder: 2-7                               --\n",
       "│    │    └─MultiheadAttention: 3-27                         410,880\n",
       "│    │    └─Sequential: 3-28                                 41,344\n",
       "│    │    └─LayerNorm: 3-29                                  640\n",
       "│    │    └─LayerNorm: 3-30                                  640\n",
       "│    │    └─Dropout: 3-31                                    --\n",
       "│    │    └─Dropout: 3-32                                    --\n",
       "│    └─TransformerEncoder: 2-8                               --\n",
       "│    │    └─MultiheadAttention: 3-33                         410,880\n",
       "│    │    └─Sequential: 3-34                                 41,344\n",
       "│    │    └─LayerNorm: 3-35                                  640\n",
       "│    │    └─LayerNorm: 3-36                                  640\n",
       "│    │    └─Dropout: 3-37                                    --\n",
       "│    │    └─Dropout: 3-38                                    --\n",
       "├─AdaptiveAvgPool1d: 1-4                                     --\n",
       "├─Dropout: 1-5                                               --\n",
       "├─Linear: 1-6                                                102,720\n",
       "├─Linear: 1-7                                                102,720\n",
       "├─Linear: 1-8                                                25,680\n",
       "├─Linear: 1-9                                                243\n",
       "=====================================================================================\n",
       "Total params: 2,307,459\n",
       "Trainable params: 2,307,459\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = TRANS_CNN(input_shape=(224,224,3),num_classes=3,num_transformer_blocks=4,embed_dim=320)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (3,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-11 19:09:01,790] A new study created in memory with name: no-name-d4e79f84-b02a-4aaa-aa62-b874a836505c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Trial 2: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:16:53,229] Trial 2 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 128, 'learning_rate': 0.00041532664657970947, 'optimizer': 'AdamW', 'weight_decay': 1.67049383478662e-05, 'batch_size': 32}. Best is trial 2 with value: 0.3333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 27\n",
      "Trial 0: Test Accuracy = 0.5636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:17:55,176] Trial 0 finished with value: 0.5635635459250458 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 5.089775844879498e-05, 'optimizer': 'Adam', 'weight_decay': 0.0003062399592188638, 'batch_size': 16}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1: Test Accuracy = 0.4976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:18:13,550] Trial 1 finished with value: 0.4976299218192355 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 5.318442667832418e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0006820338362388563, 'batch_size': 32}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n",
      "Trial 3: Test Accuracy = 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:20:59,848] Trial 3 finished with value: 0.3329070758738278 and parameters: {'num_heads': 16, 'num_transformer_blocks': 64, 'learning_rate': 1.717197327279729e-06, 'optimizer': 'Adam', 'weight_decay': 1.6661379559011553e-06, 'batch_size': 10}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Trial 4: Test Accuracy = 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:24:49,509] Trial 4 finished with value: 0.3329070758738278 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 0.003971988455779089, 'optimizer': 'SGD', 'weight_decay': 2.4956206820354428e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5635635459250458.\n",
      "[I 2025-04-11 19:25:09,286] Trial 5 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 28\n",
      "Trial 6: Test Accuracy = 0.5239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:25:13,522] Trial 6 finished with value: 0.5238690891739556 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 5.336629066166711e-05, 'optimizer': 'AdamW', 'weight_decay': 7.220476772303716e-06, 'batch_size': 32}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 30\n",
      "Trial 7: Test Accuracy = 0.3270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:30:26,289] Trial 7 finished with value: 0.32699009650647887 and parameters: {'num_heads': 4, 'num_transformer_blocks': 16, 'learning_rate': 8.480313071041028e-05, 'optimizer': 'SGD', 'weight_decay': 0.000616089612429351, 'batch_size': 10}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:36:23,708] Trial 9 finished with value: 0.39692981352075235 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 8.003135623414343e-07, 'optimizer': 'AdamW', 'weight_decay': 1.0928442813745245e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9: Test Accuracy = 0.3969\n",
      "Trial 8: Test Accuracy = 0.3688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:43:41,015] Trial 8 finished with value: 0.3688330418895848 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 6.235176598489412e-08, 'optimizer': 'AdamW', 'weight_decay': 0.00015600031047887557, 'batch_size': 16}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 11\n",
      "Trial 11: Test Accuracy = 0.3302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:46:44,613] Trial 11 finished with value: 0.33016185476815396 and parameters: {'num_heads': 2, 'num_transformer_blocks': 256, 'learning_rate': 0.00014465821425870882, 'optimizer': 'SGD', 'weight_decay': 2.6953827801721902e-06, 'batch_size': 32}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22\n",
      "Trial 10: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:50:20,154] Trial 10 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 0.0022681546579521907, 'optimizer': 'AdamW', 'weight_decay': 3.536605801162985e-06, 'batch_size': 10}. Best is trial 0 with value: 0.5635635459250458.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 25\n",
      "Trial 14: Test Accuracy = 0.5698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:53:48,356] Trial 14 finished with value: 0.5697960229395878 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 5.317021412854359e-05, 'optimizer': 'Adam', 'weight_decay': 5.799865576862175e-06, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 19:56:56,605] Trial 15 finished with value: 0.5277626042428839 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 2.5426958881442147e-05, 'optimizer': 'Adam', 'weight_decay': 9.528140843798094e-05, 'batch_size': 32}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15: Test Accuracy = 0.5278\n",
      "Trial 12: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:02:01,111] Trial 12 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 2.5793206909821422e-06, 'optimizer': 'SGD', 'weight_decay': 0.0001466310885522123, 'batch_size': 10}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 34\n",
      "Trial 16: Test Accuracy = 0.5344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:03:30,364] Trial 16 finished with value: 0.5344149056975295 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 1.1972737709005989e-05, 'optimizer': 'Adam', 'weight_decay': 7.764793303240294e-05, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:03:48,932] Trial 17 finished with value: 0.5597097477585123 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0006192518571473659, 'optimizer': 'Adam', 'weight_decay': 7.48101062452531e-05, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17: Test Accuracy = 0.5597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:05:23,837] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:11:18,063] Trial 18 finished with value: 0.5409579157464651 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0005856208884128818, 'optimizer': 'Adam', 'weight_decay': 6.310361323897404e-05, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18: Test Accuracy = 0.5410\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:11:33,734] Trial 20 finished with value: 0.5437728512037018 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0005851586536967423, 'optimizer': 'Adam', 'weight_decay': 1.1313202974434392e-06, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20: Test Accuracy = 0.5438\n",
      "Early stopping at epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:29:27,310] Trial 23 finished with value: 0.5549660252666627 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 1.546462490519835e-05, 'optimizer': 'Adam', 'weight_decay': 5.522879178632196e-06, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23: Test Accuracy = 0.5550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:40:53,981] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:41:42,841] Trial 24 finished with value: 0.5110915743108837 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0001063828776192688, 'optimizer': 'Adam', 'weight_decay': 4.3212685788800346e-05, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24: Test Accuracy = 0.5111\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:48:03,183] Trial 25 finished with value: 0.5515225418593775 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0001699064353293709, 'optimizer': 'Adam', 'weight_decay': 0.00030853034076518766, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25: Test Accuracy = 0.5515\n",
      "Early stopping at epoch 42\n",
      "Trial 21: Test Accuracy = 0.3199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 20:58:27,959] Trial 21 finished with value: 0.3199146142537809 and parameters: {'num_heads': 8, 'num_transformer_blocks': 128, 'learning_rate': 1.3454725595712541e-08, 'optimizer': 'Adam', 'weight_decay': 1.1288395495548247e-06, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 21:02:23,195] Trial 28 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.00969564215801024, 'optimizer': 'Adam', 'weight_decay': 0.00026597604066260844, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 21:03:45,735] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 21:06:58,065] Trial 30 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0016496869670100617, 'optimizer': 'Adam', 'weight_decay': 1.3320504975396392e-05, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 21:08:08,899] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-11 21:20:15,702] Trial 32 finished with value: 0.3356324385283042 and parameters: {'num_heads': 32, 'num_transformer_blocks': 64, 'learning_rate': 4.087161212463411e-06, 'optimizer': 'Adam', 'weight_decay': 0.0004234047465742844, 'batch_size': 16}. Best is trial 14 with value: 0.5697960229395878.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32: Test Accuracy = 0.3356\n",
      "Early stopping at epoch 12\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Hyperparameter suggestions\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8, 16, 32])\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [2, 4, 8,16,32, 64,128,256])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    label_smoothing = 0.3\n",
    "\n",
    "    # Model\n",
    "    foldmodel = TRANS_CNN(input_shape=input_shape, num_transformer_blocks=num_transformer_blocks, \n",
    "                          num_heads=num_heads, num_classes=num_classes, embed_dim=128).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "    optimizer = optimizer_cls[optimizer_name](foldmodel.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Train-validation split (use last fold as test)\n",
    "    test_fold = fold_indices[4]\n",
    "    train_folds = fold_indices[:4]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "    val_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    val_labels = labels_folds[test_fold]\n",
    "\n",
    "    # Balance dataset\n",
    "    balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=2)\n",
    "\n",
    "    # Convert to PyTorch tensors (keep tensors on CPU to avoid slow batch transfers)\n",
    "    train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                  torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                torch.tensor(val_labels, dtype=torch.long))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Training loop with early stopping & pruning\n",
    "    epochs = 50  # Reduce max epochs\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 10  # Early stopping patience\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        foldmodel.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = foldmodel(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        foldmodel.eval()\n",
    "        val_loss, val_preds, val_labels_list = 0.0, [], []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                val_outputs = foldmodel(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "\n",
    "        # Optuna Pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # Final evaluation\n",
    "    foldmodel.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = foldmodel(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    test_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Trial {trial.number}: Test Accuracy = {test_acc:.4f}\")\n",
    "    \n",
    "    return test_acc\n",
    "\n",
    "# Run Optuna optimization with parallel trials\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.MedianPruner(n_warmup_steps=5))\n",
    "study.optimize(objective, n_trials=300, n_jobs=4)  # Reduce trials & use parallel execution\n",
    "\n",
    "# Best parameters\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
