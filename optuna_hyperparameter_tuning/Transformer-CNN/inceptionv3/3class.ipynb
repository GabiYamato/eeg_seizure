{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® IMPORTS ‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® LOADING THE SPLIT DATA ARRAYS ‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">üìÅ reminder to change the folder path to your numpy array folder üìÅ</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"G:\\CODING\\py\\data\\numpy\\mel\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_fold_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">data balancer & early stopping</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Result plotting</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_metrics(labels, predictions, n_classes=3):\n",
    "    \"\"\"\n",
    "    Plots and computes metrics for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        labels (array-like): True labels.\n",
    "        predictions (array-like): Predicted probabilities or class predictions.\n",
    "        n_classes (int): Number of classes (default is 3 for multi-class classification).\n",
    "    \"\"\"\n",
    "    # If predictions are probabilities, convert to class predictions\n",
    "    if predictions.ndim > 1:\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predicted_classes = predictions\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, predicted_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise Accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for d in range(n_classes):\n",
    "        correct_preds = cm[d][d]\n",
    "        total_true_samples = sum(cm[d])\n",
    "        ratio_correct = correct_preds / total_true_samples if total_true_samples != 0 else 0\n",
    "        print(f'Class {d}: Correct Predictions / Total True Samples = {correct_preds}/{total_true_samples} ({ratio_correct:.2%})')\n",
    "\n",
    "    # Precision-Recall Curves and AUPRC\n",
    "    print(\"\\nPrecision-Recall Curves:\")\n",
    "    labels_binarized = label_binarize(labels, classes=np.arange(n_classes))\n",
    "    auprcs = []\n",
    "    for class_idx in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        auprcs.append(auprc)\n",
    "        plt.plot(recall, precision, label=f'Class {class_idx + 1} (AUPRC = {auprc:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curves and AUC\n",
    "    print(\"\\nROC Curves:\")\n",
    "    for class_idx in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {class_idx + 1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Mean F1 Score\n",
    "    f1 = f1_score(labels, predicted_classes, average='macro')\n",
    "    print(f\"\\nMean F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    balanced_acc = balanced_accuracy_score(labels, predicted_classes)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    # Average AUPRC\n",
    "    mean_auprc = np.mean(auprcs)\n",
    "    print(f\"Average AUPRC: {mean_auprc:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predicted_classes))\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"class_wise_accuracy\": [cm[d][d] / sum(cm[d]) if sum(cm[d]) != 0 else 0 for d in range(n_classes)],\n",
    "        \"mean_f1_score\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc,\n",
    "        \"average_auprc\": mean_auprc,\n",
    "        \"auprc_per_class\": auprcs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">MODEL GOES HERE</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, fine_tune=True):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "\n",
    "        inception = models.inception_v3(pretrained=True, aux_logits=True)\n",
    "        del inception.AuxLogits \n",
    "        # inception.aux_logits = False\n",
    "        # inception.AuxLogits = None\n",
    "\n",
    "        old_conv1 = inception.Conv2d_1a_3x3.conv\n",
    "        new_conv1 = nn.Conv2d(\n",
    "            in_channels=3, \n",
    "            out_channels=old_conv1.out_channels, \n",
    "            kernel_size=old_conv1.kernel_size, \n",
    "            stride=old_conv1.stride, \n",
    "            padding=old_conv1.padding, \n",
    "            bias=old_conv1.bias is not None\n",
    "        )\n",
    "\n",
    "        old_conv2 = inception.Conv2d_2a_3x3.conv\n",
    "        new_conv2 = nn.Conv2d(old_conv1.out_channels, old_conv2.out_channels, old_conv2.kernel_size, old_conv2.stride, old_conv2.padding, bias=old_conv2.bias is not None)\n",
    "\n",
    "        old_conv3 = inception.Conv2d_2b_3x3.conv\n",
    "        new_conv3 = nn.Conv2d(old_conv2.out_channels, old_conv3.out_channels, old_conv3.kernel_size, old_conv3.stride, old_conv3.padding, bias=old_conv3.bias is not None)\n",
    "\n",
    "\n",
    "        # Copy weights for first 3 channels\n",
    "        with torch.no_grad():\n",
    "            new_conv1.weight[:, :3, :, :] = old_conv1.weight.clone()\n",
    "            # new_conv1.weight[:, 3:, :, :] = torch.randn_like(new_conv1.weight[:, 3:, :, :]) * 0.01  \n",
    "            nn.init.kaiming_normal_(new_conv1.weight[:, 3:, :, :], mode='fan_out', nonlinearity='relu')\n",
    "            \n",
    "            if old_conv1.bias is not None:\n",
    "                new_conv1.bias.copy_(old_conv1.bias)\n",
    "\n",
    "        # Replace layers in the model\n",
    "        inception.Conv2d_1a_3x3.conv = new_conv1\n",
    "        inception.Conv2d_2a_3x3.conv = new_conv2\n",
    "        inception.Conv2d_2b_3x3.conv = new_conv3\n",
    "\n",
    "        # self.feature_extractor = nn.Sequential(*list(inception.children())[:-1]) \n",
    "        self.feature_extractor = nn.Sequential(*list(inception.children())[:-2])\n",
    "\n",
    "        # Adaptive pooling to ensure fixed-size output\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))  # Converts (batch, 2048, 8, 8) ‚Üí (batch, 2048, 1, 1)\n",
    "\n",
    "        # Freeze all layers initially\n",
    "        for param in inception.parameters():\n",
    "            param.requires_grad = False  \n",
    "\n",
    "        # Fine-tune specific layers\n",
    "        if fine_tune:\n",
    "            for param in inception.Mixed_7c.parameters():\n",
    "                param.requires_grad = True \n",
    "            for param in inception.Mixed_7b.parameters():\n",
    "                param.requires_grad = True  \n",
    "\n",
    "        for module in self.feature_extractor.modules():\n",
    "            if isinstance(module, nn.BatchNorm2d):\n",
    "                module.eval()  # Fix batch statistics\n",
    "                for param in module.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.pooling(x)  # Make sure output is (batch_size, 2048, 1, 1)\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten to (batch_size, 2048)\n",
    "        return x\n",
    "\n",
    "# Create model instance\n",
    "modified_model = Inception(fine_tune=True)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class InceptionTransformerModel(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=2048, num_heads=8, ff_dim=4096, dropout_rate=0.1):\n",
    "        super(InceptionTransformerModel, self).__init__()\n",
    "\n",
    "        # Inception feature extractor\n",
    "        self.feature_extractor = Inception()\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.transformer_encoder = TransformerEncoder(embed_dim, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)  # (batch, 2048)\n",
    "        x = x.unsqueeze(0)  # Convert to sequence format for Transformer (seq_len=1\n",
    "        \n",
    "        x = x.squeeze(0)  # Back to (batch, embed_dim)\n",
    "        return self.classifier(x)  # Final classification layer\n",
    "\n",
    "num_classes = 3\n",
    "model = InceptionTransformerModel(num_classes)\n",
    "\n",
    "# Print model structure\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Test with demo data & model Summary</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  = InceptionTransformerModel(num_classes) # declare model here\n",
    "randomdata = torch.randn((1,3,224,224))\n",
    "output = model(randomdata)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Training Code</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Fold indices and other configurations\n",
    "num_folds = 5\n",
    "fold_indices = np.random.permutation(np.arange(num_folds))\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "test_folds_chosen = []\n",
    "val_folds_chosen = []\n",
    "fold_confusion_matrices = []  # To store confusion matrices for each fold\n",
    "fold_accuracies = []  # To store balanced accuracy for each fold\n",
    "fold_auprcs = []  # To store AUPRC for each fold\n",
    "\n",
    "# Model and training configurations\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "input_shape = (3,224,224)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_info, csv_path=r\"G:\\CODING\\py\\newnotebooks\\results.csv\"):\n",
    "    \"\"\"\n",
    "    Save all results from the current experiment to a CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    - fold_accuracies: list of balanced accuracy scores for each fold\n",
    "    - fold_auprcs: list of AUPRC scores for each fold\n",
    "    - fold_confusion_matrices: list of confusion matrices for each fold\n",
    "    - model_info: string with model architecture description\n",
    "    - csv_path: path to the CSV file to save results\n",
    "    \"\"\"\n",
    "    # Current time for experiment identification\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare data for the DataFrame\n",
    "    data = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_info\": model_info,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"mean_balanced_accuracy\": np.mean(fold_accuracies),\n",
    "        \"std_balanced_accuracy\": np.std(fold_accuracies),\n",
    "        \"mean_auprc\": np.mean(fold_auprcs),\n",
    "        \"std_auprc\": np.std(fold_auprcs),\n",
    "    }\n",
    "    \n",
    "    # Add individual fold results\n",
    "    for i, (acc, auprc) in enumerate(zip(fold_accuracies, fold_auprcs)):\n",
    "        data[f\"fold_{i+1}_accuracy\"] = acc\n",
    "        data[f\"fold_{i+1}_auprc\"] = auprc\n",
    "    \n",
    "    # Add confusion matrix info\n",
    "    for i, cm in enumerate(fold_confusion_matrices):\n",
    "        data[f\"fold_{i+1}_confusion_matrix\"] = str(cm)\n",
    "    \n",
    "    # Create DataFrame and append to CSV\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if file_exists:\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for cross-validation\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f'Fold No: {fold_idx + 1}')\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    fold_model = InceptionTransformerModel(num_classes) # Initialize your model here\n",
    "    fold_model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "    optimizer = optim.Adam(fold_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    test_fold = fold_indices[fold_idx]\n",
    "    val_fold = val_fold_indices[fold_idx]\n",
    "    train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "    train_data = train_data.transpose(0, 3, 1, 2)  # Transpose to match PyTorch input format\n",
    "\n",
    "    test_folds_chosen.append(test_fold)\n",
    "    val_folds_chosen.append(val_fold)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=2)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(balanced_train_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(balanced_train_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "        \n",
    "        fold_model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fold_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = fold_model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, fold_model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    early_stopping.load_best_model(fold_model)\n",
    "\n",
    "    # Test loop\n",
    "    test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    test_labels = labels_folds[test_fold]\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "        torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    fold_model.eval()\n",
    "    test_probs = []\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = fold_model(test_inputs)\n",
    "            probabilities = torch.softmax(test_outputs, dim=1)\n",
    "            test_probs.extend(probabilities.cpu().numpy())\n",
    "            _, preds = torch.max(probabilities, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_list, test_preds)\n",
    "    fold_confusion_matrices.append(cm)\n",
    "\n",
    "    test_labels_binarized = label_binarize(test_labels_list, classes=np.arange(num_classes))\n",
    "    test_auprcs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_binarized[:, class_idx], np.array(test_probs)[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "    mean_test_auprc = np.mean(test_auprcs)\n",
    "    fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "    print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Use the plot_metrics function to visualize metrics\n",
    "    plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "# Final metrics across all folds\n",
    "average_auprc = np.mean(fold_auprcs)\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "    \n",
    "# Save all results to CSV\n",
    "model_description = f\"INCEPTIONV3-TRANSFORMER normal spectrograms MANUAL APPROACH 3 classes\"\n",
    "save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(255, 0, 0),rgb(131, 207, 207),rgb(255, 84, 84)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.8em; color: white; text-shadow: 1px 1px 3px #000;\">CLASS WEIGHTS APPROACH</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for cross-validation\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f'Fold No: {fold_idx + 1}')\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    fold_model = InceptionTransformerModel(num_classes) # Initialize your model here\n",
    "    fold_model.to(device)\n",
    "    \n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    test_fold = fold_indices[fold_idx]\n",
    "    val_fold = val_fold_indices[fold_idx]\n",
    "    train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "    train_data = train_data.transpose(0, 3, 1, 2)  # Transpose to match PyTorch input format\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    test_folds_chosen.append(test_fold)\n",
    "    val_folds_chosen.append(val_fold)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.3,weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(fold_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(train_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "        \n",
    "        fold_model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fold_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = fold_model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, fold_model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    early_stopping.load_best_model(fold_model)\n",
    "\n",
    "    # Test loop\n",
    "    test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    test_labels = labels_folds[test_fold]\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "        torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    fold_model.eval()\n",
    "    test_probs = []\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = fold_model(test_inputs)\n",
    "            probabilities = torch.softmax(test_outputs, dim=1)\n",
    "            test_probs.extend(probabilities.cpu().numpy())\n",
    "            _, preds = torch.max(probabilities, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_list, test_preds)\n",
    "    fold_confusion_matrices.append(cm)\n",
    "\n",
    "    test_labels_binarized = label_binarize(test_labels_list, classes=np.arange(num_classes))\n",
    "    test_auprcs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_binarized[:, class_idx], np.array(test_probs)[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "    mean_test_auprc = np.mean(test_auprcs)\n",
    "    fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "    print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Use the plot_metrics function to visualize metrics\n",
    "    plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "# Final metrics across all folds\n",
    "average_auprc = np.mean(fold_auprcs)\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "    \n",
    "# Save all results to CSV\n",
    "model_description = f\"INCEPTIONV3-TRANSFORMER NORMAL SPECTROGRAMS CLASS WEIGHTS APPROACH 3 CLASS\"\n",
    "save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
