{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f610267e",
   "metadata": {},
   "source": [
    "<h1> I AM RUNNING ON NORMALIZED DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\data\\numpy\\optunadata\\STFT\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'STFT_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'STFT_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'STFT_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'STFT_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'STFT_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'STFT_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'STFT_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'STFT_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'STFT_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'STFT_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'STFT_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'STFT_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'STFT_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'STFT_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'STFT_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens → 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "├─CustomCnn: 1-1                                             --\n",
       "│    └─Sequential: 2-1                                       --\n",
       "│    │    └─Conv2d: 3-1                                      448\n",
       "│    │    └─BatchNorm2d: 3-2                                 32\n",
       "│    │    └─ReLU: 3-3                                        --\n",
       "│    │    └─Conv2d: 3-4                                      4,640\n",
       "│    │    └─BatchNorm2d: 3-5                                 64\n",
       "│    │    └─ReLU: 3-6                                        --\n",
       "│    │    └─MaxPool2d: 3-7                                   --\n",
       "│    └─Sequential: 2-2                                       --\n",
       "│    │    └─Conv2d: 3-8                                      18,496\n",
       "│    │    └─BatchNorm2d: 3-9                                 128\n",
       "│    │    └─ReLU: 3-10                                       --\n",
       "│    │    └─Conv2d: 3-11                                     73,856\n",
       "│    │    └─BatchNorm2d: 3-12                                256\n",
       "│    │    └─ReLU: 3-13                                       --\n",
       "│    │    └─MaxPool2d: 3-14                                  --\n",
       "│    └─AdaptiveAvgPool2d: 2-3                                --\n",
       "│    └─Flatten: 2-4                                          --\n",
       "├─Linear: 1-2                                                164,160\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─TransformerEncoder: 2-5                               --\n",
       "│    │    └─MultiheadAttention: 3-15                         410,880\n",
       "│    │    └─Sequential: 3-16                                 41,344\n",
       "│    │    └─LayerNorm: 3-17                                  640\n",
       "│    │    └─LayerNorm: 3-18                                  640\n",
       "│    │    └─Dropout: 3-19                                    --\n",
       "│    │    └─Dropout: 3-20                                    --\n",
       "│    └─TransformerEncoder: 2-6                               --\n",
       "│    │    └─MultiheadAttention: 3-21                         410,880\n",
       "│    │    └─Sequential: 3-22                                 41,344\n",
       "│    │    └─LayerNorm: 3-23                                  640\n",
       "│    │    └─LayerNorm: 3-24                                  640\n",
       "│    │    └─Dropout: 3-25                                    --\n",
       "│    │    └─Dropout: 3-26                                    --\n",
       "│    └─TransformerEncoder: 2-7                               --\n",
       "│    │    └─MultiheadAttention: 3-27                         410,880\n",
       "│    │    └─Sequential: 3-28                                 41,344\n",
       "│    │    └─LayerNorm: 3-29                                  640\n",
       "│    │    └─LayerNorm: 3-30                                  640\n",
       "│    │    └─Dropout: 3-31                                    --\n",
       "│    │    └─Dropout: 3-32                                    --\n",
       "│    └─TransformerEncoder: 2-8                               --\n",
       "│    │    └─MultiheadAttention: 3-33                         410,880\n",
       "│    │    └─Sequential: 3-34                                 41,344\n",
       "│    │    └─LayerNorm: 3-35                                  640\n",
       "│    │    └─LayerNorm: 3-36                                  640\n",
       "│    │    └─Dropout: 3-37                                    --\n",
       "│    │    └─Dropout: 3-38                                    --\n",
       "├─AdaptiveAvgPool1d: 1-4                                     --\n",
       "├─Dropout: 1-5                                               --\n",
       "├─Linear: 1-6                                                102,720\n",
       "├─Linear: 1-7                                                102,720\n",
       "├─Linear: 1-8                                                25,680\n",
       "├─Linear: 1-9                                                243\n",
       "=====================================================================================\n",
       "Total params: 2,307,459\n",
       "Trainable params: 2,307,459\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = TRANS_CNN(input_shape=(224,224,20),num_classes=3,num_transformer_blocks=4,embed_dim=320)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (3,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-06-07 03:20:21,084] A new study created in memory with name: no-name-8a41ab70-25b5-4250-be46-9d3a9cb8c565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.002196933770802417, optimizer=Adam, weight_decay=0.00014705767378528348, batch_size=16,factor=1Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=8.669408270891795e-07, optimizer=Adam, weight_decay=3.4221989777190604e-05, batch_size=16,factor=1\n",
      "\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.0008846354118520036, optimizer=Adam, weight_decay=3.971972269808451e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:22:45,973] Trial 0 finished with value: 0.5369047619047619 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.002196933770802417, 'weight_decay': 0.00014705767378528348, 'label_smoothing': 0.25}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0, Fold 1: Test Accuracy = 0.5369\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=9.918335556137119e-05, optimizer=Adam, weight_decay=6.432809343163855e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 2, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:23:59,296] Trial 2 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.0008846354118520036, 'weight_decay': 3.971972269808451e-05, 'label_smoothing': 0.26}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=0.0038043042146537003, optimizer=Adam, weight_decay=1.6640049505496345e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:24:10,593] Trial 1 finished with value: 0.3205486542443064 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 8.669408270891795e-07, 'weight_decay': 3.4221989777190604e-05, 'label_smoothing': 0.01}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Fold 1: Test Accuracy = 0.3205\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.6387567996173093e-05, optimizer=Adam, weight_decay=0.0003994529983324474, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:26:04,767] Trial 3 finished with value: 0.5028381642512078 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 9.918335556137119e-05, 'weight_decay': 6.432809343163855e-05, 'label_smoothing': 0.22}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Fold 1: Test Accuracy = 0.5028\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=1.0642589897510876e-08, optimizer=Adam, weight_decay=0.0002189140007532373, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:26:25,794] Trial 4 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 0.0038043042146537003, 'weight_decay': 1.6640049505496345e-05, 'label_smoothing': 0.2}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=4.036357978549003e-06, optimizer=Adam, weight_decay=0.0001579131078502918, batch_size=16,factor=1\n",
      "Trial 5, Fold 1: Test Accuracy = 0.4782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:27:54,145] Trial 5 finished with value: 0.4781746031746032 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 1.6387567996173093e-05, 'weight_decay': 0.0003994529983324474, 'label_smoothing': 0.24}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=3.2890269536504596e-08, optimizer=Adam, weight_decay=1.0534162877610741e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:29:50,191] Trial 7 finished with value: 0.4390441683919945 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 4.036357978549003e-06, 'weight_decay': 0.0001579131078502918, 'label_smoothing': 0.3}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Fold 1: Test Accuracy = 0.4390\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=1.997091865688598e-05, optimizer=Adam, weight_decay=1.337650926581475e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:30:24,513] Trial 8 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 3.2890269536504596e-08, 'weight_decay': 1.0534162877610741e-05, 'label_smoothing': 0.05}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=2.282071678869887e-05, optimizer=Adam, weight_decay=0.00010503623925347791, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:33:05,361] Trial 6 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 1.0642589897510876e-08, 'weight_decay': 0.0002189140007532373, 'label_smoothing': 0.25}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=5.410485470180011e-05, optimizer=Adam, weight_decay=0.00036874701808493817, batch_size=16,factor=1\n",
      "Trial 9, Fold 1: Test Accuracy = 0.5227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:33:39,435] Trial 9 finished with value: 0.5226621808143548 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 1.997091865688598e-05, 'weight_decay': 1.337650926581475e-05, 'label_smoothing': 0.0}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.009402754110838971, optimizer=Adam, weight_decay=1.097449930008217e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 10, Fold 1: Test Accuracy = 0.5100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:34:02,967] Trial 10 finished with value: 0.5100241545893719 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 2.282071678869887e-05, 'weight_decay': 0.00010503623925347791, 'label_smoothing': 0.0}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.009291124833456316, optimizer=Adam, weight_decay=1.2213812436475636e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:35:24,894] Trial 11 finished with value: 0.5107746721877157 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 5.410485470180011e-05, 'weight_decay': 0.00036874701808493817, 'label_smoothing': 0.19}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11, Fold 1: Test Accuracy = 0.5108\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0005644029021658649, optimizer=Adam, weight_decay=1.7954813496540649e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 12, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:35:32,435] Trial 12 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.009402754110838971, 'weight_decay': 1.097449930008217e-06, 'label_smoothing': 0.13}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.0003590314540472598, optimizer=Adam, weight_decay=4.436016518138766e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 13, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:35:41,000] Trial 13 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.009291124833456316, 'weight_decay': 1.2213812436475636e-06, 'label_smoothing': 0.11}. Best is trial 0 with value: 0.5369047619047619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.0004230293430513748, optimizer=Adam, weight_decay=3.5929348863270323e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 14, Fold 1: Test Accuracy = 0.5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:37:26,823] Trial 14 finished with value: 0.5749913733609385 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0005644029021658649, 'weight_decay': 1.7954813496540649e-06, 'label_smoothing': 0.12}. Best is trial 14 with value: 0.5749913733609385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.00041949310721916476, optimizer=Adam, weight_decay=3.705654389021501e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 16, Fold 1: Test Accuracy = 0.5476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:38:03,693] Trial 16 finished with value: 0.5476276742581091 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.0004230293430513748, 'weight_decay': 3.5929348863270323e-06, 'label_smoothing': 0.09}. Best is trial 14 with value: 0.5749913733609385.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.000564182605881582, optimizer=Adam, weight_decay=3.7804820696614333e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:38:14,815] Trial 15 finished with value: 0.5879399585921324 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.0003590314540472598, 'weight_decay': 4.436016518138766e-06, 'label_smoothing': 0.11}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15, Fold 1: Test Accuracy = 0.5879\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.00020964489011621865, optimizer=Adam, weight_decay=3.6627889715098463e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 17, Fold 1: Test Accuracy = 0.5607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:39:29,155] Trial 17 finished with value: 0.5607142857142858 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.00041949310721916476, 'weight_decay': 3.705654389021501e-06, 'label_smoothing': 0.09}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.00014281065872193328, optimizer=Adam, weight_decay=3.481487248810668e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 18, Fold 1: Test Accuracy = 0.5024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:40:04,331] Trial 18 finished with value: 0.502372325741891 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.000564182605881582, 'weight_decay': 3.7804820696614333e-06, 'label_smoothing': 0.08}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00014762427100203842, optimizer=Adam, weight_decay=6.824422569625226e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 19, Fold 1: Test Accuracy = 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:41:15,894] Trial 19 finished with value: 0.5575828157349897 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.00020964489011621865, 'weight_decay': 3.6627889715098463e-06, 'label_smoothing': 0.16}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=16, learning_rate=8.032096627685236e-07, optimizer=Adam, weight_decay=2.043993835068582e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:41:41,778] Trial 20 finished with value: 0.5476104209799862 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.00014281065872193328, 'weight_decay': 3.481487248810668e-06, 'label_smoothing': 0.17}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20, Fold 1: Test Accuracy = 0.5476\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.0012458390745860453, optimizer=Adam, weight_decay=7.387330650283827e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:43:14,997] Trial 21 finished with value: 0.5525534851621808 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00014762427100203842, 'weight_decay': 6.824422569625226e-06, 'label_smoothing': 0.15}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21, Fold 1: Test Accuracy = 0.5526\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=2.226568715227034e-06, optimizer=Adam, weight_decay=1.7988974752442063e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 22, Fold 1: Test Accuracy = 0.3478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:43:23,493] Trial 22 finished with value: 0.3478088336783989 and parameters: {'num_heads': 4, 'num_transformer_blocks': 16, 'learning_rate': 8.032096627685236e-07, 'weight_decay': 2.043993835068582e-06, 'label_smoothing': 0.16}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.0019439733551506592, optimizer=Adam, weight_decay=1.9015638887461331e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 23, Fold 1: Test Accuracy = 0.5008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:44:02,060] Trial 23 finished with value: 0.5008454106280193 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.0012458390745860453, 'weight_decay': 7.387330650283827e-06, 'label_smoothing': 0.05}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=5.628692751817774e-06, optimizer=Adam, weight_decay=2.006763292239556e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 24, Fold 1: Test Accuracy = 0.3218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:44:48,660] Trial 24 finished with value: 0.321808143547274 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 2.226568715227034e-06, 'weight_decay': 1.7988974752442063e-06, 'label_smoothing': 0.05}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.00042504501659842704, optimizer=Adam, weight_decay=2.078952919710097e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:45:31,264] Trial 25 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.0019439733551506592, 'weight_decay': 1.9015638887461331e-06, 'label_smoothing': 0.05}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0004961398716922372, optimizer=Adam, weight_decay=0.000988378869717866, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 27, Fold 1: Test Accuracy = 0.5782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:46:55,537] Trial 27 finished with value: 0.5781659765355417 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.00042504501659842704, 'weight_decay': 2.078952919710097e-05, 'label_smoothing': 0.12}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=5.156382655070984e-05, optimizer=Adam, weight_decay=1.6296723393160516e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:48:07,324] Trial 26 finished with value: 0.4655969634230504 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 5.628692751817774e-06, 'weight_decay': 2.006763292239556e-06, 'label_smoothing': 0.12}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26, Fold 1: Test Accuracy = 0.4656\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=6.340550961185137e-05, optimizer=Adam, weight_decay=1.738792026169847e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 28, Fold 1: Test Accuracy = 0.5866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:48:16,877] Trial 28 finished with value: 0.5866459627329192 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0004961398716922372, 'weight_decay': 0.000988378869717866, 'label_smoothing': 0.12}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=5.085012200320637e-05, optimizer=Adam, weight_decay=0.0006529275524267692, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:49:16,740] Trial 29 finished with value: 0.5221187025534851 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 5.156382655070984e-05, 'weight_decay': 1.6296723393160516e-05, 'label_smoothing': 0.12}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29, Fold 1: Test Accuracy = 0.5221\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=0.003082197594378404, optimizer=Adam, weight_decay=0.0006551151258517576, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:50:26,074] Trial 30 finished with value: 0.51074016563147 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 6.340550961185137e-05, 'weight_decay': 1.738792026169847e-05, 'label_smoothing': 0.13}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30, Fold 1: Test Accuracy = 0.5107\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0024688213598699884, optimizer=Adam, weight_decay=2.487823924992503e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:51:54,275] Trial 33 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0024688213598699884, 'weight_decay': 2.487823924992503e-05, 'label_smoothing': 0.08}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0002850701467255021, optimizer=Adam, weight_decay=0.0009773517427519572, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:52:10,745] Trial 32 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 0.003082197594378404, 'weight_decay': 0.0006551151258517576, 'label_smoothing': 0.14}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0003770236053477921, optimizer=Adam, weight_decay=4.113190848762365e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:52:38,691] Trial 31 finished with value: 0.5479554865424431 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 5.085012200320637e-05, 'weight_decay': 0.0006529275524267692, 'label_smoothing': 0.07}. Best is trial 15 with value: 0.5879399585921324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31, Fold 1: Test Accuracy = 0.5480\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.00035553611922423275, optimizer=Adam, weight_decay=4.708320015876209e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:53:36,305] Trial 34 finished with value: 0.6118616287094548 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0002850701467255021, 'weight_decay': 0.0009773517427519572, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34, Fold 1: Test Accuracy = 0.6119\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0002766104192159226, optimizer=Adam, weight_decay=0.0009355824849813596, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 35, Fold 1: Test Accuracy = 0.5599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:53:52,644] Trial 35 finished with value: 0.5599378881987578 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0003770236053477921, 'weight_decay': 4.113190848762365e-05, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0010719128869866538, optimizer=Adam, weight_decay=0.0008373757542341069, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:54:51,397] Trial 36 finished with value: 0.5754399585921325 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.00035553611922423275, 'weight_decay': 4.708320015876209e-05, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36, Fold 1: Test Accuracy = 0.5754\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0012113149113729216, optimizer=Adam, weight_decay=8.052329066574114e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:55:40,013] Trial 37 finished with value: 0.5409851621808144 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0002766104192159226, 'weight_decay': 0.0009355824849813596, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37, Fold 1: Test Accuracy = 0.5410\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=0.0010895253658943058, optimizer=Adam, weight_decay=0.0002793700947804259, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:55:48,546] Trial 38 finished with value: 0.5482660455486542 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0010719128869866538, 'weight_decay': 0.0008373757542341069, 'label_smoothing': 0.07}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38, Fold 1: Test Accuracy = 0.5483\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=2.3437052345013164e-07, optimizer=Adam, weight_decay=0.000317478065045504, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:56:56,212] Trial 39 finished with value: 0.6028985507246377 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0012113149113729216, 'weight_decay': 8.052329066574114e-05, 'label_smoothing': 0.18}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39, Fold 1: Test Accuracy = 0.6029\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.005273273524437187, optimizer=Adam, weight_decay=0.0002834391183132102, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:57:51,788] Trial 40 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 0.0010895253658943058, 'weight_decay': 0.0002793700947804259, 'label_smoothing': 0.03}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.00010006448070595301, optimizer=Adam, weight_decay=7.810288021190653e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:58:21,505] Trial 42 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.005273273524437187, 'weight_decay': 0.0002834391183132102, 'label_smoothing': 0.21}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.00012101883543775915, optimizer=Adam, weight_decay=7.589675101073069e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 03:58:33,373] Trial 41 finished with value: 0.28915631469979297 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 2.3437052345013164e-07, 'weight_decay': 0.000317478065045504, 'label_smoothing': 0.19}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41, Fold 1: Test Accuracy = 0.2892\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=9.111205286994366e-05, optimizer=Adam, weight_decay=0.00011441692669912339, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:00:08,628] Trial 43 finished with value: 0.5041062801932368 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.00010006448070595301, 'weight_decay': 7.810288021190653e-05, 'label_smoothing': 0.19}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43, Fold 1: Test Accuracy = 0.5041\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0015797472366093715, optimizer=Adam, weight_decay=0.00013056879229514986, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:01:26,624] Trial 45 finished with value: 0.5541839199447894 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 9.111205286994366e-05, 'weight_decay': 0.00011441692669912339, 'label_smoothing': 0.18}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45, Fold 1: Test Accuracy = 0.5542\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0016411479279561175, optimizer=Adam, weight_decay=0.000499048292285542, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:01:34,278] Trial 44 finished with value: 0.5687888198757763 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.00012101883543775915, 'weight_decay': 7.589675101073069e-05, 'label_smoothing': 0.18}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44, Fold 1: Test Accuracy = 0.5688\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0016027088091020924, optimizer=Adam, weight_decay=0.0005793660417232437, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:02:38,452] Trial 46 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0015797472366093715, 'weight_decay': 0.00013056879229514986, 'label_smoothing': 0.17}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.004556165938009941, optimizer=Adam, weight_decay=0.0005104532914537137, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:04:39,051] Trial 47 finished with value: 0.5686766735679779 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0016411479279561175, 'weight_decay': 0.000499048292285542, 'label_smoothing': 0.23}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47, Fold 1: Test Accuracy = 0.5687\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.005554460706255462, optimizer=Adam, weight_decay=0.00017228974072002658, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:04:59,695] Trial 48 finished with value: 0.6074534161490683 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0016027088091020924, 'weight_decay': 0.0005793660417232437, 'label_smoothing': 0.23}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48, Fold 1: Test Accuracy = 0.6075\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.004503952402515313, optimizer=Adam, weight_decay=0.00019387123770776775, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 49, Fold 1: Test Accuracy = 0.5644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:06:04,576] Trial 49 finished with value: 0.5643978605935128 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.004556165938009941, 'weight_decay': 0.0005104532914537137, 'label_smoothing': 0.22}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0009717468885202335, optimizer=Adam, weight_decay=0.00019960048875707157, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:06:14,291] Trial 50 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.005554460706255462, 'weight_decay': 0.00017228974072002658, 'label_smoothing': 0.14}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0006750851107437723, optimizer=Adam, weight_decay=0.0009599759058827069, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 51, Fold 1: Test Accuracy = 0.5630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:06:59,565] Trial 51 finished with value: 0.5629917184265011 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.004503952402515313, 'weight_decay': 0.00019387123770776775, 'label_smoothing': 0.29}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0008924729479663604, optimizer=Adam, weight_decay=2.519083672994837e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:08:29,405] Trial 53 finished with value: 0.5711093857832988 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0006750851107437723, 'weight_decay': 0.0009599759058827069, 'label_smoothing': 0.28}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53, Fold 1: Test Accuracy = 0.5711\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=0.000702020796401536, optimizer=Adam, weight_decay=0.00045166747356197126, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:08:40,374] Trial 52 finished with value: 0.5723861283643893 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0009717468885202335, 'weight_decay': 0.00019960048875707157, 'label_smoothing': 0.28}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52, Fold 1: Test Accuracy = 0.5724\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=32, learning_rate=0.00025975133280326944, optimizer=Adam, weight_decay=0.0004408756816541176, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:09:43,753] Trial 54 finished with value: 0.5644409937888198 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0008924729479663604, 'weight_decay': 2.519083672994837e-05, 'label_smoothing': 0.28}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54, Fold 1: Test Accuracy = 0.5644\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=32, learning_rate=2.898045909375708e-05, optimizer=Adam, weight_decay=0.000458144481524508, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:11:00,872] Trial 56 finished with value: 0.35962732919254664 and parameters: {'num_heads': 8, 'num_transformer_blocks': 32, 'learning_rate': 0.00025975133280326944, 'weight_decay': 0.0004408756816541176, 'label_smoothing': 0.25}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 56, Fold 1: Test Accuracy = 0.3596\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=2.9794647360037446e-05, optimizer=Adam, weight_decay=0.000648853311788361, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:13:02,221] Trial 55 finished with value: 0.4791407867494824 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 0.000702020796401536, 'weight_decay': 0.00045166747356197126, 'label_smoothing': 0.25}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 55, Fold 1: Test Accuracy = 0.4791\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=2.5757551024318328e-05, optimizer=Adam, weight_decay=0.0006825762242840879, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:14:27,506] Trial 58 finished with value: 0.5192028985507245 and parameters: {'num_heads': 32, 'num_transformer_blocks': 4, 'learning_rate': 2.9794647360037446e-05, 'weight_decay': 0.000648853311788361, 'label_smoothing': 0.11}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 58, Fold 1: Test Accuracy = 0.5192\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0025350966327799276, optimizer=Adam, weight_decay=5.533552488273726e-06, batch_size=16,factor=1\n",
      "Trial 57, Fold 1: Test Accuracy = 0.5749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:16:05,960] Trial 57 finished with value: 0.574896480331263 and parameters: {'num_heads': 8, 'num_transformer_blocks': 32, 'learning_rate': 2.898045909375708e-05, 'weight_decay': 0.000458144481524508, 'label_smoothing': 0.25}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=0.00019304902847804082, optimizer=Adam, weight_decay=5.5166937620730906e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 59, Fold 1: Test Accuracy = 0.5113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:16:54,965] Trial 59 finished with value: 0.5112922705314009 and parameters: {'num_heads': 32, 'num_transformer_blocks': 4, 'learning_rate': 2.5757551024318328e-05, 'weight_decay': 0.0006825762242840879, 'label_smoothing': 0.11}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=0.00019026876418548218, optimizer=Adam, weight_decay=5.293584664959522e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:17:02,248] Trial 60 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0025350966327799276, 'weight_decay': 5.533552488273726e-06, 'label_smoothing': 0.15}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0002360790859669477, optimizer=Adam, weight_decay=4.751914601620292e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:17:57,841] Trial 61 finished with value: 0.5624482401656316 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 0.00019304902847804082, 'weight_decay': 5.5166937620730906e-06, 'label_smoothing': 0.15}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 61, Fold 1: Test Accuracy = 0.5624\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0003702299514840808, optimizer=Adam, weight_decay=4.1162653148658154e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 63, Fold 1: Test Accuracy = 0.5215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:19:00,151] Trial 63 finished with value: 0.5214889579020013 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0002360790859669477, 'weight_decay': 4.751914601620292e-05, 'label_smoothing': 0.09}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0003955574842703238, optimizer=Adam, weight_decay=5.797035867079819e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:19:30,044] Trial 64 finished with value: 0.5464113181504485 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0003702299514840808, 'weight_decay': 4.1162653148658154e-05, 'label_smoothing': 0.13}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 64, Fold 1: Test Accuracy = 0.5464\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0004579298071287775, optimizer=Adam, weight_decay=5.6222613606728604e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 62, Fold 1: Test Accuracy = 0.5235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:20:59,644] Trial 62 finished with value: 0.5234558316080055 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 0.00019026876418548218, 'weight_decay': 5.293584664959522e-06, 'label_smoothing': 0.15}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.00044600103529455343, optimizer=Adam, weight_decay=1.172020136613905e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:21:01,624] Trial 65 finished with value: 0.6025879917184266 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0003955574842703238, 'weight_decay': 5.797035867079819e-05, 'label_smoothing': 0.13}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 65, Fold 1: Test Accuracy = 0.6026\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.00046744400834229324, optimizer=Adam, weight_decay=5.915913796331742e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 66, Fold 1: Test Accuracy = 0.5704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:21:17,514] Trial 66 finished with value: 0.5703761214630779 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0004579298071287775, 'weight_decay': 5.6222613606728604e-05, 'label_smoothing': 0.07}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.008330494828181509, optimizer=Adam, weight_decay=1.1052539811883756e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:23:06,399] Trial 68 finished with value: 0.5150879917184265 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.00046744400834229324, 'weight_decay': 5.915913796331742e-05, 'label_smoothing': 0.13}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 68, Fold 1: Test Accuracy = 0.5151\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=1.0538848453623493e-05, optimizer=Adam, weight_decay=9.47783516125397e-06, batch_size=16,factor=1\n",
      "Trial 67, Fold 1: Test Accuracy = 0.5602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:23:07,454] Trial 67 finished with value: 0.5602311939268462 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.00044600103529455343, 'weight_decay': 1.172020136613905e-05, 'label_smoothing': 0.07}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=1.0889140369246654e-05, optimizer=Adam, weight_decay=3.361433641347365e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:23:42,902] Trial 69 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.008330494828181509, 'weight_decay': 1.1052539811883756e-05, 'label_smoothing': 0.12}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 69, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.001341440939281883, optimizer=Adam, weight_decay=3.075875814692891e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 72, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:25:21,410] Trial 72 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.001341440939281883, 'weight_decay': 3.075875814692891e-05, 'label_smoothing': 0.11}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0006143905285063188, optimizer=Adam, weight_decay=2.9375746751647605e-05, batch_size=16,factor=1\n",
      "Trial 70, Fold 1: Test Accuracy = 0.5127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:27:04,021] Trial 70 finished with value: 0.5127242926155969 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 1.0538848453623493e-05, 'weight_decay': 9.47783516125397e-06, 'label_smoothing': 0.14}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=7.630496284641564e-05, optimizer=Adam, weight_decay=9.587380295967513e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 73, Fold 1: Test Accuracy = 0.5530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:27:10,459] Trial 73 finished with value: 0.5529848171152519 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0006143905285063188, 'weight_decay': 2.9375746751647605e-05, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=7.834690823374799e-05, optimizer=Adam, weight_decay=9.214240815263158e-05, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:27:35,133] Trial 71 finished with value: 0.5025448585231194 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 1.0889140369246654e-05, 'weight_decay': 3.361433641347365e-05, 'label_smoothing': 0.12}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 71, Fold 1: Test Accuracy = 0.5025\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=7.518972281097079e-05, optimizer=Adam, weight_decay=7.627531534728272e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 74, Fold 1: Test Accuracy = 0.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:30:03,087] Trial 74 finished with value: 0.5739475500345065 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 7.630496284641564e-05, 'weight_decay': 9.587380295967513e-05, 'label_smoothing': 0.1}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0019559961704302955, optimizer=Adam, weight_decay=0.0007819523424665492, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 75, Fold 1: Test Accuracy = 0.5184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:30:09,669] Trial 75 finished with value: 0.5184351276742581 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 7.834690823374799e-05, 'weight_decay': 9.214240815263158e-05, 'label_smoothing': 0.09}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=0.000320646756303432, optimizer=Adam, weight_decay=0.0007444021356146928, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:30:36,963] Trial 76 finished with value: 0.550388198757764 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 7.518972281097079e-05, 'weight_decay': 7.627531534728272e-05, 'label_smoothing': 0.09}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 76, Fold 1: Test Accuracy = 0.5504\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=0.0020060938211906416, optimizer=Adam, weight_decay=1.9179602088318127e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:32:16,416] Trial 77 finished with value: 0.5781400966183575 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0019559961704302955, 'weight_decay': 0.0007819523424665492, 'label_smoothing': 0.08}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 77, Fold 1: Test Accuracy = 0.5781\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0018973148891251617, optimizer=Adam, weight_decay=0.0007383850556356687, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:32:27,846] Trial 79 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 0.0020060938211906416, 'weight_decay': 1.9179602088318127e-05, 'label_smoothing': 0.08}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 79, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003103180718741511, optimizer=Adam, weight_decay=0.0007777848938983311, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:32:55,681] Trial 78 finished with value: 0.5206176673567978 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 0.000320646756303432, 'weight_decay': 0.0007444021356146928, 'label_smoothing': 0.14}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 78, Fold 1: Test Accuracy = 0.5206\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003099012481348034, optimizer=Adam, weight_decay=0.0005897413494887618, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:34:52,921] Trial 81 finished with value: 0.5885179434092477 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003103180718741511, 'weight_decay': 0.0007777848938983311, 'label_smoothing': 0.06}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 81, Fold 1: Test Accuracy = 0.5885\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.006888715984126817, optimizer=Adam, weight_decay=0.0007850626689798364, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:34:54,822] Trial 80 finished with value: 0.5894064872325742 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0018973148891251617, 'weight_decay': 0.0007383850556356687, 'label_smoothing': 0.06}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 80, Fold 1: Test Accuracy = 0.5894\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003311522961194348, optimizer=Adam, weight_decay=0.0005650200598864694, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:35:26,286] Trial 82 finished with value: 0.33961352657004834 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003099012481348034, 'weight_decay': 0.0005897413494887618, 'label_smoothing': 0.06}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 82, Fold 1: Test Accuracy = 0.3396\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0007971606155532714, optimizer=Adam, weight_decay=0.00033776763231264934, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:36:36,980] Trial 83 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.006888715984126817, 'weight_decay': 0.0007850626689798364, 'label_smoothing': 0.06}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 83, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0008196533938204881, optimizer=Adam, weight_decay=0.0009674378890685176, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:37:25,173] Trial 85 finished with value: 0.5726535541752933 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0007971606155532714, 'weight_decay': 0.00033776763231264934, 'label_smoothing': 0.01}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 85, Fold 1: Test Accuracy = 0.5727\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003052974333977749, optimizer=Adam, weight_decay=2.7350446809073175e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:39:04,128] Trial 87 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003052974333977749, 'weight_decay': 2.7350446809073175e-06, 'label_smoothing': 0.03}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 87, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0013696721515052846, optimizer=Adam, weight_decay=0.0009676440016847239, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:39:06,199] Trial 84 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003311522961194348, 'weight_decay': 0.0005650200598864694, 'label_smoothing': 0.04}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 84, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0014330672372184856, optimizer=Adam, weight_decay=0.0009301752830030605, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:40:03,934] Trial 86 finished with value: 0.6113612836438923 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0008196533938204881, 'weight_decay': 0.0009674378890685176, 'label_smoothing': 0.16}. Best is trial 34 with value: 0.6118616287094548.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 86, Fold 1: Test Accuracy = 0.6114\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0011918903172181614, optimizer=Adam, weight_decay=0.0008675128652544475, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:41:44,565] Trial 88 finished with value: 0.6231193926846101 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0013696721515052846, 'weight_decay': 0.0009676440016847239, 'label_smoothing': 0.04}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 88, Fold 1: Test Accuracy = 0.6231\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.001788339418467752, optimizer=Adam, weight_decay=0.0003954872839987908, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:42:02,018] Trial 89 finished with value: 0.572420634920635 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0014330672372184856, 'weight_decay': 0.0009301752830030605, 'label_smoothing': 0.16}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 89, Fold 1: Test Accuracy = 0.5724\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0009370278022122844, optimizer=Adam, weight_decay=0.0002565007958395215, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:42:57,456] Trial 90 finished with value: 0.582876121463078 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0011918903172181614, 'weight_decay': 0.0008675128652544475, 'label_smoothing': 0.16}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90, Fold 1: Test Accuracy = 0.5829\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0008620862768958992, optimizer=Adam, weight_decay=0.0002480151574183652, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:44:10,743] Trial 92 finished with value: 0.5867753623188406 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0009370278022122844, 'weight_decay': 0.0002565007958395215, 'label_smoothing': 0.01}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 92, Fold 1: Test Accuracy = 0.5868\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0007600000630185462, optimizer=Adam, weight_decay=0.0003934872705890018, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:44:54,477] Trial 91 finished with value: 0.5093081435472739 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.001788339418467752, 'weight_decay': 0.0003954872839987908, 'label_smoothing': 0.01}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 91, Fold 1: Test Accuracy = 0.5093\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0008444256470758107, optimizer=Adam, weight_decay=0.0002463546124930509, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:45:31,437] Trial 93 finished with value: 0.5767080745341615 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0008620862768958992, 'weight_decay': 0.0002480151574183652, 'label_smoothing': 0.02}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 93, Fold 1: Test Accuracy = 0.5767\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=3.411040949372564e-08, optimizer=Adam, weight_decay=0.0005161234630801609, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:46:20,906] Trial 94 finished with value: 0.5451604554865425 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0007600000630185462, 'weight_decay': 0.0003934872705890018, 'label_smoothing': 0.01}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 94, Fold 1: Test Accuracy = 0.5452\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.004128186711992572, optimizer=Adam, weight_decay=0.0005394281562766647, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:46:42,613] Trial 95 finished with value: 0.5456866804692891 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0008444256470758107, 'weight_decay': 0.0002463546124930509, 'label_smoothing': 0.03}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 95, Fold 1: Test Accuracy = 0.5457\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.004158375843827244, optimizer=Adam, weight_decay=0.000549331813267271, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:49:10,585] Trial 97 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.004128186711992572, 'weight_decay': 0.0005394281562766647, 'label_smoothing': 0.04}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 97, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=2.409101702901767e-08, optimizer=Adam, weight_decay=0.0007039295434497356, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:49:50,329] Trial 96 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 3.411040949372564e-08, 'weight_decay': 0.0005161234630801609, 'label_smoothing': 0.0}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 96, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0022845972079059956, optimizer=Adam, weight_decay=0.000602692326677612, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:49:57,046] Trial 98 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.004158375843827244, 'weight_decay': 0.000549331813267271, 'label_smoothing': 0.04}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 98, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0024163364650114077, optimizer=Adam, weight_decay=0.00014454537741463662, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 101, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:52:11,239] Trial 101 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0024163364650114077, 'weight_decay': 0.00014454537741463662, 'label_smoothing': 0.06}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00015659602009353686, optimizer=Adam, weight_decay=0.0006947116536473927, batch_size=16,factor=1\n",
      "Trial 99, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:53:27,786] Trial 99 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 2.409101702901767e-08, 'weight_decay': 0.0007039295434497356, 'label_smoothing': 0.04}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00014314852666349136, optimizer=Adam, weight_decay=0.0009605283760649961, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:54:06,194] Trial 100 finished with value: 0.592883022774327 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0022845972079059956, 'weight_decay': 0.000602692326677612, 'label_smoothing': 0.21}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 100, Fold 1: Test Accuracy = 0.5929\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0011948017238790677, optimizer=Adam, weight_decay=0.0006255351574081472, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:55:27,763] Trial 102 finished with value: 0.5688492063492064 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00015659602009353686, 'weight_decay': 0.0006947116536473927, 'label_smoothing': 0.17}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 102, Fold 1: Test Accuracy = 0.5688\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0011496782892071033, optimizer=Adam, weight_decay=0.000990892331425129, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:55:29,573] Trial 103 finished with value: 0.5342391304347825 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00014314852666349136, 'weight_decay': 0.0009605283760649961, 'label_smoothing': 0.13}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 103, Fold 1: Test Accuracy = 0.5342\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.006246098983647171, optimizer=Adam, weight_decay=0.0006216306098361402, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:56:43,084] Trial 104 finished with value: 0.5835144927536232 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0011948017238790677, 'weight_decay': 0.0006255351574081472, 'label_smoothing': 0.22}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 104, Fold 1: Test Accuracy = 0.5835\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=0.006283774855701247, optimizer=Adam, weight_decay=1.4072389848906073e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:57:36,682] Trial 106 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.006246098983647171, 'weight_decay': 0.0006216306098361402, 'label_smoothing': 0.21}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 106, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=0.002375204889612184, optimizer=Adam, weight_decay=0.0008305960357720593, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 105, Fold 1: Test Accuracy = 0.5409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:58:04,706] Trial 105 finished with value: 0.5409161490683231 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0011496782892071033, 'weight_decay': 0.000990892331425129, 'label_smoothing': 0.21}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=0.002434686744462289, optimizer=Adam, weight_decay=0.0008130135940309657, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 107, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:58:30,429] Trial 107 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 0.006283774855701247, 'weight_decay': 1.4072389848906073e-06, 'label_smoothing': 0.2}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0005740396891375217, optimizer=Adam, weight_decay=0.0007863640188002474, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 108, Fold 1: Test Accuracy = 0.5068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:59:26,906] Trial 108 finished with value: 0.506815044858523 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 0.002375204889612184, 'weight_decay': 0.0008305960357720593, 'label_smoothing': 0.2}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0015600659068157766, optimizer=Adam, weight_decay=0.00035614538936708066, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 109, Fold 1: Test Accuracy = 0.5654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 04:59:46,563] Trial 109 finished with value: 0.565407177363699 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 0.002434686744462289, 'weight_decay': 0.0008130135940309657, 'label_smoothing': 0.2}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.000589710153150947, optimizer=Adam, weight_decay=0.0004369599074681486, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:01:42,482] Trial 110 finished with value: 0.6148982056590753 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0005740396891375217, 'weight_decay': 0.0007863640188002474, 'label_smoothing': 0.23}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 110, Fold 1: Test Accuracy = 0.6149\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0005234880794064547, optimizer=Adam, weight_decay=0.0003416678897953202, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 112, Fold 1: Test Accuracy = 0.5619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:02:15,852] Trial 112 finished with value: 0.5618702553485163 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.000589710153150947, 'weight_decay': 0.0004369599074681486, 'label_smoothing': 0.23}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0003058664710734525, optimizer=Adam, weight_decay=0.0007479094338838283, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 111, Fold 1: Test Accuracy = 0.5875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:02:25,857] Trial 111 finished with value: 0.5875345065562456 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0015600659068157766, 'weight_decay': 0.00035614538936708066, 'label_smoothing': 0.18}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0014268683384269449, optimizer=Adam, weight_decay=0.0003463319958859285, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 113, Fold 1: Test Accuracy = 0.5736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:04:37,463] Trial 113 finished with value: 0.5736024844720496 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0005234880794064547, 'weight_decay': 0.0003416678897953202, 'label_smoothing': 0.24}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0017656653634734094, optimizer=Adam, weight_decay=0.0007498658722780421, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 114, Fold 1: Test Accuracy = 0.5676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:05:38,800] Trial 114 finished with value: 0.5675552104899931 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0003058664710734525, 'weight_decay': 0.0007479094338838283, 'label_smoothing': 0.23}. Best is trial 88 with value: 0.6231193926846101.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0016211494516521823, optimizer=Adam, weight_decay=0.0008678244947756975, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 115, Fold 1: Test Accuracy = 0.6363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:06:06,617] Trial 115 finished with value: 0.6363181504485853 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0014268683384269449, 'weight_decay': 0.0003463319958859285, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.001643384009125399, optimizer=Adam, weight_decay=0.00048037046098270306, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:09:13,229] Trial 117 finished with value: 0.5787957211870255 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0016211494516521823, 'weight_decay': 0.0008678244947756975, 'label_smoothing': 0.17}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 117, Fold 1: Test Accuracy = 0.5788\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0019719204727131952, optimizer=Adam, weight_decay=0.00047404572779945, batch_size=16,factor=1\n",
      "Trial 116, Fold 1: Test Accuracy = 0.5360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:09:13,378] Trial 116 finished with value: 0.5360334713595583 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0017656653634734094, 'weight_decay': 0.0007498658722780421, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=0.0006339921747973958, optimizer=Adam, weight_decay=0.00048249846894735733, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:10:50,956] Trial 119 finished with value: 0.4282177363699103 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0019719204727131952, 'weight_decay': 0.00047404572779945, 'label_smoothing': 0.22}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 119, Fold 1: Test Accuracy = 0.4282\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=1.4348022824973814e-06, optimizer=Adam, weight_decay=0.0006055358638278887, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:10:52,135] Trial 118 finished with value: 0.5499309868875086 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.001643384009125399, 'weight_decay': 0.00048037046098270306, 'label_smoothing': 0.19}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 118, Fold 1: Test Accuracy = 0.5499\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=2.660216998578444e-06, optimizer=Adam, weight_decay=0.000632042696549062, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:13:19,336] Trial 121 finished with value: 0.3161922015182885 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 1.4348022824973814e-06, 'weight_decay': 0.0006055358638278887, 'label_smoothing': 0.19}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 121, Fold 1: Test Accuracy = 0.3162\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0032115355739763267, optimizer=Adam, weight_decay=0.0006884505290687934, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:15:29,570] Trial 120 finished with value: 0.557755348516218 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 0.0006339921747973958, 'weight_decay': 0.00048249846894735733, 'label_smoothing': 0.24}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 120, Fold 1: Test Accuracy = 0.5578\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.003231315098630607, optimizer=Adam, weight_decay=0.0009987028852522152, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:15:30,717] Trial 123 finished with value: 0.5181849551414769 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0032115355739763267, 'weight_decay': 0.0006884505290687934, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 123, Fold 1: Test Accuracy = 0.5182\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0013510400186761708, optimizer=Adam, weight_decay=0.0003816957003556357, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:17:07,257] Trial 124 finished with value: 0.48715493443754315 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.003231315098630607, 'weight_decay': 0.0009987028852522152, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 124, Fold 1: Test Accuracy = 0.4872\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0010310736291360611, optimizer=Adam, weight_decay=0.0002847020262072877, batch_size=16,factor=1\n",
      "Trial 122, Fold 1: Test Accuracy = 0.3568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:18:47,256] Trial 122 finished with value: 0.356832298136646 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 2.660216998578444e-06, 'weight_decay': 0.000632042696549062, 'label_smoothing': 0.26}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.001339432999303212, optimizer=Adam, weight_decay=0.00037287372215286, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:19:17,857] Trial 126 finished with value: 0.5711093857832988 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0010310736291360611, 'weight_decay': 0.0002847020262072877, 'label_smoothing': 0.24}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 126, Fold 1: Test Accuracy = 0.5711\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0003818874213221449, optimizer=Adam, weight_decay=0.0003960814682895306, batch_size=16,factor=1\n",
      "Trial 125, Fold 1: Test Accuracy = 0.5806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:19:37,978] Trial 125 finished with value: 0.5805728088336785 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0013510400186761708, 'weight_decay': 0.0003816957003556357, 'label_smoothing': 0.26}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.009410653461575625, optimizer=Adam, weight_decay=0.00012028062615513761, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:21:16,776] Trial 129 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.009410653461575625, 'weight_decay': 0.00012028062615513761, 'label_smoothing': 0.16}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 129, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00021757725523916479, optimizer=Adam, weight_decay=1.4317552701093831e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:21:39,673] Trial 128 finished with value: 0.5104727398205658 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0003818874213221449, 'weight_decay': 0.0003960814682895306, 'label_smoothing': 0.05}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 128, Fold 1: Test Accuracy = 0.5105\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=16, learning_rate=0.00023597681004269668, optimizer=Adam, weight_decay=0.0008726976596023714, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:23:06,058] Trial 127 finished with value: 0.533756038647343 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.001339432999303212, 'weight_decay': 0.00037287372215286, 'label_smoothing': 0.22}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 127, Fold 1: Test Accuracy = 0.5338\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=16, learning_rate=0.0007388076540564359, optimizer=Adam, weight_decay=6.567484084171686e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:23:14,714] Trial 130 finished with value: 0.5479554865424431 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00021757725523916479, 'weight_decay': 1.4317552701093831e-05, 'label_smoothing': 0.05}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 130, Fold 1: Test Accuracy = 0.5480\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0009642562330633527, optimizer=Adam, weight_decay=0.00029367003744361174, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:24:56,491] Trial 132 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 16, 'learning_rate': 0.0007388076540564359, 'weight_decay': 6.567484084171686e-05, 'label_smoothing': 0.17}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 132, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0009476862100850873, optimizer=Adam, weight_decay=0.00017741895931483323, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:25:39,638] Trial 133 finished with value: 0.5622412008281573 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0009642562330633527, 'weight_decay': 0.00029367003744361174, 'label_smoothing': 0.17}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 133, Fold 1: Test Accuracy = 0.5622\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0005222979357079563, optimizer=Adam, weight_decay=0.0008427027145774952, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 131, Fold 1: Test Accuracy = 0.5455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:25:49,434] Trial 131 finished with value: 0.5455227743271222 and parameters: {'num_heads': 4, 'num_transformer_blocks': 16, 'learning_rate': 0.00023597681004269668, 'weight_decay': 0.0008726976596023714, 'label_smoothing': 0.15}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0005014026216021724, optimizer=Adam, weight_decay=0.00021943394687670215, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:26:58,342] Trial 134 finished with value: 0.5566338854382332 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0009476862100850873, 'weight_decay': 0.00017741895931483323, 'label_smoothing': 0.15}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 134, Fold 1: Test Accuracy = 0.5566\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.0006444211704037813, optimizer=Adam, weight_decay=0.00023300939741798923, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 135, Fold 1: Test Accuracy = 0.5241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:27:31,167] Trial 135 finished with value: 0.5240942028985507 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0005222979357079563, 'weight_decay': 0.0008427027145774952, 'label_smoothing': 0.15}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.004583673852668746, optimizer=Adam, weight_decay=0.0005779582302236364, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:28:17,398] Trial 136 finished with value: 0.5779071773636991 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0005014026216021724, 'weight_decay': 0.00021943394687670215, 'label_smoothing': 0.19}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 136, Fold 1: Test Accuracy = 0.5779\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0024398300634119754, optimizer=Adam, weight_decay=0.0005778091838575165, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:30:17,553] Trial 137 finished with value: 0.5394927536231884 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.0006444211704037813, 'weight_decay': 0.00023300939741798923, 'label_smoothing': 0.21}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 137, Fold 1: Test Accuracy = 0.5395\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0025385109734462185, optimizer=Adam, weight_decay=0.0005905440182849659, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 138, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:30:32,783] Trial 138 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.004583673852668746, 'weight_decay': 0.0005779582302236364, 'label_smoothing': 0.02}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0024435957513490062, optimizer=Adam, weight_decay=0.00032084463091588435, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:32:34,849] Trial 139 finished with value: 0.5517857142857143 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0024398300634119754, 'weight_decay': 0.0005778091838575165, 'label_smoothing': 0.16}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 139, Fold 1: Test Accuracy = 0.5518\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0002964294422277012, optimizer=Adam, weight_decay=0.0007090119353904357, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:33:57,819] Trial 141 finished with value: 0.42801932367149753 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0024435957513490062, 'weight_decay': 0.00032084463091588435, 'label_smoothing': 0.06}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 141, Fold 1: Test Accuracy = 0.4280\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.00031254770456495563, optimizer=Adam, weight_decay=0.000997282862214272, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 142, Fold 1: Test Accuracy = 0.5797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:34:12,566] Trial 142 finished with value: 0.5796670117322291 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0002964294422277012, 'weight_decay': 0.0007090119353904357, 'label_smoothing': 0.06}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0012386226690798176, optimizer=Adam, weight_decay=0.0007853236134369691, batch_size=16,factor=1\n",
      "Trial 140, Fold 1: Test Accuracy = 0.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:34:32,334] Trial 140 finished with value: 0.5575914423740511 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0025385109734462185, 'weight_decay': 0.0005905440182849659, 'label_smoothing': 0.02}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0012727785101787467, optimizer=Adam, weight_decay=0.0007740944429064555, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 143, Fold 1: Test Accuracy = 0.5327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:35:49,778] Trial 143 finished with value: 0.532651828847481 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.00031254770456495563, 'weight_decay': 0.000997282862214272, 'label_smoothing': 0.11}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.0013960623783174826, optimizer=Adam, weight_decay=0.0007725261928301675, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:36:04,470] Trial 144 finished with value: 0.5637681159420289 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0012386226690798176, 'weight_decay': 0.0007853236134369691, 'label_smoothing': 0.13}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 144, Fold 1: Test Accuracy = 0.5638\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0007228764175955534, optimizer=Adam, weight_decay=0.0007140671314593801, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:36:05,148] Trial 145 finished with value: 0.5099465148378192 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0012727785101787467, 'weight_decay': 0.0007740944429064555, 'label_smoothing': 0.13}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 145, Fold 1: Test Accuracy = 0.5099\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0007840538058388696, optimizer=Adam, weight_decay=0.0008814977503519273, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 147, Fold 1: Test Accuracy = 0.5130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:38:13,438] Trial 147 finished with value: 0.5129830917874396 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0007228764175955534, 'weight_decay': 0.0007140671314593801, 'label_smoothing': 0.12}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.000421190510203952, optimizer=Adam, weight_decay=4.503490958592882e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 146, Fold 1: Test Accuracy = 0.5157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:38:45,776] Trial 146 finished with value: 0.5156918564527261 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.0013960623783174826, 'weight_decay': 0.0007725261928301675, 'label_smoothing': 0.12}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0004289631955027343, optimizer=Adam, weight_decay=0.0009019284606442918, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:38:47,640] Trial 148 finished with value: 0.5432194616977225 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0007840538058388696, 'weight_decay': 0.0008814977503519273, 'label_smoothing': 0.0}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 148, Fold 1: Test Accuracy = 0.5432\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=4.1472288326995033e-07, optimizer=Adam, weight_decay=0.00044034316353605194, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:40:58,796] Trial 150 finished with value: 0.5609817115251898 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0004289631955027343, 'weight_decay': 0.0009019284606442918, 'label_smoothing': 0.0}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 150, Fold 1: Test Accuracy = 0.5610\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0016725059979557505, optimizer=Adam, weight_decay=4.399224417717411e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:41:02,839] Trial 149 finished with value: 0.5252933057280883 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.000421190510203952, 'weight_decay': 4.503490958592882e-05, 'label_smoothing': 0.11}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 149, Fold 1: Test Accuracy = 0.5253\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0009670992212269853, optimizer=Adam, weight_decay=0.0005080269707581974, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:41:05,080] Trial 151 finished with value: 0.38050379572118703 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 4.1472288326995033e-07, 'weight_decay': 0.00044034316353605194, 'label_smoothing': 0.1}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 151, Fold 1: Test Accuracy = 0.3805\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.000983719984838618, optimizer=Adam, weight_decay=0.0006557728742907101, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 152, Fold 1: Test Accuracy = 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:42:52,281] Trial 152 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0016725059979557505, 'weight_decay': 4.399224417717411e-05, 'label_smoothing': 0.14}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0010884156212225297, optimizer=Adam, weight_decay=0.0005298691454737912, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:43:19,792] Trial 153 finished with value: 0.5528467908902691 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0009670992212269853, 'weight_decay': 0.0005080269707581974, 'label_smoothing': 0.22}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 153, Fold 1: Test Accuracy = 0.5528\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0011619275357454277, optimizer=Adam, weight_decay=0.0006531659657064784, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:44:13,575] Trial 154 finished with value: 0.5493530020703934 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.000983719984838618, 'weight_decay': 0.0006557728742907101, 'label_smoothing': 0.23}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 154, Fold 1: Test Accuracy = 0.5494\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0018933226545544764, optimizer=Adam, weight_decay=0.0006474660724554895, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:45:00,610] Trial 155 finished with value: 0.5751293995859214 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0010884156212225297, 'weight_decay': 0.0005298691454737912, 'label_smoothing': 0.22}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 155, Fold 1: Test Accuracy = 0.5751\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0032766787403158268, optimizer=Adam, weight_decay=2.4364009956873795e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:46:42,922] Trial 156 finished with value: 0.5662180814354727 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0011619275357454277, 'weight_decay': 0.0006531659657064784, 'label_smoothing': 0.23}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 156, Fold 1: Test Accuracy = 0.5662\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0019467639834497993, optimizer=Adam, weight_decay=0.0009898836662036333, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:46:52,857] Trial 158 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0032766787403158268, 'weight_decay': 2.4364009956873795e-05, 'label_smoothing': 0.08}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 158, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=0.001963281755929174, optimizer=Adam, weight_decay=8.690672904275421e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 157, Fold 1: Test Accuracy = 0.5254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:47:15,306] Trial 157 finished with value: 0.5253536922015183 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0018933226545544764, 'weight_decay': 0.0006474660724554895, 'label_smoothing': 0.23}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=0.0005682400818200053, optimizer=Adam, weight_decay=3.7052839550186895e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:48:53,762] Trial 161 finished with value: 0.5666839199447895 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 0.0005682400818200053, 'weight_decay': 3.7052839550186895e-05, 'label_smoothing': 0.07}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 161, Fold 1: Test Accuracy = 0.5667\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.0019571424702403487, optimizer=Adam, weight_decay=0.00014744567691054594, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 159, Fold 1: Test Accuracy = 0.5206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:50:30,563] Trial 159 finished with value: 0.5206176673567978 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0019467639834497993, 'weight_decay': 0.0009898836662036333, 'label_smoothing': 0.07}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0007666257569512293, optimizer=Adam, weight_decay=0.0008602991334273695, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:50:31,300] Trial 160 finished with value: 0.551164596273292 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 0.001963281755929174, 'weight_decay': 8.690672904275421e-05, 'label_smoothing': 0.24}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 160, Fold 1: Test Accuracy = 0.5512\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0014352951543059641, optimizer=Adam, weight_decay=0.000999403260577826, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:51:21,647] Trial 162 finished with value: 0.5266649413388543 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.0019571424702403487, 'weight_decay': 0.00014744567691054594, 'label_smoothing': 0.03}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 162, Fold 1: Test Accuracy = 0.5267\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.001429688226713615, optimizer=Adam, weight_decay=0.0008404313352636766, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:52:43,092] Trial 163 finished with value: 0.5805210489993099 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0007666257569512293, 'weight_decay': 0.0008602991334273695, 'label_smoothing': 0.16}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 163, Fold 1: Test Accuracy = 0.5805\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0014115240582092785, optimizer=Adam, weight_decay=0.0008415832435908093, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:53:08,868] Trial 164 finished with value: 0.5759575569358178 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0014352951543059641, 'weight_decay': 0.000999403260577826, 'label_smoothing': 0.16}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 164, Fold 1: Test Accuracy = 0.5760\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=3.8895503019638854e-05, optimizer=Adam, weight_decay=0.0007976560279619839, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 165, Fold 1: Test Accuracy = 0.5620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:54:43,828] Trial 165 finished with value: 0.5620082815734989 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.001429688226713615, 'weight_decay': 0.0008404313352636766, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.004072938428015639, optimizer=Adam, weight_decay=0.0007665441830060402, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:55:22,373] Trial 166 finished with value: 0.5737577639751553 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0014115240582092785, 'weight_decay': 0.0008415832435908093, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 166, Fold 1: Test Accuracy = 0.5738\n",
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.0005500849622453133, optimizer=Adam, weight_decay=5.232677305994526e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:56:37,375] Trial 168 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.004072938428015639, 'weight_decay': 0.0007665441830060402, 'label_smoothing': 0.14}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 168, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0006090561295217357, optimizer=Adam, weight_decay=0.0005578196700955547, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:57:08,289] Trial 169 finished with value: 0.560127674258109 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.0005500849622453133, 'weight_decay': 5.232677305994526e-05, 'label_smoothing': 0.14}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 169, Fold 1: Test Accuracy = 0.5601\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=3.849949196986085e-05, optimizer=Adam, weight_decay=0.0005388129106689604, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:57:17,674] Trial 167 finished with value: 0.5472049689440994 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 3.8895503019638854e-05, 'weight_decay': 0.0007976560279619839, 'label_smoothing': 0.18}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 167, Fold 1: Test Accuracy = 0.5472\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0006946910770004934, optimizer=Adam, weight_decay=2.607798771696165e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:58:11,650] Trial 170 finished with value: 0.5669254658385093 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0006090561295217357, 'weight_decay': 0.0005578196700955547, 'label_smoothing': 0.2}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 170, Fold 1: Test Accuracy = 0.5669\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0010542722151830337, optimizer=Adam, weight_decay=0.0003509276527144636, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 171, Fold 1: Test Accuracy = 0.5140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 05:59:43,654] Trial 171 finished with value: 0.5140441683919944 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 3.849949196986085e-05, 'weight_decay': 0.0005388129106689604, 'label_smoothing': 0.19}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0010998812562678136, optimizer=Adam, weight_decay=0.00035278718275024405, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 172, Fold 1: Test Accuracy = 0.5358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:00:19,917] Trial 172 finished with value: 0.5358436853002071 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0006946910770004934, 'weight_decay': 2.607798771696165e-06, 'label_smoothing': 0.2}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.001062194356027734, optimizer=Adam, weight_decay=0.00038724120778152444, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:00:27,490] Trial 173 finished with value: 0.5519151138716356 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0010542722151830337, 'weight_decay': 0.0003509276527144636, 'label_smoothing': 0.26}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 173, Fold 1: Test Accuracy = 0.5519\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0029810569102276066, optimizer=Adam, weight_decay=0.00044831709557894986, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:03:08,906] Trial 174 finished with value: 0.5368702553485162 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0010998812562678136, 'weight_decay': 0.00035278718275024405, 'label_smoothing': 0.26}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 174, Fold 1: Test Accuracy = 0.5369\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0024988518969955302, optimizer=Adam, weight_decay=0.0004214699685870516, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:03:25,315] Trial 176 finished with value: 0.48396307798481714 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0029810569102276066, 'weight_decay': 0.00044831709557894986, 'label_smoothing': 0.29}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 176, Fold 1: Test Accuracy = 0.4840\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0008821763293675413, optimizer=Adam, weight_decay=0.00027157849935365946, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:03:35,505] Trial 175 finished with value: 0.5924775707384403 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.001062194356027734, 'weight_decay': 0.00038724120778152444, 'label_smoothing': 0.29}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 175, Fold 1: Test Accuracy = 0.5925\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=0.00037666138863245687, optimizer=Adam, weight_decay=0.0002591163117028131, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:04:58,188] Trial 177 finished with value: 0.5364648033126295 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0024988518969955302, 'weight_decay': 0.0004214699685870516, 'label_smoothing': 0.21}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 177, Fold 1: Test Accuracy = 0.5365\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=16, learning_rate=0.0003708479999121987, optimizer=Adam, weight_decay=0.0002872963544497455, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 179, Fold 1: Test Accuracy = 0.5429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:05:17,373] Trial 179 finished with value: 0.5428571428571428 and parameters: {'num_heads': 32, 'num_transformer_blocks': 4, 'learning_rate': 0.00037666138863245687, 'weight_decay': 0.0002591163117028131, 'label_smoothing': 0.17}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=16, learning_rate=0.0008367318819960122, optimizer=Adam, weight_decay=0.0006861740873958439, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:06:27,626] Trial 178 finished with value: 0.5591701173222913 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0008821763293675413, 'weight_decay': 0.00027157849935365946, 'label_smoothing': 0.21}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 178, Fold 1: Test Accuracy = 0.5592\n",
      "Hyperparameters: num_heads=16, num_transformer_blocks=16, learning_rate=0.0008646843356636554, optimizer=Adam, weight_decay=0.0006712433808213382, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:06:43,188] Trial 180 finished with value: 0.5417960662525879 and parameters: {'num_heads': 16, 'num_transformer_blocks': 16, 'learning_rate': 0.0003708479999121987, 'weight_decay': 0.0002872963544497455, 'label_smoothing': 0.17}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 180, Fold 1: Test Accuracy = 0.5418\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.001342947106136979, optimizer=Adam, weight_decay=0.0006770665277033678, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:08:14,849] Trial 182 finished with value: 0.522851966873706 and parameters: {'num_heads': 16, 'num_transformer_blocks': 16, 'learning_rate': 0.0008646843356636554, 'weight_decay': 0.0006712433808213382, 'label_smoothing': 0.09}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 182, Fold 1: Test Accuracy = 0.5229\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0015290565547656624, optimizer=Adam, weight_decay=0.00030926574230936705, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:09:03,919] Trial 183 finished with value: 0.545212215320911 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.001342947106136979, 'weight_decay': 0.0006770665277033678, 'label_smoothing': 0.3}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 183, Fold 1: Test Accuracy = 0.5452\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0016341940120100094, optimizer=Adam, weight_decay=0.0003567804635224313, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:09:58,222] Trial 184 finished with value: 0.5069703243616287 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0015290565547656624, 'weight_decay': 0.00030926574230936705, 'label_smoothing': 0.29}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 184, Fold 1: Test Accuracy = 0.5070\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.001168227615104485, optimizer=Adam, weight_decay=0.0005996054923122027, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:10:34,771] Trial 181 finished with value: 0.5255262249827467 and parameters: {'num_heads': 16, 'num_transformer_blocks': 16, 'learning_rate': 0.0008367318819960122, 'weight_decay': 0.0006861740873958439, 'label_smoothing': 0.09}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 181, Fold 1: Test Accuracy = 0.5255\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.0011675152698642653, optimizer=Adam, weight_decay=7.881532665215719e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:12:57,822] Trial 186 finished with value: 0.5147429261559696 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.001168227615104485, 'weight_decay': 0.0005996054923122027, 'label_smoothing': 0.27}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 186, Fold 1: Test Accuracy = 0.5147\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.001803702408085901, optimizer=Adam, weight_decay=0.00041736092900608655, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 187, Fold 1: Test Accuracy = 0.4936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:13:07,340] Trial 187 finished with value: 0.49359903381642517 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0011675152698642653, 'weight_decay': 7.881532665215719e-06, 'label_smoothing': 0.27}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0021432071801159446, optimizer=Adam, weight_decay=0.00036292123053318807, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:13:19,963] Trial 185 finished with value: 0.589544513457557 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0016341940120100094, 'weight_decay': 0.0003567804635224313, 'label_smoothing': 0.28}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 185, Fold 1: Test Accuracy = 0.5895\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.002324964992049731, optimizer=Adam, weight_decay=4.461556323383362e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:14:38,406] Trial 188 finished with value: 0.513052104899931 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.001803702408085901, 'weight_decay': 0.00041736092900608655, 'label_smoothing': 0.11}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 188, Fold 1: Test Accuracy = 0.5131\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=5.103685731664199e-06, optimizer=Adam, weight_decay=0.0001979051349759669, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:15:27,116] Trial 190 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.002324964992049731, 'weight_decay': 4.461556323383362e-06, 'label_smoothing': 0.28}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 190, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.0005176098708671592, optimizer=Adam, weight_decay=0.000886789491948557, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:15:58,549] Trial 189 finished with value: 0.5939872325741891 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0021432071801159446, 'weight_decay': 0.00036292123053318807, 'label_smoothing': 0.25}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 189, Fold 1: Test Accuracy = 0.5940\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.004010687238191057, optimizer=Adam, weight_decay=0.00020613622257913575, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:17:12,371] Trial 192 finished with value: 0.42355935127674255 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.0005176098708671592, 'weight_decay': 0.000886789491948557, 'label_smoothing': 0.04}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 192, Fold 1: Test Accuracy = 0.4236\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003737795954812682, optimizer=Adam, weight_decay=0.00020133336932888778, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:17:49,242] Trial 193 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.004010687238191057, 'weight_decay': 0.00020613622257913575, 'label_smoothing': 0.22}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 193, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.003006765287849993, optimizer=Adam, weight_decay=0.0003442086041928429, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:18:29,899] Trial 194 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003737795954812682, 'weight_decay': 0.00020133336932888778, 'label_smoothing': 0.25}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 194, Fold 1: Test Accuracy = 0.3333\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0017213084124924475, optimizer=Adam, weight_decay=0.0003346035079097658, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:20:21,788] Trial 191 finished with value: 0.4478002070393375 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 5.103685731664199e-06, 'weight_decay': 0.0001979051349759669, 'label_smoothing': 0.13}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 191, Fold 1: Test Accuracy = 0.4478\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.00269537705201316, optimizer=Adam, weight_decay=0.00032954071320403513, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:20:59,546] Trial 195 finished with value: 0.5169168391994479 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.003006765287849993, 'weight_decay': 0.0003442086041928429, 'label_smoothing': 0.25}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 195, Fold 1: Test Accuracy = 0.5169\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.001834928318696754, optimizer=Adam, weight_decay=0.00046299162072857374, batch_size=16,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:22:16,936] Trial 196 finished with value: 0.5183143547273983 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0017213084124924475, 'weight_decay': 0.0003346035079097658, 'label_smoothing': 0.29}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 196, Fold 1: Test Accuracy = 0.5183\n",
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=0.00548141241435487, optimizer=Adam, weight_decay=0.0004905413489935276, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 197, Fold 1: Test Accuracy = 0.5569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:24:31,437] Trial 197 finished with value: 0.5568840579710145 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.00269537705201316, 'weight_decay': 0.00032954071320403513, 'label_smoothing': 0.23}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 198, Fold 1: Test Accuracy = 0.5492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:24:49,664] Trial 198 finished with value: 0.5491890959282263 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.001834928318696754, 'weight_decay': 0.00046299162072857374, 'label_smoothing': 0.27}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-07 06:24:58,171] Trial 199 finished with value: 0.4540113871635611 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 0.00548141241435487, 'weight_decay': 0.0004905413489935276, 'label_smoothing': 0.3}. Best is trial 115 with value: 0.6363181504485853.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 199, Fold 1: Test Accuracy = 0.4540\n",
      "Best hyperparameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.0014268683384269449, 'weight_decay': 0.0003463319958859285, 'label_smoothing': 0.18}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8, 16, 32])\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [2, 4, 8, 16, 32])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.3, step=0.01)\n",
    "    batch_size = 16\n",
    "    optimizer_name = \"Adam\"\n",
    "    factor = 1\n",
    "\n",
    "    print(f\"Hyperparameters: num_heads={num_heads}, num_transformer_blocks={num_transformer_blocks}, \"\n",
    "          f\"learning_rate={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size},factor={factor}\")\n",
    "    \n",
    "    # Use only fold 0 as test fold\n",
    "    test_fold_idx = 0\n",
    "    \n",
    "    #clear GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    test_fold = fold_indices[test_fold_idx]\n",
    "    remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "    val_fold_idx = test_fold_idx % 4\n",
    "    val_fold = remaining_folds[val_fold_idx]\n",
    "    train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "    val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "    val_labels = labels_folds[val_fold]\n",
    "\n",
    "    test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    test_labels = labels_folds[test_fold]\n",
    "\n",
    "    balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "    train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                  torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "    val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                torch.tensor(val_labels, dtype=torch.long))\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), \n",
    "                                 torch.tensor(test_labels, dtype=torch.long))\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TRANS_CNN(input_shape=input_shape, num_transformer_blocks=num_transformer_blocks, \n",
    "                      num_heads=num_heads, num_classes=num_classes, embed_dim=128).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "    epochs = 30\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                val_outputs = model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    early_stopping.load_best_model(model)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    trial.report(fold_acc, step=0)\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return fold_acc\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=200, n_jobs=3)       \n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#clear gpu and restart kernel\n",
    "torch.cuda.empty_cache()\n",
    "#restart kernel\n",
    "import gc\n",
    "gc.collect()\n",
    "# Restart the kernel to clear memory\n",
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)  # This will restart the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e535aa3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
