{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® IMPORTS ‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">‚ú® LOADING THE SPLIT DATA ARRAYS ‚ú®</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">üìÅ reminder to change the folder path to your numpy array folder üìÅ</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"G:\\CODING\\py\\data\\numpy\\mel\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1269, 224, 224, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_fold_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">data balancer & early stopping</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Result plotting</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_metrics(labels, predictions, n_classes=3):\n",
    "    \"\"\"\n",
    "    Plots and computes metrics for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        labels (array-like): True labels.\n",
    "        predictions (array-like): Predicted probabilities or class predictions.\n",
    "        n_classes (int): Number of classes (default is 3 for multi-class classification).\n",
    "    \"\"\"\n",
    "    # If predictions are probabilities, convert to class predictions\n",
    "    if predictions.ndim > 1:\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predicted_classes = predictions\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, predicted_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise Accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for d in range(n_classes):\n",
    "        correct_preds = cm[d][d]\n",
    "        total_true_samples = sum(cm[d])\n",
    "        ratio_correct = correct_preds / total_true_samples if total_true_samples != 0 else 0\n",
    "        print(f'Class {d}: Correct Predictions / Total True Samples = {correct_preds}/{total_true_samples} ({ratio_correct:.2%})')\n",
    "\n",
    "    # Precision-Recall Curves and AUPRC\n",
    "    print(\"\\nPrecision-Recall Curves:\")\n",
    "    labels_binarized = label_binarize(labels, classes=np.arange(n_classes))\n",
    "    auprcs = []\n",
    "    for class_idx in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        auprcs.append(auprc)\n",
    "        plt.plot(recall, precision, label=f'Class {class_idx + 1} (AUPRC = {auprc:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curves and AUC\n",
    "    print(\"\\nROC Curves:\")\n",
    "    for class_idx in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {class_idx + 1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Mean F1 Score\n",
    "    f1 = f1_score(labels, predicted_classes, average='macro')\n",
    "    print(f\"\\nMean F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    balanced_acc = balanced_accuracy_score(labels, predicted_classes)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    # Average AUPRC\n",
    "    mean_auprc = np.mean(auprcs)\n",
    "    print(f\"Average AUPRC: {mean_auprc:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predicted_classes))\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"class_wise_accuracy\": [cm[d][d] / sum(cm[d]) if sum(cm[d]) != 0 else 0 for d in range(n_classes)],\n",
    "        \"mean_f1_score\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc,\n",
    "        \"average_auprc\": mean_auprc,\n",
    "        \"auprc_per_class\": auprcs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">MODEL GOES HERE</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_mode_flag = False\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        self.block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_3(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 3 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens ‚Üí 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Test with demo data & model Summary</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model  = TRANS_CNN(input_shape=(224,224,3),num_classes=3,num_transformer_blocks=64,num_heads=32\n",
    "                   ,embed_dim=512) # declare model here\n",
    "randomdata = torch.randn((1,3,224,224))\n",
    "output = model(randomdata)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "‚îú‚îÄCustomCnn: 1-1                                             --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-1                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-1                                      448\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-2                                 32\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-3                                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-4                                      4,640\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-5                                 64\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-6                                        --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-7                                   --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-2                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-8                                      18,496\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-9                                 128\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-10                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-11                                     73,856\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-12                                256\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-13                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-14                                  --\n",
       "‚îÇ    ‚îî‚îÄSequential: 2-3                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-15                                     295,168\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-16                                512\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-17                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄConv2d: 3-18                                     1,180,160\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄBatchNorm2d: 3-19                                1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄReLU: 3-20                                       --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMaxPool2d: 3-21                                  --\n",
       "‚îÇ    ‚îî‚îÄAdaptiveAvgPool2d: 2-4                                --\n",
       "‚îÇ    ‚îî‚îÄFlatten: 2-5                                          --\n",
       "‚îú‚îÄLinear: 1-2                                                262,656\n",
       "‚îú‚îÄModuleList: 1-3                                            --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-6                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-22                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-23                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-24                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-25                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-26                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-27                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-7                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-28                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-29                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-30                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-31                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-32                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-33                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-8                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-34                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-35                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-36                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-37                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-38                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-39                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-9                               --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-40                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-41                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-42                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-43                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-44                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-45                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-10                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-46                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-47                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-48                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-49                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-50                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-51                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-11                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-52                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-53                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-54                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-55                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-56                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-57                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-12                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-58                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-59                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-60                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-61                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-62                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-63                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-13                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-64                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-65                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-66                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-67                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-68                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-69                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-14                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-70                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-71                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-72                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-73                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-74                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-75                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-15                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-76                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-77                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-78                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-79                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-80                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-81                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-16                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-82                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-83                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-84                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-85                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-86                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-87                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-17                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-88                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-89                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-90                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-91                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-92                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-93                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-18                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-94                         1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-95                                 66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-96                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-97                                  1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-98                                    --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-99                                    --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-19                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-100                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-101                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-102                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-103                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-104                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-105                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-20                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-106                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-107                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-108                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-109                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-110                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-111                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-21                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-112                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-113                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-114                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-115                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-116                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-117                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-22                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-118                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-119                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-120                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-121                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-122                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-123                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-23                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-124                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-125                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-126                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-127                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-128                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-129                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-24                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-130                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-131                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-132                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-133                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-134                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-135                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-25                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-136                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-137                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-138                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-139                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-140                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-141                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-26                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-142                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-143                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-144                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-145                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-146                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-147                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-27                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-148                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-149                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-150                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-151                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-152                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-153                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-28                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-154                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-155                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-156                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-157                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-158                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-159                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-29                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-160                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-161                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-162                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-163                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-164                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-165                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-30                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-166                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-167                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-168                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-169                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-170                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-171                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-31                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-172                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-173                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-174                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-175                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-176                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-177                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-32                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-178                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-179                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-180                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-181                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-182                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-183                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-33                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-184                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-185                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-186                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-187                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-188                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-189                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-34                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-190                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-191                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-192                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-193                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-194                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-195                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-35                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-196                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-197                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-198                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-199                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-200                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-201                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-36                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-202                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-203                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-204                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-205                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-206                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-207                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-37                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-208                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-209                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-210                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-211                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-212                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-213                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-38                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-214                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-215                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-216                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-217                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-218                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-219                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-39                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-220                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-221                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-222                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-223                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-224                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-225                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-40                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-226                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-227                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-228                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-229                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-230                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-231                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-41                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-232                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-233                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-234                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-235                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-236                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-237                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-42                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-238                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-239                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-240                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-241                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-242                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-243                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-43                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-244                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-245                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-246                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-247                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-248                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-249                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-44                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-250                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-251                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-252                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-253                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-254                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-255                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-45                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-256                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-257                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-258                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-259                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-260                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-261                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-46                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-262                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-263                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-264                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-265                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-266                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-267                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-47                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-268                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-269                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-270                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-271                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-272                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-273                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-48                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-274                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-275                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-276                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-277                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-278                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-279                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-49                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-280                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-281                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-282                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-283                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-284                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-285                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-50                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-286                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-287                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-288                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-289                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-290                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-291                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-51                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-292                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-293                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-294                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-295                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-296                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-297                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-52                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-298                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-299                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-300                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-301                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-302                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-303                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-53                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-304                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-305                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-306                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-307                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-308                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-309                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-54                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-310                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-311                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-312                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-313                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-314                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-315                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-55                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-316                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-317                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-318                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-319                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-320                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-321                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-56                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-322                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-323                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-324                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-325                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-326                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-327                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-57                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-328                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-329                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-330                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-331                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-332                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-333                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-58                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-334                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-335                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-336                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-337                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-338                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-339                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-59                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-340                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-341                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-342                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-343                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-344                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-345                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-60                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-346                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-347                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-348                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-349                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-350                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-351                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-61                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-352                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-353                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-354                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-355                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-356                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-357                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-62                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-358                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-359                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-360                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-361                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-362                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-363                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-63                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-364                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-365                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-366                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-367                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-368                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-369                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-64                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-370                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-371                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-372                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-373                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-374                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-375                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-65                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-376                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-377                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-378                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-379                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-380                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-381                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-66                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-382                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-383                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-384                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-385                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-386                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-387                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-67                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-388                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-389                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-390                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-391                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-392                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-393                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-68                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-394                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-395                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-396                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-397                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-398                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-399                                   --\n",
       "‚îÇ    ‚îî‚îÄTransformerEncoder: 2-69                              --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄMultiheadAttention: 3-400                        1,050,624\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄSequential: 3-401                                66,112\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-402                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄLayerNorm: 3-403                                 1,024\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-404                                   --\n",
       "‚îÇ    ‚îÇ    ‚îî‚îÄDropout: 3-405                                   --\n",
       "‚îú‚îÄAdaptiveAvgPool1d: 1-4                                     --\n",
       "‚îú‚îÄDropout: 1-5                                               --\n",
       "‚îú‚îÄLinear: 1-6                                                262,656\n",
       "‚îú‚îÄLinear: 1-7                                                262,656\n",
       "‚îú‚îÄLinear: 1-8                                                65,664\n",
       "‚îú‚îÄLinear: 1-9                                                387\n",
       "=====================================================================================\n",
       "Total params: 74,030,979\n",
       "Trainable params: 74,030,979\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(238, 206, 148),rgb(238, 150, 150)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.2em; color: white; text-shadow: 1px 1px 3px #000;\">Training Code</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Fold indices and other configurations\n",
    "num_folds = 5\n",
    "fold_indices = np.random.permutation(np.arange(num_folds))\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "test_folds_chosen = []\n",
    "val_folds_chosen = []\n",
    "fold_confusion_matrices = []  # To store confusion matrices for each fold\n",
    "fold_accuracies = []  # To store balanced accuracy for each fold\n",
    "fold_auprcs = []  # To store AUPRC for each fold\n",
    "\n",
    "# Model and training configurations\n",
    "num_classes = 3\n",
    "learning_rate = 0.0001\n",
    "epochs = 100\n",
    "input_shape = (3,224,224)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_info, csv_path=r\"G:\\CODING\\py\\newnotebooks\\results.csv\"):\n",
    "    \"\"\"\n",
    "    Save all results from the current experiment to a CSV file\n",
    "    \n",
    "    Parameters:\n",
    "    - fold_accuracies: list of balanced accuracy scores for each fold\n",
    "    - fold_auprcs: list of AUPRC scores for each fold\n",
    "    - fold_confusion_matrices: list of confusion matrices for each fold\n",
    "    - model_info: string with model architecture description\n",
    "    - csv_path: path to the CSV file to save results\n",
    "    \"\"\"\n",
    "    # Current time for experiment identification\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # Prepare data for the DataFrame\n",
    "    data = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_info\": model_info,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"mean_balanced_accuracy\": np.mean(fold_accuracies),\n",
    "        \"std_balanced_accuracy\": np.std(fold_accuracies),\n",
    "        \"mean_auprc\": np.mean(fold_auprcs),\n",
    "        \"std_auprc\": np.std(fold_auprcs),\n",
    "    }\n",
    "    \n",
    "    # Add individual fold results\n",
    "    for i, (acc, auprc) in enumerate(zip(fold_accuracies, fold_auprcs)):\n",
    "        data[f\"fold_{i+1}_accuracy\"] = acc\n",
    "        data[f\"fold_{i+1}_auprc\"] = auprc\n",
    "    \n",
    "    # Add confusion matrix info\n",
    "    for i, cm in enumerate(fold_confusion_matrices):\n",
    "        data[f\"fold_{i+1}_confusion_matrix\"] = str(cm)\n",
    "    \n",
    "    # Create DataFrame and append to CSV\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "    \n",
    "    # Save to CSV\n",
    "    if file_exists:\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "    \n",
    "    print(f\"Results saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold No: 1\n",
      "Epoch [1/100], Loss: 1.1396, Accuracy: 0.3261\n",
      "Validation Loss: 1.0653, Validation Accuracy: 0.3333\n",
      "Epoch [2/100], Loss: 1.1045, Accuracy: 0.3460\n",
      "Validation Loss: 1.0850, Validation Accuracy: 0.3333\n",
      "Epoch [3/100], Loss: 1.1028, Accuracy: 0.2862\n",
      "Validation Loss: 1.0773, Validation Accuracy: 0.3333\n",
      "Epoch [4/100], Loss: 1.1023, Accuracy: 0.3134\n",
      "Validation Loss: 1.1259, Validation Accuracy: 0.3333\n",
      "Epoch [5/100], Loss: 1.1117, Accuracy: 0.2808\n",
      "Validation Loss: 1.1417, Validation Accuracy: 0.3333\n",
      "Epoch [6/100], Loss: 1.1090, Accuracy: 0.3116\n",
      "Validation Loss: 1.1708, Validation Accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "# Training loop for cross-validation\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f'Fold No: {fold_idx + 1}')\n",
    "    \n",
    "    # Initialize model with updated hyperparameters\n",
    "    fold_model = TRANS_CNN(input_shape=input_shape, \n",
    "                          num_classes=num_classes, \n",
    "                          embed_dim=512, \n",
    "                          num_heads=32,  # Updated hyperparameter\n",
    "                          num_transformer_blocks=64)  # Updated hyperparameter\n",
    "    fold_model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "    # Updated optimizer with weight decay\n",
    "    optimizer = optim.Adam(fold_model.parameters(), \n",
    "                          lr=0.000016837190784408735,  # Updated learning rate\n",
    "                          weight_decay=0.0006199566716544295)  # Added weight decay\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    test_fold = fold_indices[fold_idx]\n",
    "    val_fold = val_fold_indices[fold_idx]\n",
    "    train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "    train_data = train_data.transpose(0, 3, 1, 2)  # Transpose to match PyTorch input format\n",
    "\n",
    "    test_folds_chosen.append(test_fold)\n",
    "    val_folds_chosen.append(val_fold)\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=2)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(balanced_train_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(balanced_train_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        # Updated batch size\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        fold_model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fold_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        # Updated batch size for validation\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = fold_model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, fold_model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    early_stopping.load_best_model(fold_model)\n",
    "\n",
    "    # Test loop\n",
    "    test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    test_labels = labels_folds[test_fold]\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "        torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    )\n",
    "    # Updated batch size for testing\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    fold_model.eval()\n",
    "    test_probs = []\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = fold_model(test_inputs)\n",
    "            probabilities = torch.softmax(test_outputs, dim=1)\n",
    "            test_probs.extend(probabilities.cpu().numpy())\n",
    "            _, preds = torch.max(probabilities, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_list, test_preds)\n",
    "    fold_confusion_matrices.append(cm)\n",
    "\n",
    "    test_labels_binarized = label_binarize(test_labels_list, classes=np.arange(num_classes))\n",
    "    test_auprcs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_binarized[:, class_idx], np.array(test_probs)[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "    mean_test_auprc = np.mean(test_auprcs)\n",
    "    fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "    print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Use the plot_metrics function to visualize metrics\n",
    "    plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "# Final metrics across all folds\n",
    "average_auprc = np.mean(fold_auprcs)\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "    \n",
    "# Save all results to CSV\n",
    "model_description = f\"TRANS_CNN normal spectrograms (num_heads=4, num_transformer_blocks=5, lr={0.00016837190784408735}, weight_decay={0.0006199566716544295}, batch_size=32)\"\n",
    "save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(255, 0, 0),rgb(131, 207, 207),rgb(255, 84, 84)); padding: 15px; border-radius: 8px; text-align: center; box-shadow: 0 8px 16px rgba(0,0,0,0.19), 0 4px 4px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 700; font-size: 1.8em; color: white; text-shadow: 1px 1px 3px #000;\">CLASS WEIGHTS APPROACH</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for cross-validation\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f'Fold No: {fold_idx + 1}')\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    fold_model = TRANS_CNN(input_shape=(100,100,20),num_classes=num_classes,embed_dim=320) # Initialize your model here\n",
    "    fold_model.to(device)\n",
    "    \n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    test_fold = fold_indices[fold_idx]\n",
    "    val_fold = val_fold_indices[fold_idx]\n",
    "    train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "    train_data = train_data.transpose(0, 3, 1, 2)  # Transpose to match PyTorch input format\n",
    "    \n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "    test_folds_chosen.append(test_fold)\n",
    "    val_folds_chosen.append(val_fold)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.3,weight=class_weights_tensor)\n",
    "    optimizer = optim.Adam(fold_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    \n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(train_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(train_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "        \n",
    "        fold_model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fold_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = fold_model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, fold_model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    early_stopping.load_best_model(fold_model)\n",
    "\n",
    "    # Test loop\n",
    "    test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "    test_labels = labels_folds[test_fold]\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "        torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    fold_model.eval()\n",
    "    test_probs = []\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = fold_model(test_inputs)\n",
    "            probabilities = torch.softmax(test_outputs, dim=1)\n",
    "            test_probs.extend(probabilities.cpu().numpy())\n",
    "            _, preds = torch.max(probabilities, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_list, test_preds)\n",
    "    fold_confusion_matrices.append(cm)\n",
    "\n",
    "    test_labels_binarized = label_binarize(test_labels_list, classes=np.arange(num_classes))\n",
    "    test_auprcs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_binarized[:, class_idx], np.array(test_probs)[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "    mean_test_auprc = np.mean(test_auprcs)\n",
    "    fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "    print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Use the plot_metrics function to visualize metrics\n",
    "    plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "# Final metrics across all folds\n",
    "average_auprc = np.mean(fold_auprcs)\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "    \n",
    "# Save all results to CSV\n",
    "model_description = f\"TRANS_CNN NORMAL SPECTROGRAMS CLASS WEIGHTS APPROACH 3 CLASS\"\n",
    "save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
