{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f610267e",
   "metadata": {},
   "source": [
    "<h1> I AM RUNNING ON NORMALIZED DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\data\\numpy\\cwtnormalized\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'CWT_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'CWT_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'CWT_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens → 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "├─CustomCnn: 1-1                                             --\n",
       "│    └─Sequential: 2-1                                       --\n",
       "│    │    └─Conv2d: 3-1                                      448\n",
       "│    │    └─BatchNorm2d: 3-2                                 32\n",
       "│    │    └─ReLU: 3-3                                        --\n",
       "│    │    └─Conv2d: 3-4                                      4,640\n",
       "│    │    └─BatchNorm2d: 3-5                                 64\n",
       "│    │    └─ReLU: 3-6                                        --\n",
       "│    │    └─MaxPool2d: 3-7                                   --\n",
       "│    └─Sequential: 2-2                                       --\n",
       "│    │    └─Conv2d: 3-8                                      18,496\n",
       "│    │    └─BatchNorm2d: 3-9                                 128\n",
       "│    │    └─ReLU: 3-10                                       --\n",
       "│    │    └─Conv2d: 3-11                                     73,856\n",
       "│    │    └─BatchNorm2d: 3-12                                256\n",
       "│    │    └─ReLU: 3-13                                       --\n",
       "│    │    └─MaxPool2d: 3-14                                  --\n",
       "│    └─AdaptiveAvgPool2d: 2-3                                --\n",
       "│    └─Flatten: 2-4                                          --\n",
       "├─Linear: 1-2                                                164,160\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─TransformerEncoder: 2-5                               --\n",
       "│    │    └─MultiheadAttention: 3-15                         410,880\n",
       "│    │    └─Sequential: 3-16                                 41,344\n",
       "│    │    └─LayerNorm: 3-17                                  640\n",
       "│    │    └─LayerNorm: 3-18                                  640\n",
       "│    │    └─Dropout: 3-19                                    --\n",
       "│    │    └─Dropout: 3-20                                    --\n",
       "│    └─TransformerEncoder: 2-6                               --\n",
       "│    │    └─MultiheadAttention: 3-21                         410,880\n",
       "│    │    └─Sequential: 3-22                                 41,344\n",
       "│    │    └─LayerNorm: 3-23                                  640\n",
       "│    │    └─LayerNorm: 3-24                                  640\n",
       "│    │    └─Dropout: 3-25                                    --\n",
       "│    │    └─Dropout: 3-26                                    --\n",
       "│    └─TransformerEncoder: 2-7                               --\n",
       "│    │    └─MultiheadAttention: 3-27                         410,880\n",
       "│    │    └─Sequential: 3-28                                 41,344\n",
       "│    │    └─LayerNorm: 3-29                                  640\n",
       "│    │    └─LayerNorm: 3-30                                  640\n",
       "│    │    └─Dropout: 3-31                                    --\n",
       "│    │    └─Dropout: 3-32                                    --\n",
       "│    └─TransformerEncoder: 2-8                               --\n",
       "│    │    └─MultiheadAttention: 3-33                         410,880\n",
       "│    │    └─Sequential: 3-34                                 41,344\n",
       "│    │    └─LayerNorm: 3-35                                  640\n",
       "│    │    └─LayerNorm: 3-36                                  640\n",
       "│    │    └─Dropout: 3-37                                    --\n",
       "│    │    └─Dropout: 3-38                                    --\n",
       "├─AdaptiveAvgPool1d: 1-4                                     --\n",
       "├─Dropout: 1-5                                               --\n",
       "├─Linear: 1-6                                                102,720\n",
       "├─Linear: 1-7                                                102,720\n",
       "├─Linear: 1-8                                                25,680\n",
       "├─Linear: 1-9                                                243\n",
       "=====================================================================================\n",
       "Total params: 2,307,459\n",
       "Trainable params: 2,307,459\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = TRANS_CNN(input_shape=(224,224,20),num_classes=3,num_transformer_blocks=4,embed_dim=320)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (3,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-02 15:44:43,723] A new study created in memory with name: no-name-5c71fe26-6e25-43b7-87f0-f14c2b642e02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=1.6226818125041195e-07, optimizer=Adam, weight_decay=1.130428003171421e-06, batch_size=10,factor=1\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=9.141710801635213e-08, optimizer=Adam, weight_decay=2.2210374951044208e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 0, Fold 1: Test Accuracy = 0.3465\n",
      "Trial 1, Fold 1: Test Accuracy = 0.3037\n",
      "Trial 1, Fold 2: Test Accuracy = 0.3342\n",
      "Trial 0, Fold 2: Test Accuracy = 0.3258\n",
      "Trial 1, Fold 3: Test Accuracy = 0.3187\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 1, Fold 4: Test Accuracy = 0.3546\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 0, Fold 3: Test Accuracy = 0.3477\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 1, Fold 5: Test Accuracy = 0.2869\n",
      "Trial 1: Mean Accuracy = 0.3196, Fold Accuracies = [np.float64(0.303702570379437), np.float64(0.3341580186155986), np.float64(0.3187242562929062), np.float64(0.3545907197520101), np.float64(0.28687236683018286)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 16:06:44,024] Trial 1 finished with value: 0.319609586374027 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 9.141710801635213e-08, 'weight_decay': 2.2210374951044208e-05, 'batch_size': 32}. Best is trial 1 with value: 0.319609586374027.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=16, learning_rate=9.217747260961902e-08, optimizer=Adam, weight_decay=5.374510743181859e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 2, Fold 1: Test Accuracy = 0.3090\n",
      "Trial 0, Fold 4: Test Accuracy = 0.3406\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 2, Fold 2: Test Accuracy = 0.3878\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 2, Fold 3: Test Accuracy = 0.3352\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 0, Fold 5: Test Accuracy = 0.3310\n",
      "Trial 0: Mean Accuracy = 0.3383, Fold Accuracies = [np.float64(0.3464824233597366), np.float64(0.3258472213106918), np.float64(0.3477065853038393), np.float64(0.3405807849499152), np.float64(0.3309782759755549)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 16:19:16,392] Trial 0 finished with value: 0.33831905817994756 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 1.6226818125041195e-07, 'weight_decay': 1.130428003171421e-06, 'batch_size': 10}. Best is trial 0 with value: 0.33831905817994756.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=1.5751924911617186e-06, optimizer=Adam, weight_decay=2.509906197582111e-05, batch_size=32,factor=1\n",
      "Trial 2, Fold 4: Test Accuracy = 0.3516\n",
      "Trial 3, Fold 1: Test Accuracy = 0.3481\n",
      "Trial 3, Fold 2: Test Accuracy = 0.4066\n",
      "Trial 2, Fold 5: Test Accuracy = 0.3147\n",
      "Trial 2: Mean Accuracy = 0.3397, Fold Accuracies = [np.float64(0.3089733409622763), np.float64(0.38784205240885267), np.float64(0.33520340706839563), np.float64(0.3515717759409062), np.float64(0.3147344360302963)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 16:30:57,224] Trial 2 finished with value: 0.33966500248214543 and parameters: {'num_heads': 16, 'num_transformer_blocks': 16, 'learning_rate': 9.217747260961902e-08, 'weight_decay': 5.374510743181859e-06, 'batch_size': 16}. Best is trial 2 with value: 0.33966500248214543.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=2.5822436972906645e-06, optimizer=Adam, weight_decay=2.506014907843724e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 3, Fold 3: Test Accuracy = 0.4694\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 4, Fold 1: Test Accuracy = 0.3470\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 3, Fold 4: Test Accuracy = 0.3755\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 3, Fold 5: Test Accuracy = 0.3184\n",
      "Trial 3: Mean Accuracy = 0.3836, Fold Accuracies = [np.float64(0.3480697094662286), np.float64(0.40662095853640484), np.float64(0.4693866005593694), np.float64(0.3755355600598557), np.float64(0.3183718240663793)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 16:36:27,988] Trial 3 finished with value: 0.3835969305376476 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 1.5751924911617186e-06, 'weight_decay': 2.509906197582111e-05, 'batch_size': 32}. Best is trial 3 with value: 0.3835969305376476.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.00016646694541849971, optimizer=Adam, weight_decay=3.3145248147291097e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 5, Fold 1: Test Accuracy = 0.4179\n",
      "Trial 4, Fold 2: Test Accuracy = 0.5365\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 5, Fold 2: Test Accuracy = 0.5526\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 5, Fold 3: Test Accuracy = 0.4893\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 5, Fold 4: Test Accuracy = 0.5164\n",
      "Trial 4, Fold 3: Test Accuracy = 0.4602\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 5, Fold 5: Test Accuracy = 0.5716\n",
      "Trial 5: Mean Accuracy = 0.5096, Fold Accuracies = [np.float64(0.417856995789452), np.float64(0.5525606225158298), np.float64(0.4893401983218917), np.float64(0.5164275177586733), np.float64(0.571646634781615)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 16:49:44,499] Trial 5 finished with value: 0.5095663938334924 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.00016646694541849971, 'weight_decay': 3.3145248147291097e-06, 'batch_size': 16}. Best is trial 5 with value: 0.5095663938334924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.0299303600768504e-05, optimizer=Adam, weight_decay=0.0006900846305446895, batch_size=10,factor=1\n",
      "Trial 6, Fold 1: Test Accuracy = 0.4670\n",
      "Trial 4, Fold 4: Test Accuracy = 0.4898\n",
      "Trial 6, Fold 2: Test Accuracy = 0.5464\n",
      "Trial 4, Fold 5: Test Accuracy = 0.5501\n",
      "Trial 4: Mean Accuracy = 0.4767, Fold Accuracies = [np.float64(0.3469553040388426), np.float64(0.5365276018399229), np.float64(0.46017480294940255), np.float64(0.48981657034739934), np.float64(0.5501147137665606)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 17:02:23,457] Trial 4 finished with value: 0.4767177985884256 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 2.5822436972906645e-06, 'weight_decay': 2.506014907843724e-06, 'batch_size': 10}. Best is trial 5 with value: 0.5095663938334924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=0.0008385782909945929, optimizer=Adam, weight_decay=0.00010889698801610895, batch_size=32,factor=1\n",
      "Trial 6, Fold 3: Test Accuracy = 0.4841\n",
      "Trial 7, Fold 1: Test Accuracy = 0.5102\n",
      "Trial 6, Fold 4: Test Accuracy = 0.5460\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 7, Fold 2: Test Accuracy = 0.5439\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 7, Fold 3: Test Accuracy = 0.3333\n",
      "Trial 6, Fold 5: Test Accuracy = 0.5324\n",
      "Trial 6: Mean Accuracy = 0.5152, Fold Accuracies = [np.float64(0.46698341951113), np.float64(0.5464048329292132), np.float64(0.4841463259598271), np.float64(0.5460395634507517), np.float64(0.5324116128403329)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 17:17:50,428] Trial 6 finished with value: 0.515197150938251 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 1.0299303600768504e-05, 'weight_decay': 0.0006900846305446895, 'batch_size': 10}. Best is trial 6 with value: 0.515197150938251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=4.337438601921003e-08, optimizer=Adam, weight_decay=2.8456638444925105e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 7, Fold 4: Test Accuracy = 0.3343\n",
      "Trial 8, Fold 1: Test Accuracy = 0.3347\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 7, Fold 5: Test Accuracy = 0.3329\n",
      "Trial 7: Mean Accuracy = 0.4109, Fold Accuracies = [np.float64(0.5102352726994918), np.float64(0.5439093686046153), np.float64(0.3333333333333333), np.float64(0.3343160473662719), np.float64(0.33291255878859544)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 17:26:57,352] Trial 7 finished with value: 0.4109413161584615 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 0.0008385782909945929, 'weight_decay': 0.00010889698801610895, 'batch_size': 32}. Best is trial 6 with value: 0.515197150938251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=6.424019532646754e-07, optimizer=Adam, weight_decay=2.033712228107347e-05, batch_size=32,factor=1\n",
      "Trial 8, Fold 2: Test Accuracy = 0.2925\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 8, Fold 3: Test Accuracy = 0.3574\n",
      "Trial 9, Fold 1: Test Accuracy = 0.2917\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 9, Fold 2: Test Accuracy = 0.2619\n",
      "Trial 8, Fold 4: Test Accuracy = 0.2167\n",
      "Trial 9, Fold 3: Test Accuracy = 0.4705\n",
      "Trial 8, Fold 5: Test Accuracy = 0.3543\n",
      "Trial 8: Mean Accuracy = 0.3111, Fold Accuracies = [np.float64(0.3346960315950938), np.float64(0.29245672003689677), np.float64(0.3574377065853038), np.float64(0.21670073503474893), np.float64(0.35428084277189137)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 17:43:35,798] Trial 8 finished with value: 0.31111440720478695 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 4.337438601921003e-08, 'weight_decay': 2.8456638444925105e-05, 'batch_size': 16}. Best is trial 6 with value: 0.515197150938251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=4, learning_rate=6.306265781279087e-08, optimizer=Adam, weight_decay=4.6669157456943815e-06, batch_size=10,factor=1\n",
      "Trial 9, Fold 4: Test Accuracy = 0.4004\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 9, Fold 5: Test Accuracy = 0.4180\n",
      "Trial 9: Mean Accuracy = 0.3685, Fold Accuracies = [np.float64(0.29174184534810577), np.float64(0.2618806188357444), np.float64(0.47047037884566495), np.float64(0.4004237966018285), np.float64(0.4179936551262724)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 17:47:41,130] Trial 9 finished with value: 0.3685020589515232 and parameters: {'num_heads': 32, 'num_transformer_blocks': 4, 'learning_rate': 6.424019532646754e-07, 'weight_decay': 2.033712228107347e-05, 'batch_size': 32}. Best is trial 6 with value: 0.515197150938251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=3.708462569776459e-05, optimizer=Adam, weight_decay=0.0005453180081021676, batch_size=10,factor=1\n",
      "Trial 10, Fold 1: Test Accuracy = 0.3026\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 10, Fold 2: Test Accuracy = 0.3324\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 11, Fold 1: Test Accuracy = 0.4663\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 11, Fold 2: Test Accuracy = 0.6199\n",
      "Trial 10, Fold 3: Test Accuracy = 0.3585\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 10, Fold 4: Test Accuracy = 0.3180\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 11, Fold 3: Test Accuracy = 0.4961\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 10, Fold 5: Test Accuracy = 0.2758\n",
      "Trial 10: Mean Accuracy = 0.3175, Fold Accuracies = [np.float64(0.30263543947425314), np.float64(0.33240611961057026), np.float64(0.35852466310704295), np.float64(0.3179668439366275), np.float64(0.2758004596786224)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:02:01,283] Trial 10 finished with value: 0.3174667051614232 and parameters: {'num_heads': 8, 'num_transformer_blocks': 4, 'learning_rate': 6.306265781279087e-08, 'weight_decay': 4.6669157456943815e-06, 'batch_size': 10}. Best is trial 6 with value: 0.515197150938251.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=2, learning_rate=0.00010680347329042984, optimizer=Adam, weight_decay=0.0009396339864431085, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 12, Fold 1: Test Accuracy = 0.4758\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 11, Fold 4: Test Accuracy = 0.5458\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 12, Fold 2: Test Accuracy = 0.5327\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 11, Fold 5: Test Accuracy = 0.5798\n",
      "Trial 11: Mean Accuracy = 0.5416, Fold Accuracies = [np.float64(0.46626339738286077), np.float64(0.6198555221810561), np.float64(0.4961168319349098), np.float64(0.5458473418293753), np.float64(0.5797712402635183)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:09:55,195] Trial 11 finished with value: 0.5415708667183441 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 3.708462569776459e-05, 'weight_decay': 0.0005453180081021676, 'batch_size': 10}. Best is trial 11 with value: 0.5415708667183441.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=3.2956162020308236e-05, optimizer=Adam, weight_decay=0.0008709679486067379, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 12, Fold 3: Test Accuracy = 0.4340\n",
      "Trial 13, Fold 1: Test Accuracy = 0.4828\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 12, Fold 4: Test Accuracy = 0.5290\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 13, Fold 2: Test Accuracy = 0.5760\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 12, Fold 5: Test Accuracy = 0.5089\n",
      "Trial 12: Mean Accuracy = 0.4961, Fold Accuracies = [np.float64(0.4757900263491119), np.float64(0.5326977131924772), np.float64(0.4340096618357488), np.float64(0.5289859070994228), np.float64(0.5088661864886568)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:21:13,917] Trial 12 finished with value: 0.4960698989930835 and parameters: {'num_heads': 4, 'num_transformer_blocks': 2, 'learning_rate': 0.00010680347329042984, 'weight_decay': 0.0009396339864431085, 'batch_size': 16}. Best is trial 11 with value: 0.5415708667183441.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=2.5972023971506666e-05, optimizer=Adam, weight_decay=0.0009056926882072129, batch_size=10,factor=1\n",
      "Trial 13, Fold 3: Test Accuracy = 0.5132\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 14, Fold 1: Test Accuracy = 0.4903\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 13, Fold 4: Test Accuracy = 0.5353\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 14, Fold 2: Test Accuracy = 0.6250\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 13, Fold 5: Test Accuracy = 0.5303\n",
      "Trial 13: Mean Accuracy = 0.5275, Fold Accuracies = [np.float64(0.4828260736463909), np.float64(0.5760430068036629), np.float64(0.5131642512077295), np.float64(0.5352537919708193), np.float64(0.530283078189811)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:34:44,864] Trial 13 finished with value: 0.5275140403636828 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 3.2956162020308236e-05, 'weight_decay': 0.0008709679486067379, 'batch_size': 10}. Best is trial 11 with value: 0.5415708667183441.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.009989557515373997, optimizer=Adam, weight_decay=0.00025563459484606604, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 14, Fold 3: Test Accuracy = 0.4924\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 15, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 15, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 14, Fold 4: Test Accuracy = 0.5540\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 15, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 15, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 14, Fold 5: Test Accuracy = 0.5553\n",
      "Trial 14: Mean Accuracy = 0.5434, Fold Accuracies = [np.float64(0.49031055161917264), np.float64(0.6250430979067566), np.float64(0.49236270022883294), np.float64(0.5540264723686529), np.float64(0.5553332849046494)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:45:19,017] Trial 14 finished with value: 0.5434152214056128 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 2.5972023971506666e-05, 'weight_decay': 0.0009056926882072129, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=0.009863391108338929, optimizer=Adam, weight_decay=0.00022698964520383795, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 15, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 15: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 18:46:43,957] Trial 15 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.009989557515373997, 'weight_decay': 0.00025563459484606604, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=0.000734538240607978, optimizer=Adam, weight_decay=0.00019545647308208188, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 16, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 17, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 16, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 17, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 16, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 17, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 16, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 16, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 16: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:06:00,086] Trial 16 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 0.009863391108338929, 'weight_decay': 0.00022698964520383795, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00028752549947136787, optimizer=Adam, weight_decay=9.188393039661266e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 17, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 18, Fold 1: Test Accuracy = 0.4746\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 17, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 17: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:11:00,950] Trial 17 finished with value: 0.3333333333333333 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 0.000734538240607978, 'weight_decay': 0.00019545647308208188, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=1.1242050027497732e-05, optimizer=Adam, weight_decay=5.7530109729987256e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 18, Fold 2: Test Accuracy = 0.5363\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 18, Fold 3: Test Accuracy = 0.5218\n",
      "Trial 19, Fold 1: Test Accuracy = 0.4971\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 18, Fold 4: Test Accuracy = 0.5222\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 18, Fold 5: Test Accuracy = 0.5441\n",
      "Trial 18: Mean Accuracy = 0.5198, Fold Accuracies = [np.float64(0.4745599405722683), np.float64(0.5362748581833504), np.float64(0.5217976099669462), np.float64(0.5222216990367256), np.float64(0.54410946744516)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:20:10,832] Trial 18 finished with value: 0.5197927150408901 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00028752549947136787, 'weight_decay': 9.188393039661266e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.4996464618623795e-05, optimizer=Adam, weight_decay=0.0004104892950404676, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 19, Fold 2: Test Accuracy = 0.6179\n",
      "Trial 20, Fold 1: Test Accuracy = 0.4752\n",
      "Trial 19, Fold 3: Test Accuracy = 0.4722\n",
      "Trial 20, Fold 2: Test Accuracy = 0.6334\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 19, Fold 4: Test Accuracy = 0.5045\n",
      "Trial 20, Fold 3: Test Accuracy = 0.4664\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 20, Fold 4: Test Accuracy = 0.5325\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 19, Fold 5: Test Accuracy = 0.5481\n",
      "Trial 19: Mean Accuracy = 0.5280, Fold Accuracies = [np.float64(0.4971262102413141), np.float64(0.6178879088813231), np.float64(0.47216946351385713), np.float64(0.5044930025983517), np.float64(0.5481026854600396)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:40:55,616] Trial 19 finished with value: 0.527955854138977 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 1.1242050027497732e-05, 'weight_decay': 5.7530109729987256e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.0263678879371138e-08, optimizer=Adam, weight_decay=0.00044308597287374317, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 21, Fold 1: Test Accuracy = 0.3021\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 20, Fold 5: Test Accuracy = 0.5262\n",
      "Trial 20: Mean Accuracy = 0.5267, Fold Accuracies = [np.float64(0.47524542114420054), np.float64(0.6333777573295286), np.float64(0.4663723620645818), np.float64(0.5324894187776997), np.float64(0.5261995375463513)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:44:39,963] Trial 20 finished with value: 0.5267368993724725 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 1.4996464618623795e-05, 'weight_decay': 0.0004104892950404676, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=4.1461535743771105e-05, optimizer=Adam, weight_decay=6.430375680204082e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 21, Fold 2: Test Accuracy = 0.3446\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 22, Fold 1: Test Accuracy = 0.4462\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 21, Fold 3: Test Accuracy = 0.2467\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 22, Fold 2: Test Accuracy = 0.5395\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 21, Fold 4: Test Accuracy = 0.3096\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 22, Fold 3: Test Accuracy = 0.4880\n",
      "Trial 21, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 21: Mean Accuracy = 0.3073, Fold Accuracies = [np.float64(0.3021280172388103), np.float64(0.34459625296572033), np.float64(0.24673404525807272), np.float64(0.3096214058076043), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 19:58:53,035] Trial 21 finished with value: 0.3072826109207082 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 1.0263678879371138e-08, 'weight_decay': 0.00044308597287374317, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=4.9632393601790756e-05, optimizer=Adam, weight_decay=7.84905982695952e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 22, Fold 4: Test Accuracy = 0.5111\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 23, Fold 1: Test Accuracy = 0.4347\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 22, Fold 5: Test Accuracy = 0.5942\n",
      "Trial 22: Mean Accuracy = 0.5158, Fold Accuracies = [np.float64(0.44619001215320947), np.float64(0.5394922171286403), np.float64(0.4880008899059242), np.float64(0.5110601332242819), np.float64(0.5942416146427573)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:03:52,274] Trial 22 finished with value: 0.5157969734109626 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 4.1461535743771105e-05, 'weight_decay': 6.430375680204082e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=2.0709265596550155e-06, optimizer=Adam, weight_decay=1.135171282506963e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 23, Fold 2: Test Accuracy = 0.5785\n",
      "Trial 24, Fold 1: Test Accuracy = 0.4590\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 23, Fold 3: Test Accuracy = 0.4441\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 23, Fold 4: Test Accuracy = 0.5063\n",
      "Trial 24, Fold 2: Test Accuracy = 0.4563\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 23, Fold 5: Test Accuracy = 0.5744\n",
      "Trial 23: Mean Accuracy = 0.5076, Fold Accuracies = [np.float64(0.43473352611585486), np.float64(0.5785492596525076), np.float64(0.4440706839562676), np.float64(0.506284561409102), np.float64(0.5743751569624221)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:16:19,432] Trial 23 finished with value: 0.5076026376192309 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 4.9632393601790756e-05, 'weight_decay': 7.84905982695952e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=4.10233883513962e-06, optimizer=Adam, weight_decay=1.0064137036080512e-05, batch_size=10,factor=1\n",
      "Trial 24, Fold 3: Test Accuracy = 0.4808\n",
      "Trial 25, Fold 1: Test Accuracy = 0.3900\n",
      "Trial 24, Fold 4: Test Accuracy = 0.4867\n",
      "Trial 25, Fold 2: Test Accuracy = 0.5103\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 24, Fold 5: Test Accuracy = 0.4021\n",
      "Trial 24: Mean Accuracy = 0.4570, Fold Accuracies = [np.float64(0.45898068273621145), np.float64(0.4563303014583388), np.float64(0.4807754894482583), np.float64(0.48665871261216304), np.float64(0.40212310097009096)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:30:17,626] Trial 24 finished with value: 0.45697365744501256 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 2.0709265596550155e-06, 'weight_decay': 1.135171282506963e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=4.678566744208165e-06, optimizer=Adam, weight_decay=0.000460718740382649, batch_size=10,factor=1\n",
      "Trial 25, Fold 3: Test Accuracy = 0.5312\n",
      "Trial 26, Fold 1: Test Accuracy = 0.4538\n",
      "Trial 25, Fold 4: Test Accuracy = 0.5194\n",
      "Trial 26, Fold 2: Test Accuracy = 0.5528\n",
      "Trial 26, Fold 3: Test Accuracy = 0.5175\n",
      "Trial 25, Fold 5: Test Accuracy = 0.5607\n",
      "Trial 25: Mean Accuracy = 0.5023, Fold Accuracies = [np.float64(0.38996414721909317), np.float64(0.5102722671394438), np.float64(0.5312147215865751), np.float64(0.5193820280059561), np.float64(0.5606520255386561)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 20:48:04,755] Trial 25 finished with value: 0.5022970378979449 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 4.10233883513962e-06, 'weight_decay': 1.0064137036080512e-05, 'batch_size': 10}. Best is trial 14 with value: 0.5434152214056128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=5.43211038647271e-07, optimizer=Adam, weight_decay=0.00048290749032893016, batch_size=10,factor=1\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8, 16, 32])\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [2, 4, 8, 16, 32])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    optimizer_name = \"Adam\"\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    label_smoothing = 0.3\n",
    "    factor = 1\n",
    "\n",
    "    print(f\"Hyperparameters: num_heads={num_heads}, num_transformer_blocks={num_transformer_blocks}, \"\n",
    "          f\"learning_rate={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size},factor={factor}\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "\n",
    "    for test_fold_idx in range(5):\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_fold = fold_indices[test_fold_idx]\n",
    "        remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "        val_fold_idx = test_fold_idx % 4\n",
    "        val_fold = remaining_folds[val_fold_idx]\n",
    "        train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "\n",
    "        test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "        test_labels = labels_folds[test_fold]\n",
    "\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                      torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                    torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), \n",
    "                                     torch.tensor(test_labels, dtype=torch.long))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TRANS_CNN(input_shape=input_shape, num_transformer_blocks=num_transformer_blocks, \n",
    "                          num_heads=num_heads, num_classes=num_classes, embed_dim=128).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "        optimizer = optimizer_cls[optimizer_name](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Trial {trial.number}: Mean Accuracy = {mean_accuracy:.4f}, Fold Accuracies = {fold_accuracies}\")\n",
    "    \n",
    "    trial.set_user_attr(\"fold_accuracies\", fold_accuracies)\n",
    "    trial.report(mean_accuracy, step=0)  # Single report after all folds\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=300, n_jobs=2)       \n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
