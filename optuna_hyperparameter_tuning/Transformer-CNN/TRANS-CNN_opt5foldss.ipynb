{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f610267e",
   "metadata": {},
   "source": [
    "<h1> I AM RUNNING ON NORMALIZED DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"D:\\PYTHONIG\\newwindow\\NOTEBOOKS_2025\\aprilmay2025\\data\\numpy\\melnormalized\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "\n",
    "\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self, debug_mode_flag=False):\n",
    "        super().__init__()\n",
    "        self.debug_mode_flag = debug_mode_flag\n",
    "        \n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces spatial size\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Further reduces spatial size\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling to reduce spatial dimensions \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d((8, 8))  # Keeps a manageable seq_len\n",
    "        self.flatten = nn.Flatten(start_dim=2)  # Keeps batch & channel dims\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.debug_mode_flag: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 1 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if self.debug_mode_flag: print(f\"Block 2 shape: {x.shape}\")\n",
    "        \n",
    "        x = self.global_avg_pool(x)  # (batch, 128, 8, 8)\n",
    "        if self.debug_mode_flag: print(f\"Global Avg Pool shape: {x.shape}\")\n",
    "\n",
    "        # x = self.flatten(x)  # (batch, 128, 64)\n",
    "        # if self.debug_mode_flag: print(f\"Flattened shape (Transformer Input): {x.shape}\")\n",
    "        \n",
    "        return x\n",
    "\n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens → 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "TRANS_CNN                                                    --\n",
       "├─CustomCnn: 1-1                                             --\n",
       "│    └─Sequential: 2-1                                       --\n",
       "│    │    └─Conv2d: 3-1                                      448\n",
       "│    │    └─BatchNorm2d: 3-2                                 32\n",
       "│    │    └─ReLU: 3-3                                        --\n",
       "│    │    └─Conv2d: 3-4                                      4,640\n",
       "│    │    └─BatchNorm2d: 3-5                                 64\n",
       "│    │    └─ReLU: 3-6                                        --\n",
       "│    │    └─MaxPool2d: 3-7                                   --\n",
       "│    └─Sequential: 2-2                                       --\n",
       "│    │    └─Conv2d: 3-8                                      18,496\n",
       "│    │    └─BatchNorm2d: 3-9                                 128\n",
       "│    │    └─ReLU: 3-10                                       --\n",
       "│    │    └─Conv2d: 3-11                                     73,856\n",
       "│    │    └─BatchNorm2d: 3-12                                256\n",
       "│    │    └─ReLU: 3-13                                       --\n",
       "│    │    └─MaxPool2d: 3-14                                  --\n",
       "│    └─AdaptiveAvgPool2d: 2-3                                --\n",
       "│    └─Flatten: 2-4                                          --\n",
       "├─Linear: 1-2                                                164,160\n",
       "├─ModuleList: 1-3                                            --\n",
       "│    └─TransformerEncoder: 2-5                               --\n",
       "│    │    └─MultiheadAttention: 3-15                         410,880\n",
       "│    │    └─Sequential: 3-16                                 41,344\n",
       "│    │    └─LayerNorm: 3-17                                  640\n",
       "│    │    └─LayerNorm: 3-18                                  640\n",
       "│    │    └─Dropout: 3-19                                    --\n",
       "│    │    └─Dropout: 3-20                                    --\n",
       "│    └─TransformerEncoder: 2-6                               --\n",
       "│    │    └─MultiheadAttention: 3-21                         410,880\n",
       "│    │    └─Sequential: 3-22                                 41,344\n",
       "│    │    └─LayerNorm: 3-23                                  640\n",
       "│    │    └─LayerNorm: 3-24                                  640\n",
       "│    │    └─Dropout: 3-25                                    --\n",
       "│    │    └─Dropout: 3-26                                    --\n",
       "│    └─TransformerEncoder: 2-7                               --\n",
       "│    │    └─MultiheadAttention: 3-27                         410,880\n",
       "│    │    └─Sequential: 3-28                                 41,344\n",
       "│    │    └─LayerNorm: 3-29                                  640\n",
       "│    │    └─LayerNorm: 3-30                                  640\n",
       "│    │    └─Dropout: 3-31                                    --\n",
       "│    │    └─Dropout: 3-32                                    --\n",
       "│    └─TransformerEncoder: 2-8                               --\n",
       "│    │    └─MultiheadAttention: 3-33                         410,880\n",
       "│    │    └─Sequential: 3-34                                 41,344\n",
       "│    │    └─LayerNorm: 3-35                                  640\n",
       "│    │    └─LayerNorm: 3-36                                  640\n",
       "│    │    └─Dropout: 3-37                                    --\n",
       "│    │    └─Dropout: 3-38                                    --\n",
       "├─AdaptiveAvgPool1d: 1-4                                     --\n",
       "├─Dropout: 1-5                                               --\n",
       "├─Linear: 1-6                                                102,720\n",
       "├─Linear: 1-7                                                102,720\n",
       "├─Linear: 1-8                                                25,680\n",
       "├─Linear: 1-9                                                243\n",
       "=====================================================================================\n",
       "Total params: 2,307,459\n",
       "Trainable params: 2,307,459\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "model = TRANS_CNN(input_shape=(224,224,20),num_classes=3,num_transformer_blocks=4,embed_dim=320)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (3,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-30 21:27:57,840] A new study created in memory with name: no-name-161658ce-6462-4cc2-8305-bd9f89253656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=6.498686732547997e-08, optimizer=AdamW, weight_decay=4.3860827139279903e-05, batch_size=16,factor=1\n",
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.0003817356210826404, optimizer=SGD, weight_decay=3.473507027614272e-05, batch_size=32,factor=1\n",
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=0.003827867538316409, optimizer=AdamW, weight_decay=5.124889337801125e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 2, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 1, Fold 1: Test Accuracy = 0.3214\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 2, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 0, Fold 1: Test Accuracy = 0.3345\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 1, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 1, Fold 3: Test Accuracy = 0.3506\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 2, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 2, Fold 4: Test Accuracy = 0.3333\n",
      "Trial 0, Fold 2: Test Accuracy = 0.3517\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 1, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 2, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 2: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 21:55:32,260] Trial 2 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 0.003827867538316409, 'optimizer': 'AdamW', 'weight_decay': 5.124889337801125e-06, 'batch_size': 16, 'label_smoothing': 0.25}. Best is trial 2 with value: 0.3333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=1.2071155153016448e-08, optimizer=SGD, weight_decay=3.65061488613633e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 1, Fold 5: Test Accuracy = 0.3258\n",
      "Trial 1: Mean Accuracy = 0.3329, Fold Accuracies = [np.float64(0.32135397070289823), np.float64(0.3333333333333333), np.float64(0.35062990003542654), np.float64(0.3333333333333333), np.float64(0.32576427138933567)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 21:58:54,961] Trial 1 finished with value: 0.3328829617588654 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.0003817356210826404, 'optimizer': 'SGD', 'weight_decay': 3.473507027614272e-05, 'batch_size': 32, 'label_smoothing': 0.27}. Best is trial 2 with value: 0.3333333333333333.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=4.107920367640701e-05, optimizer=SGD, weight_decay=0.000168712366040144, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 3, Fold 1: Test Accuracy = 0.3264\n",
      "Trial 0, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 0, Fold 4: Test Accuracy = 0.3325\n",
      "Trial 4, Fold 1: Test Accuracy = 0.3337\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 0, Fold 5: Test Accuracy = 0.3325\n",
      "Trial 0: Mean Accuracy = 0.3369, Fold Accuracies = [np.float64(0.33451118963486454), np.float64(0.3517441860465116), np.float64(0.3333333333333333), np.float64(0.33245614035087717), np.float64(0.33245468662325633)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:10:20,282] Trial 0 finished with value: 0.3368999071977686 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 6.498686732547997e-08, 'optimizer': 'AdamW', 'weight_decay': 4.3860827139279903e-05, 'batch_size': 16, 'label_smoothing': 0.21}. Best is trial 0 with value: 0.3368999071977686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0018931662692351979, optimizer=SGD, weight_decay=3.7614889566838576e-06, batch_size=10,factor=1\n",
      "Trial 3, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 3, Fold 3: Test Accuracy = 0.3333\n",
      "Trial 4, Fold 2: Test Accuracy = 0.2855\n",
      "Trial 5, Fold 1: Test Accuracy = 0.5264\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 3, Fold 4: Test Accuracy = 0.3329\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 4, Fold 3: Test Accuracy = 0.3455\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 4, Fold 4: Test Accuracy = 0.3324\n",
      "Trial 5, Fold 2: Test Accuracy = 0.5656\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 3, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 3: Mean Accuracy = 0.3319, Fold Accuracies = [np.float64(0.32637245696400624), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3328947368421053), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:30:24,432] Trial 3 finished with value: 0.3318534387612223 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 1.2071155153016448e-08, 'optimizer': 'SGD', 'weight_decay': 3.65061488613633e-06, 'batch_size': 16, 'label_smoothing': 0.2}. Best is trial 0 with value: 0.3368999071977686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=3.168829376349224e-05, optimizer=SGD, weight_decay=1.1862008292243698e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 4, Fold 5: Test Accuracy = 0.3348\n",
      "Trial 4: Mean Accuracy = 0.3264, Fold Accuracies = [np.float64(0.3336592843278754), np.float64(0.2855443370444806), np.float64(0.34549543305146097), np.float64(0.3323886639676113), np.float64(0.3347874435255806)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:31:40,380] Trial 4 finished with value: 0.32637503238340176 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 4.107920367640701e-05, 'optimizer': 'SGD', 'weight_decay': 0.000168712366040144, 'batch_size': 10, 'label_smoothing': 0.04}. Best is trial 0 with value: 0.3368999071977686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.9941911979473215e-05, optimizer=Adam, weight_decay=0.0006233186263273591, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 6, Fold 1: Test Accuracy = 0.3199\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 7, Fold 1: Test Accuracy = 0.4727\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 5, Fold 3: Test Accuracy = 0.4474\n",
      "Trial 6, Fold 2: Test Accuracy = 0.3498\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 7, Fold 2: Test Accuracy = 0.5670\n",
      "Trial 6, Fold 3: Test Accuracy = 0.3263\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 6, Fold 4: Test Accuracy = 0.3285\n",
      "Trial 5, Fold 4: Test Accuracy = 0.4318\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 7, Fold 3: Test Accuracy = 0.5461\n",
      "Trial 6, Fold 5: Test Accuracy = 0.3276\n",
      "Trial 6: Mean Accuracy = 0.3304, Fold Accuracies = [np.float64(0.31986263872990595), np.float64(0.3497517040439808), np.float64(0.3263312693498452), np.float64(0.3285087719298246), np.float64(0.3275655831473821)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:56:33,426] Trial 6 finished with value: 0.3304039934401877 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 3.168829376349224e-05, 'optimizer': 'SGD', 'weight_decay': 1.1862008292243698e-06, 'batch_size': 32, 'label_smoothing': 0.08}. Best is trial 0 with value: 0.3368999071977686.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.0002722963051083165, optimizer=AdamW, weight_decay=9.426789817625876e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 5, Fold 5: Test Accuracy = 0.5528\n",
      "Trial 5: Mean Accuracy = 0.5048, Fold Accuracies = [np.float64(0.5264028932132252), np.float64(0.5656349358258632), np.float64(0.4473954530751814), np.float64(0.4318488529014845), np.float64(0.5527724796723018)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:58:06,854] Trial 5 finished with value: 0.5048109229376112 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0018931662692351979, 'optimizer': 'SGD', 'weight_decay': 3.7614889566838576e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 5 with value: 0.5048109229376112.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=4, learning_rate=1.696925709928159e-07, optimizer=Adam, weight_decay=1.0686534433045452e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 7, Fold 4: Test Accuracy = 0.4621\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 8, Fold 1: Test Accuracy = 0.4568\n",
      "Trial 9, Fold 1: Test Accuracy = 0.3651\n",
      "Trial 7, Fold 5: Test Accuracy = 0.6241\n",
      "Trial 7: Mean Accuracy = 0.5344, Fold Accuracies = [np.float64(0.47271790618070414), np.float64(0.566970789336566), np.float64(0.5461360380758745), np.float64(0.4620782726045884), np.float64(0.624128702772884)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:10:13,099] Trial 7 finished with value: 0.5344063417941234 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.9941911979473215e-05, 'optimizer': 'Adam', 'weight_decay': 0.0006233186263273591, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=0.003056586122628245, optimizer=Adam, weight_decay=7.10523762864986e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 8, Fold 2: Test Accuracy = 0.5425\n",
      "Trial 9, Fold 2: Test Accuracy = 0.3450\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 8, Fold 3: Test Accuracy = 0.5367\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 10, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 9, Fold 3: Test Accuracy = 0.3262\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 8, Fold 4: Test Accuracy = 0.4149\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 8, Fold 5: Test Accuracy = 0.5635\n",
      "Trial 8: Mean Accuracy = 0.5029, Fold Accuracies = [np.float64(0.45682867997146603), np.float64(0.5424814708478216), np.float64(0.5367097176655423), np.float64(0.4148785425101214), np.float64(0.5634656941516574)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:22:08,760] Trial 8 finished with value: 0.5028728210293216 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.0002722963051083165, 'optimizer': 'AdamW', 'weight_decay': 9.426789817625876e-05, 'batch_size': 32, 'label_smoothing': 0.21}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=16, learning_rate=1.989492863360699e-07, optimizer=SGD, weight_decay=8.336991800236238e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 9, Fold 4: Test Accuracy = 0.3269\n",
      "Trial 10, Fold 2: Test Accuracy = 0.3484\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 10, Fold 3: Test Accuracy = 0.3333\n",
      "Trial 9, Fold 5: Test Accuracy = 0.3235\n",
      "Trial 9: Mean Accuracy = 0.3373, Fold Accuracies = [np.float64(0.3650855521823521), np.float64(0.3450130794015376), np.float64(0.32624770882429954), np.float64(0.32685560053981105), np.float64(0.32353555256873673)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:28:57,519] Trial 9 finished with value: 0.3373474987033474 and parameters: {'num_heads': 32, 'num_transformer_blocks': 4, 'learning_rate': 1.696925709928159e-07, 'optimizer': 'Adam', 'weight_decay': 1.0686534433045452e-06, 'batch_size': 32, 'label_smoothing': 0.1}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=1.8229450908749177e-06, optimizer=Adam, weight_decay=0.000970440110339154, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 1: Test Accuracy = 0.4030\n",
      "Trial 11, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 10, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 10, Fold 5: Test Accuracy = 0.4401\n",
      "Trial 10: Mean Accuracy = 0.3577, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.34842249657064467), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.4401044965406118)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:37:10,656] Trial 10 finished with value: 0.35770539862225126 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 0.003056586122628245, 'optimizer': 'Adam', 'weight_decay': 7.10523762864986e-06, 'batch_size': 10, 'label_smoothing': 0.25}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.3655588237011103e-06, optimizer=Adam, weight_decay=0.0009778855640130981, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 11, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 2: Test Accuracy = 0.4124\n",
      "Trial 13, Fold 1: Test Accuracy = 0.3733\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 11, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 12, Fold 3: Test Accuracy = 0.3976\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 11, Fold 4: Test Accuracy = 0.3333\n",
      "Trial 13, Fold 2: Test Accuracy = 0.4704\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 4: Test Accuracy = 0.3259\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 12, Fold 5: Test Accuracy = 0.3805\n",
      "Trial 12: Mean Accuracy = 0.3839, Fold Accuracies = [np.float64(0.4029635085740197), np.float64(0.4123746557352644), np.float64(0.3976381251636554), np.float64(0.32587719298245615), np.float64(0.3804717303781457)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:52:51,381] Trial 12 finished with value: 0.3838650425667083 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 1.8229450908749177e-06, 'optimizer': 'Adam', 'weight_decay': 0.000970440110339154, 'batch_size': 10, 'label_smoothing': 0.0}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.741220038408454e-06, optimizer=Adam, weight_decay=0.0006618504088010355, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 11, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 11: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 23:57:36,649] Trial 11 finished with value: 0.3333333333333333 and parameters: {'num_heads': 8, 'num_transformer_blocks': 16, 'learning_rate': 1.989492863360699e-07, 'optimizer': 'SGD', 'weight_decay': 8.336991800236238e-05, 'batch_size': 16, 'label_smoothing': 0.09}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.032617206017311e-06, optimizer=Adam, weight_decay=0.0009016303463666882, batch_size=10,factor=1\n",
      "Trial 13, Fold 3: Test Accuracy = 0.4256\n",
      "Trial 14, Fold 1: Test Accuracy = 0.4491\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 14, Fold 2: Test Accuracy = 0.3829\n",
      "Trial 15, Fold 1: Test Accuracy = 0.3999\n",
      "Trial 13, Fold 4: Test Accuracy = 0.3950\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 14, Fold 3: Test Accuracy = 0.3708\n",
      "Trial 15, Fold 2: Test Accuracy = 0.4054\n",
      "Trial 14, Fold 4: Test Accuracy = 0.3941\n",
      "Trial 13, Fold 5: Test Accuracy = 0.4832\n",
      "Trial 13: Mean Accuracy = 0.4295, Fold Accuracies = [np.float64(0.37329688061624555), np.float64(0.47037994066417843), np.float64(0.42556990588851407), np.float64(0.3950067476383266), np.float64(0.4831517351552477)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 00:15:33,234] Trial 13 finished with value: 0.4294810419925025 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.3655588237011103e-06, 'optimizer': 'Adam', 'weight_decay': 0.0009778855640130981, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.0004372051002757779, optimizer=Adam, weight_decay=1.2293426807358074e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 14, Fold 5: Test Accuracy = 0.4427\n",
      "Trial 14: Mean Accuracy = 0.4079, Fold Accuracies = [np.float64(0.4490906617561672), np.float64(0.3828768834869897), np.float64(0.37077738243765695), np.float64(0.39409581646423747), np.float64(0.4427217807681029)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 00:17:57,288] Trial 14 finished with value: 0.4079125049826309 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.741220038408454e-06, 'optimizer': 'Adam', 'weight_decay': 0.0006618504088010355, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.00041485436078539357, optimizer=SGD, weight_decay=1.301980895226147e-05, batch_size=10,factor=1\n",
      "Trial 15, Fold 3: Test Accuracy = 0.3826\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 16, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 15, Fold 4: Test Accuracy = 0.3328\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 17, Fold 1: Test Accuracy = 0.3360\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 17, Fold 2: Test Accuracy = 0.3348\n",
      "Trial 15, Fold 5: Test Accuracy = 0.4021\n",
      "Trial 15: Mean Accuracy = 0.3846, Fold Accuracies = [np.float64(0.39987852043553035), np.float64(0.4054003041226699), np.float64(0.38258521633319464), np.float64(0.33279352226720643), np.float64(0.40211125444291235)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 00:30:20,754] Trial 15 finished with value: 0.3845537635203028 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.032617206017311e-06, 'optimizer': 'Adam', 'weight_decay': 0.0009016303463666882, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=0.0008434667508691908, optimizer=SGD, weight_decay=1.3270590777152315e-05, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 16, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 17, Fold 3: Test Accuracy = 0.3448\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 18, Fold 1: Test Accuracy = 0.3341\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 17, Fold 4: Test Accuracy = 0.3337\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 16, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 17, Fold 5: Test Accuracy = 0.3100\n",
      "Trial 17: Mean Accuracy = 0.3318, Fold Accuracies = [np.float64(0.336), np.float64(0.334759572952223), np.float64(0.34478343576236464), np.float64(0.3336707152496626), np.float64(0.3099886961437538)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 00:44:57,424] Trial 17 finished with value: 0.3318404840216008 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.00041485436078539357, 'optimizer': 'SGD', 'weight_decay': 1.301980895226147e-05, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.009995215728797675, optimizer=Adam, weight_decay=0.00020740185416929648, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 18, Fold 2: Test Accuracy = 0.3328\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 16, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 19, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 19, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 16, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 16: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 00:53:08,996] Trial 16 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.0004372051002757779, 'optimizer': 'Adam', 'weight_decay': 1.2293426807358074e-05, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=0.008319053020732235, optimizer=SGD, weight_decay=0.00034183898676053707, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 18, Fold 3: Test Accuracy = 0.3519\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 19, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 20, Fold 1: Test Accuracy = 0.4673\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 19, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 18, Fold 4: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 19, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 19: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:01:36,695] Trial 19 finished with value: 0.3333333333333333 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.009995215728797675, 'optimizer': 'Adam', 'weight_decay': 0.00020740185416929648, 'batch_size': 10, 'label_smoothing': 0.3}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=8, learning_rate=4.340435152830084e-05, optimizer=SGD, weight_decay=2.6712655546719673e-06, batch_size=10,factor=1\n",
      "Trial 20, Fold 2: Test Accuracy = 0.5005\n",
      "Trial 21, Fold 1: Test Accuracy = 0.3410\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 20, Fold 3: Test Accuracy = 0.4968\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 18, Fold 5: Test Accuracy = 0.3622\n",
      "Trial 18: Mean Accuracy = 0.3429, Fold Accuracies = [np.float64(0.334107511184105), np.float64(0.33282158845609894), np.float64(0.35190363969625554), np.float64(0.3333333333333333), np.float64(0.36218948320784344)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:13:49,441] Trial 18 finished with value: 0.34287111117552727 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 0.0008434667508691908, 'optimizer': 'SGD', 'weight_decay': 1.3270590777152315e-05, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=3.578588486008383e-05, optimizer=AdamW, weight_decay=2.460418945385307e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 21, Fold 2: Test Accuracy = 0.3569\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 20, Fold 4: Test Accuracy = 0.4026\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 22, Fold 1: Test Accuracy = 0.5079\n",
      "Trial 21, Fold 3: Test Accuracy = 0.3386\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 20, Fold 5: Test Accuracy = 0.5784\n",
      "Trial 20: Mean Accuracy = 0.4891, Fold Accuracies = [np.float64(0.4672641992512594), np.float64(0.5004545889558809), np.float64(0.49683078415969684), np.float64(0.40256410256410263), np.float64(0.5784152412639063)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:23:50,010] Trial 20 finished with value: 0.48910578323896925 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 0.008319053020732235, 'optimizer': 'SGD', 'weight_decay': 0.00034183898676053707, 'batch_size': 10, 'label_smoothing': 0.11}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=5.2267627767109705e-05, optimizer=AdamW, weight_decay=0.0001015237898324104, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 21, Fold 4: Test Accuracy = 0.2615\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 23, Fold 1: Test Accuracy = 0.5223\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 21, Fold 5: Test Accuracy = 0.2611\n",
      "Trial 21: Mean Accuracy = 0.3118, Fold Accuracies = [np.float64(0.3410084440684152), np.float64(0.3568722684786423), np.float64(0.33862175192150695), np.float64(0.2615047233468286), np.float64(0.26114837798585616)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:31:08,636] Trial 21 finished with value: 0.3118311131602498 and parameters: {'num_heads': 16, 'num_transformer_blocks': 8, 'learning_rate': 4.340435152830084e-05, 'optimizer': 'SGD', 'weight_decay': 2.6712655546719673e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00011090865334840052, optimizer=AdamW, weight_decay=8.25271607634704e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 22, Fold 2: Test Accuracy = 0.5396\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 23, Fold 2: Test Accuracy = 0.5802\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 24, Fold 1: Test Accuracy = 0.4825\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 23, Fold 3: Test Accuracy = 0.5226\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 24, Fold 2: Test Accuracy = 0.5598\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 24, Fold 3: Test Accuracy = 0.5413\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 23, Fold 4: Test Accuracy = 0.4157\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 22, Fold 3: Test Accuracy = 0.5529\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 24, Fold 4: Test Accuracy = 0.4370\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 23, Fold 5: Test Accuracy = 0.6202\n",
      "Trial 23: Mean Accuracy = 0.5322, Fold Accuracies = [np.float64(0.522305767072004), np.float64(0.5802362799204602), np.float64(0.5226180244289389), np.float64(0.41572199730094467), np.float64(0.6202461084043237)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:47:51,008] Trial 23 finished with value: 0.5322256354253343 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 5.2267627767109705e-05, 'optimizer': 'AdamW', 'weight_decay': 0.0001015237898324104, 'batch_size': 32, 'label_smoothing': 0.21}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=1.0151340265762974e-05, optimizer=AdamW, weight_decay=0.00043518444583324307, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 24, Fold 5: Test Accuracy = 0.5877\n",
      "Trial 24: Mean Accuracy = 0.5217, Fold Accuracies = [np.float64(0.4825122292451212), np.float64(0.5598262991673845), np.float64(0.5412762811330345), np.float64(0.4369770580296895), np.float64(0.5876966934578992)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 01:50:36,727] Trial 24 finished with value: 0.5216577122066258 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00011090865334840052, 'optimizer': 'AdamW', 'weight_decay': 8.25271607634704e-05, 'batch_size': 32, 'label_smoothing': 0.22}. Best is trial 7 with value: 0.5344063417941234.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=1.3148345101561702e-05, optimizer=AdamW, weight_decay=0.00036975649958118057, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 22, Fold 4: Test Accuracy = 0.4888\n",
      "Trial 25, Fold 1: Test Accuracy = 0.4657\n",
      "Trial 26, Fold 1: Test Accuracy = 0.5185\n",
      "Trial 25, Fold 2: Test Accuracy = 0.6165\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 22, Fold 5: Test Accuracy = 0.5853\n",
      "Trial 22: Mean Accuracy = 0.5349, Fold Accuracies = [np.float64(0.5078583696920431), np.float64(0.5396210163652024), np.float64(0.5529086764320811), np.float64(0.48879892037786776), np.float64(0.5852917689743637)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 02:03:56,617] Trial 22 finished with value: 0.5348957503683117 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 3.578588486008383e-05, 'optimizer': 'AdamW', 'weight_decay': 2.460418945385307e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=8.058405683953688e-06, optimizer=AdamW, weight_decay=0.0004454995794666148, batch_size=32,factor=1\n",
      "Trial 26, Fold 2: Test Accuracy = 0.6124\n",
      "Trial 25, Fold 3: Test Accuracy = 0.5042\n",
      "Trial 27, Fold 1: Test Accuracy = 0.4894\n",
      "Trial 26, Fold 3: Test Accuracy = 0.4691\n",
      "Trial 25, Fold 4: Test Accuracy = 0.4427\n",
      "Trial 27, Fold 2: Test Accuracy = 0.5899\n",
      "Trial 26, Fold 4: Test Accuracy = 0.4778\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 25, Fold 5: Test Accuracy = 0.4854\n",
      "Trial 25: Mean Accuracy = 0.5029, Fold Accuracies = [np.float64(0.4657065534155067), np.float64(0.616465158813709), np.float64(0.5042067526146358), np.float64(0.4426788124156545), np.float64(0.48542696142132974)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 02:27:51,129] Trial 25 finished with value: 0.5028968477361672 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 1.0151340265762974e-05, 'optimizer': 'AdamW', 'weight_decay': 0.00043518444583324307, 'batch_size': 32, 'label_smoothing': 0.06}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=6.57740502504373e-06, optimizer=AdamW, weight_decay=0.0001900209472561317, batch_size=32,factor=1\n",
      "Trial 27, Fold 3: Test Accuracy = 0.5004\n",
      "Trial 26, Fold 5: Test Accuracy = 0.5657\n",
      "Trial 26: Mean Accuracy = 0.5287, Fold Accuracies = [np.float64(0.5184603593291196), np.float64(0.6124496230367605), np.float64(0.4690946967946645), np.float64(0.477834008097166), np.float64(0.5657497604709438)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 02:36:11,193] Trial 26 finished with value: 0.5287176895457308 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 1.3148345101561702e-05, 'optimizer': 'AdamW', 'weight_decay': 0.00036975649958118057, 'batch_size': 32, 'label_smoothing': 0.23}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=0.0001143953049161876, optimizer=AdamW, weight_decay=2.2118095485811648e-05, batch_size=32,factor=1\n",
      "Trial 28, Fold 1: Test Accuracy = 0.4505\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 29, Fold 1: Test Accuracy = 0.5068\n",
      "Trial 27, Fold 4: Test Accuracy = 0.4690\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 29, Fold 2: Test Accuracy = 0.5691\n",
      "Trial 28, Fold 2: Test Accuracy = 0.5799\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 29, Fold 3: Test Accuracy = 0.5657\n",
      "Trial 27, Fold 5: Test Accuracy = 0.5856\n",
      "Trial 27: Mean Accuracy = 0.5269, Fold Accuracies = [np.float64(0.48943085762316335), np.float64(0.5898996182516135), np.float64(0.5004263512160559), np.float64(0.4689946018893387), np.float64(0.5856426669889773)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 02:52:00,322] Trial 27 finished with value: 0.5268788191938298 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 8.058405683953688e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0004454995794666148, 'batch_size': 32, 'label_smoothing': 0.18}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=0.00014038949212035303, optimizer=AdamW, weight_decay=2.1280745248619434e-05, batch_size=16,factor=1\n",
      "Trial 28, Fold 3: Test Accuracy = 0.4894\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 29, Fold 4: Test Accuracy = 0.4230\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 30, Fold 1: Test Accuracy = 0.5137\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 29, Fold 5: Test Accuracy = 0.5899\n",
      "Trial 29: Mean Accuracy = 0.5309, Fold Accuracies = [np.float64(0.5067524013337978), np.float64(0.5691334098956838), np.float64(0.5657335612956887), np.float64(0.4230094466936573), np.float64(0.5898755970488413)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 03:00:31,417] Trial 29 finished with value: 0.5309008832535338 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 0.0001143953049161876, 'optimizer': 'AdamW', 'weight_decay': 2.2118095485811648e-05, 'batch_size': 32, 'label_smoothing': 0.18}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=0.00011574704437376741, optimizer=AdamW, weight_decay=4.456542835707629e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 30, Fold 2: Test Accuracy = 0.4700\n",
      "Trial 28, Fold 4: Test Accuracy = 0.4512\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 31, Fold 1: Test Accuracy = 0.4807\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 30, Fold 3: Test Accuracy = 0.5910\n",
      "Trial 28, Fold 5: Test Accuracy = 0.5688\n",
      "Trial 28: Mean Accuracy = 0.5080, Fold Accuracies = [np.float64(0.450493958647843), np.float64(0.5799491711062196), np.float64(0.4894183109221693), np.float64(0.45124831309041835), np.float64(0.5687643757333875)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 03:09:44,916] Trial 28 finished with value: 0.5079748259000075 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 6.57740502504373e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0001900209472561317, 'batch_size': 32, 'label_smoothing': 0.18}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=4.3827154502351545e-07, optimizer=AdamW, weight_decay=5.505094401197587e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 31, Fold 2: Test Accuracy = 0.5043\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 30, Fold 4: Test Accuracy = 0.4718\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 30, Fold 5: Test Accuracy = 0.5622\n",
      "Trial 30: Mean Accuracy = 0.5217, Fold Accuracies = [np.float64(0.5137200019907431), np.float64(0.46996124031007747), np.float64(0.5909699952251128), np.float64(0.4718286099865048), np.float64(0.5622400029955293)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 03:17:45,721] Trial 30 finished with value: 0.5217439701015935 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 0.00014038949212035303, 'optimizer': 'AdamW', 'weight_decay': 2.1280745248619434e-05, 'batch_size': 16, 'label_smoothing': 0.17}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=8.646610374182682e-05, optimizer=AdamW, weight_decay=2.197701013609936e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 31, Fold 3: Test Accuracy = 0.5522\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 33, Fold 1: Test Accuracy = 0.4913\n",
      "Trial 32, Fold 1: Test Accuracy = 0.3124\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 33, Fold 2: Test Accuracy = 0.5575\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 31, Fold 4: Test Accuracy = 0.4946\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 32, Fold 2: Test Accuracy = 0.3447\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 33, Fold 3: Test Accuracy = 0.5420\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 32, Fold 3: Test Accuracy = 0.3527\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 31, Fold 5: Test Accuracy = 0.5540\n",
      "Trial 31: Mean Accuracy = 0.5172, Fold Accuracies = [np.float64(0.4806933979218855), np.float64(0.5043239119107623), np.float64(0.5521595582459221), np.float64(0.49463562753036444), np.float64(0.5540099382706279)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 03:34:15,548] Trial 31 finished with value: 0.5171644867759124 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 0.00011574704437376741, 'optimizer': 'AdamW', 'weight_decay': 4.456542835707629e-05, 'batch_size': 16, 'label_smoothing': 0.2}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=2.561696280259492e-05, optimizer=AdamW, weight_decay=5.059535193850758e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 33, Fold 4: Test Accuracy = 0.4602\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 33, Fold 5: Test Accuracy = 0.5859\n",
      "Trial 33: Mean Accuracy = 0.5274, Fold Accuracies = [np.float64(0.49134042259050964), np.float64(0.5574975276740997), np.float64(0.5419616314711273), np.float64(0.46015519568151153), np.float64(0.5859447007618835)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 03:38:47,979] Trial 33 finished with value: 0.5273798956358263 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 8.646610374182682e-05, 'optimizer': 'AdamW', 'weight_decay': 2.197701013609936e-05, 'batch_size': 32, 'label_smoothing': 0.2}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=1.8744860492714645e-05, optimizer=AdamW, weight_decay=5.506733415692717e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 34, Fold 1: Test Accuracy = 0.4835\n",
      "Trial 35, Fold 1: Test Accuracy = 0.4774\n",
      "Trial 34, Fold 2: Test Accuracy = 0.5970\n",
      "Trial 32, Fold 4: Test Accuracy = 0.2814\n",
      "Trial 34, Fold 3: Test Accuracy = 0.5183\n",
      "Trial 35, Fold 2: Test Accuracy = 0.5730\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 35, Fold 3: Test Accuracy = 0.4849\n",
      "Trial 34, Fold 4: Test Accuracy = 0.4157\n",
      "Trial 32, Fold 5: Test Accuracy = 0.2814\n",
      "Trial 32: Mean Accuracy = 0.3145, Fold Accuracies = [np.float64(0.3123503265371578), np.float64(0.3446767899107836), np.float64(0.35274494401059714), np.float64(0.2814102564102564), np.float64(0.28137948062727564)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:03:14,163] Trial 32 finished with value: 0.3145123594992141 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 4.3827154502351545e-07, 'optimizer': 'AdamW', 'weight_decay': 5.505094401197587e-05, 'batch_size': 16, 'label_smoothing': 0.2}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=4, learning_rate=2.3877333129173686e-05, optimizer=AdamW, weight_decay=2.080941910752093e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 36, Fold 1: Test Accuracy = 0.4656\n",
      "Trial 34, Fold 5: Test Accuracy = 0.5876\n",
      "Trial 34: Mean Accuracy = 0.5204, Fold Accuracies = [np.float64(0.48345942478585685), np.float64(0.5969550515200817), np.float64(0.5183070714538761), np.float64(0.4156882591093118), np.float64(0.5875992792139818)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:09:56,170] Trial 34 finished with value: 0.5204018172166216 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 2.561696280259492e-05, 'optimizer': 'AdamW', 'weight_decay': 5.059535193850758e-05, 'batch_size': 32, 'label_smoothing': 0.24}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=1.753357183238428e-05, optimizer=AdamW, weight_decay=2.2178781288808603e-06, batch_size=32,factor=1\n",
      "Trial 35, Fold 4: Test Accuracy = 0.4800\n",
      "Trial 36, Fold 2: Test Accuracy = 0.5816\n",
      "Trial 37, Fold 1: Test Accuracy = 0.4921\n",
      "Trial 35, Fold 5: Test Accuracy = 0.6227\n",
      "Trial 35: Mean Accuracy = 0.5276, Fold Accuracies = [np.float64(0.4774242881711153), np.float64(0.5729508937591051), np.float64(0.48492583522018395), np.float64(0.48002699055330633), np.float64(0.622655523204875)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:22:07,118] Trial 35 finished with value: 0.5275967061817172 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 1.8744860492714645e-05, 'optimizer': 'AdamW', 'weight_decay': 5.506733415692717e-05, 'batch_size': 32, 'label_smoothing': 0.24}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0010075999569536134, optimizer=AdamW, weight_decay=2.17341166951845e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 38, Fold 1: Test Accuracy = 0.3629\n",
      "Trial 36, Fold 3: Test Accuracy = 0.5330\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 38, Fold 2: Test Accuracy = 0.3691\n",
      "Trial 37, Fold 2: Test Accuracy = 0.5807\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 37, Fold 3: Test Accuracy = 0.4891\n",
      "Trial 38, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 36, Fold 4: Test Accuracy = 0.3893\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 38, Fold 4: Test Accuracy = 0.4736\n",
      "Trial 37, Fold 4: Test Accuracy = 0.4414\n",
      "Trial 36, Fold 5: Test Accuracy = 0.6072\n",
      "Trial 36: Mean Accuracy = 0.5153, Fold Accuracies = [np.float64(0.46560878581263787), np.float64(0.5816053636179964), np.float64(0.5330330545415337), np.float64(0.3893049932523616), np.float64(0.6071684942422311)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:46:43,122] Trial 36 finished with value: 0.5153441382933521 and parameters: {'num_heads': 4, 'num_transformer_blocks': 4, 'learning_rate': 2.3877333129173686e-05, 'optimizer': 'AdamW', 'weight_decay': 2.080941910752093e-06, 'batch_size': 32, 'label_smoothing': 0.26}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=0.0011012476502249265, optimizer=AdamW, weight_decay=0.0001262868854363495, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 38, Fold 5: Test Accuracy = 0.5787\n",
      "Trial 38: Mean Accuracy = 0.4235, Fold Accuracies = [np.float64(0.3629091391695283), np.float64(0.36910762327070107), np.float64(0.3333333333333333), np.float64(0.473582995951417), np.float64(0.5787104361930334)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:48:37,447] Trial 38 finished with value: 0.4235287055836026 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0010075999569536134, 'optimizer': 'AdamW', 'weight_decay': 2.17341166951845e-06, 'batch_size': 32, 'label_smoothing': 0.28}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=6.283851841955046e-05, optimizer=AdamW, weight_decay=7.570465825050045e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 39, Fold 1: Test Accuracy = 0.3333\n",
      "Trial 37, Fold 5: Test Accuracy = 0.6029\n",
      "Trial 37: Mean Accuracy = 0.5212, Fold Accuracies = [np.float64(0.4921057084556811), np.float64(0.5806988441211812), np.float64(0.4890580379834573), np.float64(0.4413967611336032), np.float64(0.6028893709763804)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 04:53:31,827] Trial 37 finished with value: 0.5212297445340607 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 1.753357183238428e-05, 'optimizer': 'AdamW', 'weight_decay': 2.2178781288808603e-06, 'batch_size': 32, 'label_smoothing': 0.28}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=4.80299654348983e-05, optimizer=AdamW, weight_decay=0.00013085971158328916, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 40, Fold 1: Test Accuracy = 0.4843\n",
      "Trial 39, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 41, Fold 1: Test Accuracy = 0.5028\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 39, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 40, Fold 2: Test Accuracy = 0.5679\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 41, Fold 2: Test Accuracy = 0.5419\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 39, Fold 4: Test Accuracy = 0.3400\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 39, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 39: Mean Accuracy = 0.3347, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.34001349527665314), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 05:09:10,200] Trial 39 finished with value: 0.3346693657219973 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 0.0011012476502249265, 'optimizer': 'AdamW', 'weight_decay': 0.0001262868854363495, 'batch_size': 32, 'label_smoothing': 0.28}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=5.8049357555621815e-05, optimizer=Adam, weight_decay=6.987874137512187e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 40, Fold 3: Test Accuracy = 0.5666\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 41, Fold 3: Test Accuracy = 0.5208\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 42, Fold 1: Test Accuracy = 0.4580\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 40, Fold 4: Test Accuracy = 0.4376\n",
      "Trial 41, Fold 4: Test Accuracy = 0.4862\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 42, Fold 2: Test Accuracy = 0.5876\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 40, Fold 5: Test Accuracy = 0.5162\n",
      "Trial 40: Mean Accuracy = 0.5145, Fold Accuracies = [np.float64(0.4842545275579666), np.float64(0.5679225018874745), np.float64(0.5666496465043204), np.float64(0.43761808367071525), np.float64(0.5161931251112967)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 05:20:20,899] Trial 40 finished with value: 0.5145275769463546 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 6.283851841955046e-05, 'optimizer': 'AdamW', 'weight_decay': 7.570465825050045e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00021074068667085614, optimizer=AdamW, weight_decay=0.000290420836101534, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 41, Fold 5: Test Accuracy = 0.5289\n",
      "Trial 41: Mean Accuracy = 0.5161, Fold Accuracies = [np.float64(0.5027939193859664), np.float64(0.5418819982773472), np.float64(0.5208302912681176), np.float64(0.48620107962213227), np.float64(0.5288815784653459)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 05:24:46,296] Trial 41 finished with value: 0.516117773403782 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 4.80299654348983e-05, 'optimizer': 'AdamW', 'weight_decay': 0.00013085971158328916, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=6.956048834544209e-06, optimizer=AdamW, weight_decay=0.00026493383619109056, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 43, Fold 1: Test Accuracy = 0.4885\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 44, Fold 1: Test Accuracy = 0.3397\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 42, Fold 3: Test Accuracy = 0.5110\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 43, Fold 2: Test Accuracy = 0.5771\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 43, Fold 3: Test Accuracy = 0.5402\n",
      "Trial 44, Fold 2: Test Accuracy = 0.5768\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 42, Fold 4: Test Accuracy = 0.4395\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 43, Fold 4: Test Accuracy = 0.4719\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 42, Fold 5: Test Accuracy = 0.6093\n",
      "Trial 42: Mean Accuracy = 0.5211, Fold Accuracies = [np.float64(0.45801503010998856), np.float64(0.5875695175508554), np.float64(0.5110057298646088), np.float64(0.4394736842105263), np.float64(0.6092560699121354)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 05:42:37,596] Trial 42 finished with value: 0.5210640063296228 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 5.8049357555621815e-05, 'optimizer': 'Adam', 'weight_decay': 6.987874137512187e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=0.00024727450230046865, optimizer=AdamW, weight_decay=0.0002927795748870205, batch_size=32,factor=1\n",
      "Trial 44, Fold 3: Test Accuracy = 0.5129\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 43, Fold 5: Test Accuracy = 0.5421\n",
      "Trial 43: Mean Accuracy = 0.5239, Fold Accuracies = [np.float64(0.48847636269126343), np.float64(0.5770794121712871), np.float64(0.5402044729910818), np.float64(0.4718960863697706), np.float64(0.5420929511499653)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 05:44:57,043] Trial 43 finished with value: 0.5239498570746737 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00021074068667085614, 'optimizer': 'AdamW', 'weight_decay': 0.000290420836101534, 'batch_size': 32, 'label_smoothing': 0.22}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=4.477761728575685e-06, optimizer=AdamW, weight_decay=0.0006396319331659041, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 45, Fold 1: Test Accuracy = 0.4828\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 45, Fold 2: Test Accuracy = 0.5604\n",
      "Trial 44, Fold 4: Test Accuracy = 0.4196\n",
      "Trial 46, Fold 1: Test Accuracy = 0.4293\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 45, Fold 3: Test Accuracy = 0.5994\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 45, Fold 4: Test Accuracy = 0.4425\n",
      "Trial 44, Fold 5: Test Accuracy = 0.5858\n",
      "Trial 44: Mean Accuracy = 0.4870, Fold Accuracies = [np.float64(0.3396941112714765), np.float64(0.576782998904733), np.float64(0.5129326278822606), np.float64(0.41960188933873144), np.float64(0.5858347334770267)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 06:03:34,666] Trial 44 finished with value: 0.48696927217484565 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 6.956048834544209e-06, 'optimizer': 'AdamW', 'weight_decay': 0.00026493383619109056, 'batch_size': 32, 'label_smoothing': 0.22}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=4, num_transformer_blocks=8, learning_rate=3.2217804699296593e-06, optimizer=AdamW, weight_decay=0.0006208459060514864, batch_size=32,factor=1\n",
      "Trial 46, Fold 2: Test Accuracy = 0.5129\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 45, Fold 5: Test Accuracy = 0.5340\n",
      "Trial 45: Mean Accuracy = 0.5238, Fold Accuracies = [np.float64(0.4827994713471248), np.float64(0.5604164672855457), np.float64(0.5994299400828674), np.float64(0.44254385964912285), np.float64(0.5340307812802637)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 06:05:14,464] Trial 45 finished with value: 0.5238441039289848 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 0.00024727450230046865, 'optimizer': 'AdamW', 'weight_decay': 0.0002927795748870205, 'batch_size': 32, 'label_smoothing': 0.21}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=1.4338089312657102e-05, optimizer=Adam, weight_decay=0.0006154361290634533, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 47, Fold 1: Test Accuracy = 0.3036\n",
      "Trial 46, Fold 3: Test Accuracy = 0.4575\n",
      "Trial 48, Fold 1: Test Accuracy = 0.5149\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 46, Fold 4: Test Accuracy = 0.3243\n",
      "Trial 47, Fold 2: Test Accuracy = 0.5044\n",
      "Trial 48, Fold 2: Test Accuracy = 0.6004\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 46, Fold 5: Test Accuracy = 0.3549\n",
      "Trial 46: Mean Accuracy = 0.4158, Fold Accuracies = [np.float64(0.4292682581551342), np.float64(0.5128879956614668), np.float64(0.45746730742571967), np.float64(0.3242914979757085), np.float64(0.3548710768790732)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 06:27:03,869] Trial 46 finished with value: 0.4157572272194205 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 4.477761728575685e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0006396319331659041, 'batch_size': 32, 'label_smoothing': 0.23}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=1.3387799875203094e-05, optimizer=Adam, weight_decay=2.9290212526135622e-05, batch_size=16,factor=1\n",
      "Trial 47, Fold 3: Test Accuracy = 0.4757\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 48, Fold 3: Test Accuracy = 0.5145\n",
      "Trial 47, Fold 4: Test Accuracy = 0.4062\n",
      "Trial 48, Fold 4: Test Accuracy = 0.4583\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 47, Fold 5: Test Accuracy = 0.4051\n",
      "Trial 47: Mean Accuracy = 0.4190, Fold Accuracies = [np.float64(0.30362056437565316), np.float64(0.5044142980189492), np.float64(0.4756891240392465), np.float64(0.4062415654520917), np.float64(0.40514078781197965)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 06:40:15,196] Trial 47 finished with value: 0.419021267939584 and parameters: {'num_heads': 4, 'num_transformer_blocks': 8, 'learning_rate': 3.2217804699296593e-06, 'optimizer': 'AdamW', 'weight_decay': 0.0006208459060514864, 'batch_size': 32, 'label_smoothing': 0.17}. Best is trial 22 with value: 0.5348957503683117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=1.0372748018962798e-08, optimizer=Adam, weight_decay=3.1608000523714935e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 49, Fold 1: Test Accuracy = 0.4729\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 50, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 48, Fold 5: Test Accuracy = 0.6159\n",
      "Trial 48: Mean Accuracy = 0.5408, Fold Accuracies = [np.float64(0.5148948942970742), np.float64(0.6004402335151688), np.float64(0.5145406866595814), np.float64(0.45826585695006744), np.float64(0.6158961241320106)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 06:46:33,939] Trial 48 finished with value: 0.5408075591107805 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 1.4338089312657102e-05, 'optimizer': 'Adam', 'weight_decay': 0.0006154361290634533, 'batch_size': 32, 'label_smoothing': 0.17}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=7.915684840706254e-07, optimizer=Adam, weight_decay=2.8780173594989446e-05, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 49, Fold 2: Test Accuracy = 0.5687\n",
      "Trial 51, Fold 1: Test Accuracy = 0.3289\n",
      "Trial 50, Fold 2: Test Accuracy = 0.3333\n",
      "Trial 50, Fold 3: Test Accuracy = 0.3309\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 50, Fold 4: Test Accuracy = 0.3272\n",
      "Trial 49, Fold 3: Test Accuracy = 0.4945\n",
      "Trial 51, Fold 2: Test Accuracy = 0.3718\n",
      "Trial 50, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 50: Mean Accuracy = 0.3316, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3309189347380743), np.float64(0.32719298245614037), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 07:16:53,791] Trial 50 finished with value: 0.33162238343884287 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 1.0372748018962798e-08, 'optimizer': 'Adam', 'weight_decay': 3.1608000523714935e-05, 'batch_size': 16, 'label_smoothing': 0.19}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=8.596753716785412e-07, optimizer=Adam, weight_decay=0.00012435297401749572, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 51, Fold 3: Test Accuracy = 0.3336\n",
      "Trial 49, Fold 4: Test Accuracy = 0.4500\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 51, Fold 4: Test Accuracy = 0.3311\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 52, Fold 1: Test Accuracy = 0.3665\n",
      "Trial 49, Fold 5: Test Accuracy = 0.6321\n",
      "Trial 49: Mean Accuracy = 0.5236, Fold Accuracies = [np.float64(0.472932375564735), np.float64(0.568652236790336), np.float64(0.4944789211835559), np.float64(0.4499662618083671), np.float64(0.632105597966586)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 07:32:54,265] Trial 49 finished with value: 0.523627078662716 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 1.3387799875203094e-05, 'optimizer': 'Adam', 'weight_decay': 2.9290212526135622e-05, 'batch_size': 16, 'label_smoothing': 0.17}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=2.5198448093373934e-05, optimizer=Adam, weight_decay=0.0004641556896534395, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 53, Fold 1: Test Accuracy = 0.4555\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 52, Fold 2: Test Accuracy = 0.3465\n",
      "Trial 51, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 51: Mean Accuracy = 0.3397, Fold Accuracies = [np.float64(0.3289275977814275), np.float64(0.37177667187715996), np.float64(0.3336171310629515), np.float64(0.33107287449392714), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 07:44:25,934] Trial 51 finished with value: 0.33974552170975986 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 7.915684840706254e-07, 'optimizer': 'Adam', 'weight_decay': 2.8780173594989446e-05, 'batch_size': 16, 'label_smoothing': 0.08}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.347315664279605e-05, optimizer=Adam, weight_decay=0.0004830915921433187, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 53, Fold 2: Test Accuracy = 0.6103\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 54, Fold 1: Test Accuracy = 0.5086\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 52, Fold 3: Test Accuracy = 0.3433\n",
      "Trial 53, Fold 3: Test Accuracy = 0.5617\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 54, Fold 2: Test Accuracy = 0.6073\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 54, Fold 3: Test Accuracy = 0.4874\n",
      "Trial 53, Fold 4: Test Accuracy = 0.4530\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 54, Fold 4: Test Accuracy = 0.4278\n",
      "Trial 53, Fold 5: Test Accuracy = 0.6209\n",
      "Trial 53: Mean Accuracy = 0.5403, Fold Accuracies = [np.float64(0.4554750631784425), np.float64(0.6103029529673227), np.float64(0.5617229641267346), np.float64(0.45296896086369776), np.float64(0.6208862721449327)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:11:21,416] Trial 53 finished with value: 0.5402712426562262 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 2.5198448093373934e-05, 'optimizer': 'Adam', 'weight_decay': 0.0004641556896534395, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.3514963145013645e-05, optimizer=Adam, weight_decay=0.0005430868463338111, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 54, Fold 5: Test Accuracy = 0.6420\n",
      "Trial 54: Mean Accuracy = 0.5346, Fold Accuracies = [np.float64(0.5086215763367009), np.float64(0.6072816643804299), np.float64(0.4874495941345902), np.float64(0.4277665317139001), np.float64(0.6419595200980718)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:12:50,300] Trial 54 finished with value: 0.5346157773327385 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.347315664279605e-05, 'optimizer': 'Adam', 'weight_decay': 0.0004830915921433187, 'batch_size': 32, 'label_smoothing': 0.15}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.719033074722906e-05, optimizer=Adam, weight_decay=0.0005467665438572343, batch_size=10,factor=1\n",
      "Trial 52, Fold 4: Test Accuracy = 0.3199\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 55, Fold 1: Test Accuracy = 0.4780\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 56, Fold 1: Test Accuracy = 0.5031\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 55, Fold 2: Test Accuracy = 0.5842\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 52, Fold 5: Test Accuracy = 0.2978\n",
      "Trial 52: Mean Accuracy = 0.3348, Fold Accuracies = [np.float64(0.36645766076632547), np.float64(0.34651508384640745), np.float64(0.3432835820895523), np.float64(0.3199055330634278), np.float64(0.2977802105427781)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:27:06,621] Trial 52 finished with value: 0.33478841406169824 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 8.596753716785412e-07, 'optimizer': 'Adam', 'weight_decay': 0.00012435297401749572, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.7401056457789475e-08, optimizer=Adam, weight_decay=0.0005343518684455463, batch_size=32,factor=1\n",
      "Trial 55, Fold 3: Test Accuracy = 0.4854\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 56, Fold 2: Test Accuracy = 0.5683\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 57, Fold 1: Test Accuracy = 0.3234\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 57, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 55, Fold 4: Test Accuracy = 0.4293\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 56, Fold 3: Test Accuracy = 0.5274\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 55, Fold 5: Test Accuracy = 0.6152\n",
      "Trial 55: Mean Accuracy = 0.5184, Fold Accuracies = [np.float64(0.4779964277222029), np.float64(0.5842052934358418), np.float64(0.4854080988247616), np.float64(0.42925101214574896), np.float64(0.6152275547948093)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:40:22,802] Trial 55 finished with value: 0.5184176773846729 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.3514963145013645e-05, 'optimizer': 'Adam', 'weight_decay': 0.0005430868463338111, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=5.5818461769512103e-08, optimizer=Adam, weight_decay=0.0008163073710661637, batch_size=10,factor=1\n",
      "Trial 57, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 57, Fold 4: Test Accuracy = 0.3450\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 56, Fold 4: Test Accuracy = 0.4485\n",
      "Trial 57, Fold 5: Test Accuracy = 0.3300\n",
      "Trial 57: Mean Accuracy = 0.3330, Fold Accuracies = [np.float64(0.32341832700166445), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3449730094466936), np.float64(0.3299621058591161)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:48:56,172] Trial 57 finished with value: 0.3330040217948282 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.7401056457789475e-08, 'optimizer': 'Adam', 'weight_decay': 0.0005343518684455463, 'batch_size': 32, 'label_smoothing': 0.11}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.494528343337859e-05, optimizer=Adam, weight_decay=0.0008115136328617926, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 56, Fold 5: Test Accuracy = 0.5731\n",
      "Trial 56: Mean Accuracy = 0.5241, Fold Accuracies = [np.float64(0.5030559011706676), np.float64(0.5682787294903288), np.float64(0.5273875206013278), np.float64(0.4485492577597841), np.float64(0.5730783630444297)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 08:51:13,751] Trial 56 finished with value: 0.5240699544133076 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.719033074722906e-05, 'optimizer': 'Adam', 'weight_decay': 0.0005467665438572343, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=4.589621409907898e-06, optimizer=Adam, weight_decay=0.0008298505092395885, batch_size=10,factor=1\n",
      "Trial 58, Fold 1: Test Accuracy = 0.3147\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 59, Fold 1: Test Accuracy = 0.5049\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 58, Fold 2: Test Accuracy = 0.3501\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 59, Fold 2: Test Accuracy = 0.5443\n",
      "Trial 60, Fold 1: Test Accuracy = 0.4743\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 59, Fold 3: Test Accuracy = 0.5512\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 58, Fold 3: Test Accuracy = 0.3664\n",
      "Trial 60, Fold 2: Test Accuracy = 0.5086\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 58, Fold 4: Test Accuracy = 0.3149\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 59, Fold 4: Test Accuracy = 0.4553\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 60, Fold 3: Test Accuracy = 0.5073\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 59, Fold 5: Test Accuracy = 0.5413\n",
      "Trial 59: Mean Accuracy = 0.5194, Fold Accuracies = [np.float64(0.504880837439241), np.float64(0.5443264108208122), np.float64(0.5512384671071885), np.float64(0.4552631578947368), np.float64(0.5412520214489295)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 09:22:29,549] Trial 59 finished with value: 0.5193921789421816 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.494528343337859e-05, 'optimizer': 'Adam', 'weight_decay': 0.0008115136328617926, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=8, learning_rate=3.726937697069053e-06, optimizer=Adam, weight_decay=0.00023455884519748126, batch_size=32,factor=1\n",
      "Trial 58, Fold 5: Test Accuracy = 0.3296\n",
      "Trial 58: Mean Accuracy = 0.3351, Fold Accuracies = [np.float64(0.3146807788229179), np.float64(0.3500707138375814), np.float64(0.36635291037074685), np.float64(0.31491228070175437), np.float64(0.32957608744603784)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 09:22:58,784] Trial 58 finished with value: 0.33511855423580766 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 5.5818461769512103e-08, 'optimizer': 'Adam', 'weight_decay': 0.0008163073710661637, 'batch_size': 10, 'label_smoothing': 0.1}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=4.426700850402441e-06, optimizer=Adam, weight_decay=0.000398651670193739, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 61, Fold 1: Test Accuracy = 0.3638\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 61, Fold 2: Test Accuracy = 0.4019\n",
      "Trial 60, Fold 4: Test Accuracy = 0.3837\n",
      "Trial 62, Fold 1: Test Accuracy = 0.4535\n",
      "Trial 61, Fold 3: Test Accuracy = 0.4951\n",
      "Trial 62, Fold 2: Test Accuracy = 0.5202\n",
      "Trial 61, Fold 4: Test Accuracy = 0.4273\n",
      "Trial 60, Fold 5: Test Accuracy = 0.5864\n",
      "Trial 60: Mean Accuracy = 0.4921, Fold Accuracies = [np.float64(0.4743072269502369), np.float64(0.5085747174104912), np.float64(0.5072712289943472), np.float64(0.3837044534412955), np.float64(0.5864483191717996)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 09:43:07,787] Trial 60 finished with value: 0.49206118919363406 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 4.589621409907898e-06, 'optimizer': 'Adam', 'weight_decay': 0.0008298505092395885, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=8.342369264783816e-05, optimizer=Adam, weight_decay=0.00021456746116437387, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 62, Fold 3: Test Accuracy = 0.4173\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 63, Fold 1: Test Accuracy = 0.4911\n",
      "Trial 61, Fold 5: Test Accuracy = 0.5337\n",
      "Trial 61: Mean Accuracy = 0.4444, Fold Accuracies = [np.float64(0.36383452501423935), np.float64(0.40191246371263595), np.float64(0.49509518968624366), np.float64(0.42726045883940617), np.float64(0.533729218866602)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 09:50:15,194] Trial 61 finished with value: 0.4443663712238254 and parameters: {'num_heads': 32, 'num_transformer_blocks': 8, 'learning_rate': 3.726937697069053e-06, 'optimizer': 'Adam', 'weight_decay': 0.00023455884519748126, 'batch_size': 32, 'label_smoothing': 0.1}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=0.00016732111569474857, optimizer=Adam, weight_decay=0.00036584193600764227, batch_size=32,factor=1\n",
      "Trial 62, Fold 4: Test Accuracy = 0.3580\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 63, Fold 2: Test Accuracy = 0.5470\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 64, Fold 1: Test Accuracy = 0.4954\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 62, Fold 5: Test Accuracy = 0.3842\n",
      "Trial 62: Mean Accuracy = 0.4266, Fold Accuracies = [np.float64(0.4535119693425571), np.float64(0.5201906615199753), np.float64(0.4173239067818801), np.float64(0.3579622132253711), np.float64(0.3842303572653338)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 09:55:48,641] Trial 62 finished with value: 0.42664382162702347 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 4.426700850402441e-06, 'optimizer': 'Adam', 'weight_decay': 0.000398651670193739, 'batch_size': 32, 'label_smoothing': 0.14}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.0001588556387141944, optimizer=Adam, weight_decay=7.260229488993113e-05, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 64, Fold 2: Test Accuracy = 0.5458\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 63, Fold 3: Test Accuracy = 0.5501\n",
      "Trial 65, Fold 1: Test Accuracy = 0.4549\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 64, Fold 3: Test Accuracy = 0.5377\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 65, Fold 2: Test Accuracy = 0.4977\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 63, Fold 4: Test Accuracy = 0.4718\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 64, Fold 4: Test Accuracy = 0.4545\n",
      "Trial 65, Fold 3: Test Accuracy = 0.5217\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 63, Fold 5: Test Accuracy = 0.5899\n",
      "Trial 63: Mean Accuracy = 0.5300, Fold Accuracies = [np.float64(0.4911305982735834), np.float64(0.5470114099169511), np.float64(0.5500909538992346), np.float64(0.47176113360323885), np.float64(0.5898697257319329)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:12:09,689] Trial 63 finished with value: 0.5299727642849883 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 8.342369264783816e-05, 'optimizer': 'Adam', 'weight_decay': 0.00021456746116437387, 'batch_size': 32, 'label_smoothing': 0.19}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=1.1359160681745147e-05, optimizer=Adam, weight_decay=1.5504709098853087e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 64, Fold 5: Test Accuracy = 0.5947\n",
      "Trial 64: Mean Accuracy = 0.5256, Fold Accuracies = [np.float64(0.49540436968098334), np.float64(0.5458390489254686), np.float64(0.537691573094281), np.float64(0.4544871794871795), np.float64(0.5947318955751721)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:12:40,745] Trial 64 finished with value: 0.525630813352617 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 0.00016732111569474857, 'optimizer': 'Adam', 'weight_decay': 0.00036584193600764227, 'batch_size': 32, 'label_smoothing': 0.19}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0003646834140841682, optimizer=SGD, weight_decay=0.00014994779827544056, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 65, Fold 4: Test Accuracy = 0.4495\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 65, Fold 5: Test Accuracy = 0.6015\n",
      "Trial 65: Mean Accuracy = 0.5051, Fold Accuracies = [np.float64(0.454853155051234), np.float64(0.4976871789963952), np.float64(0.521720961754694), np.float64(0.4494939271255061), np.float64(0.6015030951680765)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:17:50,727] Trial 65 finished with value: 0.5050516636191812 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.0001588556387141944, 'optimizer': 'Adam', 'weight_decay': 7.260229488993113e-05, 'batch_size': 32, 'label_smoothing': 0.18}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=8, learning_rate=0.0003221746117994415, optimizer=SGD, weight_decay=3.839044160924209e-06, batch_size=32,factor=1\n",
      "Trial 66, Fold 1: Test Accuracy = 0.4760\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 67, Fold 1: Test Accuracy = 0.3387\n",
      "Trial 68, Fold 1: Test Accuracy = 0.3551\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 67, Fold 2: Test Accuracy = 0.2920\n",
      "Trial 66, Fold 2: Test Accuracy = 0.5712\n",
      "Trial 68, Fold 2: Test Accuracy = 0.3365\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 67, Fold 3: Test Accuracy = 0.3561\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 67, Fold 4: Test Accuracy = 0.3386\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 67, Fold 5: Test Accuracy = 0.3708\n",
      "Trial 67: Mean Accuracy = 0.3392, Fold Accuracies = [np.float64(0.33874948157733203), np.float64(0.29197637200795395), np.float64(0.356089136361536), np.float64(0.3385964912280702), np.float64(0.3707614156965639)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:37:34,966] Trial 67 finished with value: 0.3392345793742912 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0003646834140841682, 'optimizer': 'SGD', 'weight_decay': 0.00014994779827544056, 'batch_size': 32, 'label_smoothing': 0.15}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=1.2292958406478385e-05, optimizer=Adam, weight_decay=5.221031733062504e-06, batch_size=32,factor=1\n",
      "Trial 66, Fold 3: Test Accuracy = 0.5350\n",
      "Trial 68, Fold 3: Test Accuracy = 0.3329\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 66, Fold 4: Test Accuracy = 0.4226\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 68, Fold 4: Test Accuracy = 0.2750\n",
      "Trial 69, Fold 1: Test Accuracy = 0.4929\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 66, Fold 5: Test Accuracy = 0.3543\n",
      "Trial 66: Mean Accuracy = 0.4718, Fold Accuracies = [np.float64(0.4760060828259704), np.float64(0.571191023064408), np.float64(0.5350275711227146), np.float64(0.4226045883940621), np.float64(0.3542888489782676)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:45:57,300] Trial 66 finished with value: 0.4718236228770845 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 1.1359160681745147e-05, 'optimizer': 'Adam', 'weight_decay': 1.5504709098853087e-06, 'batch_size': 32, 'label_smoothing': 0.19}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=8, learning_rate=7.701631712505597e-05, optimizer=SGD, weight_decay=5.234077705449512e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 68, Fold 5: Test Accuracy = 0.3067\n",
      "Trial 68: Mean Accuracy = 0.3212, Fold Accuracies = [np.float64(0.35506826589691265), np.float64(0.33647823821524653), np.float64(0.3328650863330406), np.float64(0.27503373819163296), np.float64(0.3067459430068456)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:48:02,059] Trial 68 finished with value: 0.32123825432873565 and parameters: {'num_heads': 8, 'num_transformer_blocks': 8, 'learning_rate': 0.0003221746117994415, 'optimizer': 'SGD', 'weight_decay': 3.839044160924209e-06, 'batch_size': 32, 'label_smoothing': 0.15}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=32, learning_rate=0.0005649187449794972, optimizer=Adam, weight_decay=0.00010425284365765631, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 70, Fold 1: Test Accuracy = 0.3385\n",
      "Trial 69, Fold 2: Test Accuracy = 0.6450\n",
      "Trial 70, Fold 2: Test Accuracy = 0.3474\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 71, Fold 1: Test Accuracy = 0.3333\n",
      "Trial 69, Fold 3: Test Accuracy = 0.5213\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 70, Fold 3: Test Accuracy = 0.3355\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 70, Fold 4: Test Accuracy = 0.4373\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 70, Fold 5: Test Accuracy = 0.3424\n",
      "Trial 70: Mean Accuracy = 0.3602, Fold Accuracies = [np.float64(0.33848674773414733), np.float64(0.34741761572080265), np.float64(0.33552816413289593), np.float64(0.437280701754386), np.float64(0.3423666024807687)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:05:26,765] Trial 70 finished with value: 0.3602159663646002 and parameters: {'num_heads': 2, 'num_transformer_blocks': 8, 'learning_rate': 7.701631712505597e-05, 'optimizer': 'SGD', 'weight_decay': 5.234077705449512e-06, 'batch_size': 32, 'label_smoothing': 0.04}. Best is trial 48 with value: 0.5408075591107805.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=32, learning_rate=0.0005260161561029145, optimizer=Adam, weight_decay=1.597586695911942e-05, batch_size=10,factor=1\n",
      "Trial 69, Fold 4: Test Accuracy = 0.4156\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 71, Fold 2: Test Accuracy = 0.3333\n",
      "Trial 69, Fold 5: Test Accuracy = 0.6555\n",
      "Trial 69: Mean Accuracy = 0.5460, Fold Accuracies = [np.float64(0.4928563955385236), np.float64(0.6449646962495081), np.float64(0.5212646519723365), np.float64(0.41562078272604586), np.float64(0.6554541879582532)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:10:26,187] Trial 69 finished with value: 0.5460321428889335 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 1.2292958406478385e-05, 'optimizer': 'Adam', 'weight_decay': 5.221031733062504e-06, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=2.4078508508286425e-05, optimizer=Adam, weight_decay=1.3991181350094591e-06, batch_size=32,factor=1\n",
      "Trial 73, Fold 1: Test Accuracy = 0.5356\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 71, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 72, Fold 1: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 73, Fold 2: Test Accuracy = 0.5558\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 71, Fold 4: Test Accuracy = 0.3333\n",
      "Trial 73, Fold 3: Test Accuracy = 0.4810\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 71, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 71: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:29:13,504] Trial 71 finished with value: 0.3333333333333333 and parameters: {'num_heads': 2, 'num_transformer_blocks': 32, 'learning_rate': 0.0005649187449794972, 'optimizer': 'Adam', 'weight_decay': 0.00010425284365765631, 'batch_size': 32, 'label_smoothing': 0.12}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=1.909050216241631e-05, optimizer=Adam, weight_decay=1.6244012857684503e-06, batch_size=32,factor=1\n",
      "Trial 73, Fold 4: Test Accuracy = 0.4595\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 72, Fold 2: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 74, Fold 1: Test Accuracy = 0.4623\n",
      "Trial 73, Fold 5: Test Accuracy = 0.6206\n",
      "Trial 73: Mean Accuracy = 0.5305, Fold Accuracies = [np.float64(0.5356373308559642), np.float64(0.5558200678427495), np.float64(0.48099725829059037), np.float64(0.45951417004048584), np.float64(0.6206413224574051)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:24,391] Trial 73 finished with value: 0.530522029897439 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 2.4078508508286425e-05, 'optimizer': 'Adam', 'weight_decay': 1.3991181350094591e-06, 'batch_size': 32, 'label_smoothing': 0.12}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=1.8612580136166856e-05, optimizer=Adam, weight_decay=9.840346656954297e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 74, Fold 2: Test Accuracy = 0.6122\n",
      "Trial 75, Fold 1: Test Accuracy = 0.4832\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 72, Fold 3: Test Accuracy = 0.3333\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 74, Fold 3: Test Accuracy = 0.5180\n",
      "Trial 75, Fold 2: Test Accuracy = 0.5604\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 75, Fold 3: Test Accuracy = 0.5008\n",
      "Trial 74, Fold 4: Test Accuracy = 0.4833\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 72, Fold 4: Test Accuracy = 0.3333\n",
      "Trial 75, Fold 4: Test Accuracy = 0.4610\n",
      "Trial 74, Fold 5: Test Accuracy = 0.5958\n",
      "Trial 74: Mean Accuracy = 0.5343, Fold Accuracies = [np.float64(0.4622610638309637), np.float64(0.6122449250858668), np.float64(0.5179823791260416), np.float64(0.48333333333333334), np.float64(0.5958219096939527)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 12:02:20,724] Trial 74 finished with value: 0.5343287222140316 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 1.909050216241631e-05, 'optimizer': 'Adam', 'weight_decay': 1.6244012857684503e-06, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=1.6019548422019688e-05, optimizer=Adam, weight_decay=3.471489469857592e-06, batch_size=32,factor=1\n",
      "Trial 75, Fold 5: Test Accuracy = 0.6324\n",
      "Trial 75: Mean Accuracy = 0.5276, Fold Accuracies = [np.float64(0.48323703666838097), np.float64(0.5604483682649057), np.float64(0.5008104215763289), np.float64(0.4609649122807018), np.float64(0.6323893066151697)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 12:06:36,325] Trial 75 finished with value: 0.5275700090810973 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 1.8612580136166856e-05, 'optimizer': 'Adam', 'weight_decay': 9.840346656954297e-06, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=8.734462674314478e-06, optimizer=Adam, weight_decay=3.454464701825009e-06, batch_size=10,factor=1\n",
      "Trial 76, Fold 1: Test Accuracy = 0.5086\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 72, Fold 5: Test Accuracy = 0.3333\n",
      "Trial 72: Mean Accuracy = 0.3333, Fold Accuracies = [np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333), np.float64(0.3333333333333333)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 12:11:54,460] Trial 72 finished with value: 0.3333333333333333 and parameters: {'num_heads': 32, 'num_transformer_blocks': 32, 'learning_rate': 0.0005260161561029145, 'optimizer': 'Adam', 'weight_decay': 1.597586695911942e-05, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=9.751851100150189e-06, optimizer=Adam, weight_decay=2.9261036765171518e-06, batch_size=32,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 76, Fold 2: Test Accuracy = 0.5651\n",
      "Trial 77, Fold 1: Test Accuracy = 0.4852\n",
      "Trial 78, Fold 1: Test Accuracy = 0.4516\n",
      "Trial 76, Fold 3: Test Accuracy = 0.5333\n",
      "Trial 78, Fold 2: Test Accuracy = 0.5827\n",
      "Trial 76, Fold 4: Test Accuracy = 0.4491\n",
      "Trial 77, Fold 2: Test Accuracy = 0.5748\n",
      "Trial 78, Fold 3: Test Accuracy = 0.4892\n",
      "Trial 76, Fold 5: Test Accuracy = 0.5976\n",
      "Trial 76: Mean Accuracy = 0.5307, Fold Accuracies = [np.float64(0.5085709229858933), np.float64(0.5650873023468487), np.float64(0.5332886650339633), np.float64(0.449055330634278), np.float64(0.5975944271907666)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 12:35:00,046] Trial 76 finished with value: 0.5307193296383501 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 1.6019548422019688e-05, 'optimizer': 'Adam', 'weight_decay': 3.471489469857592e-06, 'batch_size': 32, 'label_smoothing': 0.16}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=7.401036929348286e-06, optimizer=Adam, weight_decay=1.0387395714813444e-06, batch_size=32,factor=1\n",
      "Trial 78, Fold 4: Test Accuracy = 0.4691\n",
      "Trial 79, Fold 1: Test Accuracy = 0.4476\n",
      "Trial 77, Fold 3: Test Accuracy = 0.5194\n",
      "Trial 78, Fold 5: Test Accuracy = 0.6189\n",
      "Trial 78: Mean Accuracy = 0.5223, Fold Accuracies = [np.float64(0.45157997533690564), np.float64(0.5827099350283387), np.float64(0.4891509172404233), np.float64(0.46906207827260454), np.float64(0.6188733614332906)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 12:46:06,043] Trial 78 finished with value: 0.5222752534623126 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 9.751851100150189e-06, 'optimizer': 'Adam', 'weight_decay': 2.9261036765171518e-06, 'batch_size': 32, 'label_smoothing': 0.17}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=5.1600777967054974e-05, optimizer=Adam, weight_decay=1.094870001803517e-06, batch_size=10,factor=1\n",
      "Trial 79, Fold 2: Test Accuracy = 0.6221\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 80, Fold 1: Test Accuracy = 0.4758\n",
      "Trial 77, Fold 4: Test Accuracy = 0.4871\n",
      "Trial 79, Fold 3: Test Accuracy = 0.5245\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 80, Fold 2: Test Accuracy = 0.5267\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 79, Fold 4: Test Accuracy = 0.4126\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 79, Fold 5: Test Accuracy = 0.4059\n",
      "Trial 79: Mean Accuracy = 0.4825, Fold Accuracies = [np.float64(0.44761547692120524), np.float64(0.6221023277081273), np.float64(0.5245322150855629), np.float64(0.4125843454790823), np.float64(0.4058500842418204)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:01:40,778] Trial 79 finished with value: 0.4825368898871596 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 7.401036929348286e-06, 'optimizer': 'Adam', 'weight_decay': 1.0387395714813444e-06, 'batch_size': 32, 'label_smoothing': 0.14}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=5.380157707158239e-05, optimizer=Adam, weight_decay=0.0009833215008215829, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 80, Fold 3: Test Accuracy = 0.6058\n",
      "Trial 77, Fold 5: Test Accuracy = 0.6190\n",
      "Trial 77: Mean Accuracy = 0.5371, Fold Accuracies = [np.float64(0.4852494124543097), np.float64(0.5748024797694623), np.float64(0.5194474223310691), np.float64(0.4871120107962213), np.float64(0.6189960388788351)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:04:27,013] Trial 77 finished with value: 0.5371214728459796 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 8.734462674314478e-06, 'optimizer': 'Adam', 'weight_decay': 3.454464701825009e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=2.341357311510307e-06, optimizer=Adam, weight_decay=5.284994698615909e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 81, Fold 1: Test Accuracy = 0.4906\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 80, Fold 4: Test Accuracy = 0.5218\n",
      "Trial 82, Fold 1: Test Accuracy = 0.4245\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 80, Fold 5: Test Accuracy = 0.5952\n",
      "Trial 80: Mean Accuracy = 0.5451, Fold Accuracies = [np.float64(0.4757703788494611), np.float64(0.5267170702140556), np.float64(0.6057921691850345), np.float64(0.5217948717948718), np.float64(0.5951785125943139)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:12:42,171] Trial 80 finished with value: 0.5450506005275474 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 5.1600777967054974e-05, 'optimizer': 'Adam', 'weight_decay': 1.094870001803517e-06, 'batch_size': 10, 'label_smoothing': 0.21}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.5648532646044315e-05, optimizer=Adam, weight_decay=1.6214656573609924e-06, batch_size=10,factor=1\n",
      "Trial 81, Fold 2: Test Accuracy = 0.5973\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 82, Fold 2: Test Accuracy = 0.3866\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 83, Fold 1: Test Accuracy = 0.5084\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 81, Fold 3: Test Accuracy = 0.5033\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 81, Fold 4: Test Accuracy = 0.4413\n",
      "Trial 83, Fold 2: Test Accuracy = 0.5369\n",
      "Trial 82, Fold 3: Test Accuracy = 0.4859\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 82, Fold 4: Test Accuracy = 0.3327\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 81, Fold 5: Test Accuracy = 0.6088\n",
      "Trial 81: Mean Accuracy = 0.5283, Fold Accuracies = [np.float64(0.49059548654313007), np.float64(0.5973072915005158), np.float64(0.5033411117785684), np.float64(0.44129554655870445), np.float64(0.6087660464951735)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:30:38,552] Trial 81 finished with value: 0.5282610965752185 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 5.380157707158239e-05, 'optimizer': 'Adam', 'weight_decay': 0.0009833215008215829, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.3607299710569063e-06, optimizer=Adam, weight_decay=1.5919650870588936e-06, batch_size=10,factor=1\n",
      "Trial 83, Fold 3: Test Accuracy = 0.5682\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 82, Fold 5: Test Accuracy = 0.3841\n",
      "Trial 82: Mean Accuracy = 0.4028, Fold Accuracies = [np.float64(0.42448839562700114), np.float64(0.38659600599738414), np.float64(0.4858949832878949), np.float64(0.3326923076923077), np.float64(0.3840825985462917)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:31:18,526] Trial 82 finished with value: 0.40275085823017587 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 2.341357311510307e-06, 'optimizer': 'Adam', 'weight_decay': 5.284994698615909e-06, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.6350527796646215e-05, optimizer=Adam, weight_decay=1.2916350029688115e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 85, Fold 1: Test Accuracy = 0.4927\n",
      "Trial 83, Fold 4: Test Accuracy = 0.4405\n",
      "Trial 84, Fold 1: Test Accuracy = 0.3550\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 84, Fold 2: Test Accuracy = 0.3525\n",
      "Trial 85, Fold 2: Test Accuracy = 0.4997\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 83, Fold 5: Test Accuracy = 0.6229\n",
      "Trial 83: Mean Accuracy = 0.5354, Fold Accuracies = [np.float64(0.5083892345040009), np.float64(0.5368961410448634), np.float64(0.5682164256118787), np.float64(0.4404520917678812), np.float64(0.6229198399687402)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 13:45:43,699] Trial 83 finished with value: 0.5353747465794729 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.5648532646044315e-05, 'optimizer': 'Adam', 'weight_decay': 1.6214656573609924e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.782228330205127e-05, optimizer=Adam, weight_decay=1.89223276202094e-06, batch_size=10,factor=1\n",
      "Trial 84, Fold 3: Test Accuracy = 0.4411\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 86, Fold 1: Test Accuracy = 0.4936\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 85, Fold 3: Test Accuracy = 0.5162\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 86, Fold 2: Test Accuracy = 0.6563\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 85, Fold 4: Test Accuracy = 0.4557\n",
      "Trial 84, Fold 4: Test Accuracy = 0.3055\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 86, Fold 3: Test Accuracy = 0.5457\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 85, Fold 5: Test Accuracy = 0.6036\n",
      "Trial 85: Mean Accuracy = 0.5136, Fold Accuracies = [np.float64(0.4926908984333958), np.float64(0.49969959911102607), np.float64(0.5161797051892242), np.float64(0.455668016194332), np.float64(0.6035656887980086)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:03:54,873] Trial 85 finished with value: 0.5135607815451974 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.6350527796646215e-05, 'optimizer': 'Adam', 'weight_decay': 1.2916350029688115e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.018084103041024e-05, optimizer=Adam, weight_decay=1.991375009288026e-06, batch_size=10,factor=1\n",
      "Trial 84, Fold 5: Test Accuracy = 0.4729\n",
      "Trial 84: Mean Accuracy = 0.3854, Fold Accuracies = [np.float64(0.3550242041175202), np.float64(0.35253905211556663), np.float64(0.44113380466090596), np.float64(0.3054655870445344), np.float64(0.4729460053685006)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:05:37,625] Trial 84 finished with value: 0.38542173066140556 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.3607299710569063e-06, 'optimizer': 'Adam', 'weight_decay': 1.5919650870588936e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 69 with value: 0.5460321428889335.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.0678980640595713e-05, optimizer=Adam, weight_decay=1.7619392112147252e-06, batch_size=10,factor=1\n",
      "Trial 86, Fold 4: Test Accuracy = 0.4758\n",
      "Trial 87, Fold 1: Test Accuracy = 0.5057\n",
      "Trial 88, Fold 1: Test Accuracy = 0.4914\n",
      "Trial 86, Fold 5: Test Accuracy = 0.6053\n",
      "Trial 86: Mean Accuracy = 0.5554, Fold Accuracies = [np.float64(0.4936020394056526), np.float64(0.6563187864867451), np.float64(0.5457228563067017), np.float64(0.47584345479082324), np.float64(0.6052824949398757)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:16:44,263] Trial 86 finished with value: 0.5553539263859596 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.782228330205127e-05, 'optimizer': 'Adam', 'weight_decay': 1.89223276202094e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=9.564096200521738e-06, optimizer=Adam, weight_decay=1.8199740137210633e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 87, Fold 2: Test Accuracy = 0.5746\n",
      "Trial 88, Fold 2: Test Accuracy = 0.5656\n",
      "Trial 89, Fold 1: Test Accuracy = 0.4818\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 87, Fold 3: Test Accuracy = 0.4852\n",
      "Trial 88, Fold 3: Test Accuracy = 0.4789\n",
      "Trial 89, Fold 2: Test Accuracy = 0.6037\n",
      "Trial 87, Fold 4: Test Accuracy = 0.4280\n",
      "Trial 88, Fold 4: Test Accuracy = 0.4357\n",
      "Trial 89, Fold 3: Test Accuracy = 0.4755\n",
      "Trial 87, Fold 5: Test Accuracy = 0.6102\n",
      "Trial 87: Mean Accuracy = 0.5207, Fold Accuracies = [np.float64(0.5056596603571172), np.float64(0.5746270243829819), np.float64(0.48518144571261335), np.float64(0.4279689608636977), np.float64(0.6102251349026023)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:43:29,890] Trial 87 finished with value: 0.5207324452438025 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.018084103041024e-05, 'optimizer': 'Adam', 'weight_decay': 1.991375009288026e-06, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=5.326289317197433e-06, optimizer=Adam, weight_decay=2.76169716293357e-06, batch_size=10,factor=1\n",
      "Trial 88, Fold 5: Test Accuracy = 0.5773\n",
      "Trial 88: Mean Accuracy = 0.5098, Fold Accuracies = [np.float64(0.49137134546580624), np.float64(0.5655977180166097), np.float64(0.47890377832201225), np.float64(0.4356950067476384), np.float64(0.5773204887442968)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:46:51,472] Trial 88 finished with value: 0.5097776674592727 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.0678980640595713e-05, 'optimizer': 'Adam', 'weight_decay': 1.7619392112147252e-06, 'batch_size': 10, 'label_smoothing': 0.2}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=5.274461535147382e-06, optimizer=Adam, weight_decay=2.779376813650823e-06, batch_size=10,factor=1\n",
      "Trial 89, Fold 4: Test Accuracy = 0.4215\n",
      "Trial 90, Fold 1: Test Accuracy = 0.4564\n",
      "Trial 91, Fold 1: Test Accuracy = 0.4444\n",
      "Trial 89, Fold 5: Test Accuracy = 0.5864\n",
      "Trial 89: Mean Accuracy = 0.5138, Fold Accuracies = [np.float64(0.48182564408832257), np.float64(0.6037419848789357), np.float64(0.4755037505968609), np.float64(0.42152496626180835), np.float64(0.5864293572990236)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 14:56:26,650] Trial 89 finished with value: 0.5138051406249902 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 9.564096200521738e-06, 'optimizer': 'Adam', 'weight_decay': 1.8199740137210633e-06, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=3.63236763928152e-05, optimizer=Adam, weight_decay=2.695050719529811e-06, batch_size=10,factor=1\n",
      "Trial 90, Fold 2: Test Accuracy = 0.5766\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 92, Fold 1: Test Accuracy = 0.4974\n",
      "Trial 91, Fold 2: Test Accuracy = 0.5128\n",
      "Trial 90, Fold 3: Test Accuracy = 0.5052\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 92, Fold 2: Test Accuracy = 0.5123\n",
      "Trial 91, Fold 3: Test Accuracy = 0.4282\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 92, Fold 3: Test Accuracy = 0.5030\n",
      "Trial 90, Fold 4: Test Accuracy = 0.4362\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 90, Fold 5: Test Accuracy = 0.3836\n",
      "Trial 90: Mean Accuracy = 0.4716, Fold Accuracies = [np.float64(0.4563644829321433), np.float64(0.5765769717463659), np.float64(0.5051711257951728), np.float64(0.4362010796221323), np.float64(0.3836306808030116)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:19:02,402] Trial 90 finished with value: 0.4715888681797652 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 5.326289317197433e-06, 'optimizer': 'Adam', 'weight_decay': 2.76169716293357e-06, 'batch_size': 10, 'label_smoothing': 0.2}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=3.983753858138908e-05, optimizer=Adam, weight_decay=1.175387684364768e-06, batch_size=10,factor=1\n",
      "Trial 91, Fold 4: Test Accuracy = 0.4976\n",
      "Trial 92, Fold 4: Test Accuracy = 0.4844\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 91, Fold 5: Test Accuracy = 0.3855\n",
      "Trial 91: Mean Accuracy = 0.4537, Fold Accuracies = [np.float64(0.4443630451732776), np.float64(0.5128162184579067), np.float64(0.42818762226021595), np.float64(0.4976045883940621), np.float64(0.385519690188965)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:22:54,002] Trial 91 finished with value: 0.4536982328948855 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 5.274461535147382e-06, 'optimizer': 'Adam', 'weight_decay': 2.779376813650823e-06, 'batch_size': 10, 'label_smoothing': 0.21}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=4.301904846542686e-05, optimizer=Adam, weight_decay=4.387048694597285e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 92, Fold 5: Test Accuracy = 0.5938\n",
      "Trial 92: Mean Accuracy = 0.5182, Fold Accuracies = [np.float64(0.4973551209099907), np.float64(0.5123416913899257), np.float64(0.5029731374089306), np.float64(0.4843792172739541), np.float64(0.5937677261052824)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:26:11,073] Trial 92 finished with value: 0.5181633786176166 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 3.63236763928152e-05, 'optimizer': 'Adam', 'weight_decay': 2.695050719529811e-06, 'batch_size': 10, 'label_smoothing': 0.0}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=2.6407175053286313e-05, optimizer=Adam, weight_decay=1.244954029740668e-06, batch_size=10,factor=1\n",
      "Trial 93, Fold 1: Test Accuracy = 0.5257\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 94, Fold 1: Test Accuracy = 0.5125\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 95, Fold 1: Test Accuracy = 0.4640\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 94, Fold 2: Test Accuracy = 0.5394\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 93, Fold 2: Test Accuracy = 0.5737\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 95, Fold 2: Test Accuracy = 0.6187\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 93, Fold 3: Test Accuracy = 0.5380\n",
      "Trial 94, Fold 3: Test Accuracy = 0.5598\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 95, Fold 3: Test Accuracy = 0.5447\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 93, Fold 4: Test Accuracy = 0.4791\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 94, Fold 4: Test Accuracy = 0.3978\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 95, Fold 4: Test Accuracy = 0.4591\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 93, Fold 5: Test Accuracy = 0.6566\n",
      "Trial 93: Mean Accuracy = 0.5546, Fold Accuracies = [np.float64(0.5256649468858696), np.float64(0.5736979083591199), np.float64(0.53803967777213), np.float64(0.47908232118758437), np.float64(0.6566254247174205)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:51:49,741] Trial 93 finished with value: 0.5546220557844249 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 3.983753858138908e-05, 'optimizer': 'Adam', 'weight_decay': 1.175387684364768e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=6.499381144288622e-05, optimizer=Adam, weight_decay=1.0481088834526924e-06, batch_size=10,factor=1\n",
      "Trial 94, Fold 5: Test Accuracy = 0.5590\n",
      "Trial 94: Mean Accuracy = 0.5137, Fold Accuracies = [np.float64(0.5124826003528039), np.float64(0.5394482193936687), np.float64(0.5597527840672797), np.float64(0.3978407557354926), np.float64(0.5589843577698038)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:51:53,549] Trial 94 finished with value: 0.5137017434638096 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 4.301904846542686e-05, 'optimizer': 'Adam', 'weight_decay': 4.387048694597285e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=16, learning_rate=6.873096129692719e-05, optimizer=Adam, weight_decay=1.1456329395361291e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 96, Fold 1: Test Accuracy = 0.4952\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 95, Fold 5: Test Accuracy = 0.5915\n",
      "Trial 95: Mean Accuracy = 0.5356, Fold Accuracies = [np.float64(0.4640063261390092), np.float64(0.6186756308418668), np.float64(0.5446800055450303), np.float64(0.4591093117408907), np.float64(0.5914911601866101)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 15:57:51,450] Trial 95 finished with value: 0.5355924868906815 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 2.6407175053286313e-05, 'optimizer': 'Adam', 'weight_decay': 1.244954029740668e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=8.04347198423726e-05, optimizer=Adam, weight_decay=1.2068485444422388e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 97, Fold 1: Test Accuracy = 0.4734\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 98, Fold 1: Test Accuracy = 0.4883\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 96, Fold 2: Test Accuracy = 0.5771\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 96, Fold 3: Test Accuracy = 0.5320\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 98, Fold 2: Test Accuracy = 0.5854\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 97, Fold 2: Test Accuracy = 0.5323\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 96, Fold 4: Test Accuracy = 0.4458\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 98, Fold 3: Test Accuracy = 0.5532\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 97, Fold 3: Test Accuracy = 0.5537\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 96, Fold 5: Test Accuracy = 0.5854\n",
      "Trial 96: Mean Accuracy = 0.5271, Fold Accuracies = [np.float64(0.4951512798818826), np.float64(0.5771259344328538), np.float64(0.531989818708316), np.float64(0.4457827260458839), np.float64(0.5853523675804272)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:17:24,540] Trial 96 finished with value: 0.5270804253298726 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 6.499381144288622e-05, 'optimizer': 'Adam', 'weight_decay': 1.0481088834526924e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.2632748897720655e-05, optimizer=Adam, weight_decay=1.2845212624107576e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 98, Fold 4: Test Accuracy = 0.4597\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 98, Fold 5: Test Accuracy = 0.6130\n",
      "Trial 98: Mean Accuracy = 0.5399, Fold Accuracies = [np.float64(0.4882542842449277), np.float64(0.5853776544273243), np.float64(0.5532282088011953), np.float64(0.45968286099865047), np.float64(0.6130040291953631)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:23:29,726] Trial 98 finished with value: 0.5399094075334921 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 8.04347198423726e-05, 'optimizer': 'Adam', 'weight_decay': 1.2068485444422388e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=0.00010194141755995403, optimizer=Adam, weight_decay=1.2110904512515856e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 99, Fold 1: Test Accuracy = 0.4267\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 97, Fold 4: Test Accuracy = 0.4904\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 100, Fold 1: Test Accuracy = 0.5096\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 97, Fold 5: Test Accuracy = 0.5755\n",
      "Trial 97: Mean Accuracy = 0.5251, Fold Accuracies = [np.float64(0.4734419173067459), np.float64(0.5322758158675471), np.float64(0.5536832093402954), np.float64(0.4903846153846154), np.float64(0.5755043829215332)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:31:17,906] Trial 97 finished with value: 0.5250579881641474 and parameters: {'num_heads': 32, 'num_transformer_blocks': 16, 'learning_rate': 6.873096129692719e-05, 'optimizer': 'Adam', 'weight_decay': 1.1456329395361291e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.475616204328865e-05, optimizer=Adam, weight_decay=1.2484353415166117e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 100, Fold 2: Test Accuracy = 0.5157\n",
      "Trial 99, Fold 2: Test Accuracy = 0.5375\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 100, Fold 3: Test Accuracy = 0.5838\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 99, Fold 3: Test Accuracy = 0.5580\n",
      "Trial 101, Fold 1: Test Accuracy = 0.5013\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 100, Fold 4: Test Accuracy = 0.4407\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 100, Fold 5: Test Accuracy = 0.5490\n",
      "Trial 100: Mean Accuracy = 0.5198, Fold Accuracies = [np.float64(0.5096075692474438), np.float64(0.515748450144086), np.float64(0.583822143154198), np.float64(0.4406545209176788), np.float64(0.549035444346308)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:46:08,598] Trial 100 finished with value: 0.5197736255619428 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 0.00010194141755995403, 'optimizer': 'Adam', 'weight_decay': 1.2110904512515856e-06, 'batch_size': 10, 'label_smoothing': 0.11}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.4762568214777568e-05, optimizer=Adam, weight_decay=2.266623320246937e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 101, Fold 2: Test Accuracy = 0.5385\n",
      "Trial 99, Fold 4: Test Accuracy = 0.4466\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 102, Fold 1: Test Accuracy = 0.4564\n",
      "Trial 101, Fold 3: Test Accuracy = 0.5275\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 99, Fold 5: Test Accuracy = 0.6120\n",
      "Trial 99: Mean Accuracy = 0.5162, Fold Accuracies = [np.float64(0.4267274949263702), np.float64(0.537527514594698), np.float64(0.5580400628436764), np.float64(0.4465587044534413), np.float64(0.6120261737684545)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 16:54:39,435] Trial 99 finished with value: 0.5161759901173281 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.2632748897720655e-05, 'optimizer': 'Adam', 'weight_decay': 1.2845212624107576e-06, 'batch_size': 10, 'label_smoothing': 0.11}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.3724258889929559e-05, optimizer=Adam, weight_decay=2.371946395462843e-06, batch_size=10,factor=1\n",
      "Trial 101, Fold 4: Test Accuracy = 0.4564\n",
      "Trial 102, Fold 2: Test Accuracy = 0.5101\n",
      "Trial 103, Fold 1: Test Accuracy = 0.5079\n",
      "Trial 102, Fold 3: Test Accuracy = 0.5039\n",
      "Trial 101, Fold 5: Test Accuracy = 0.5937\n",
      "Trial 101: Mean Accuracy = 0.5235, Fold Accuracies = [np.float64(0.501314797303649), np.float64(0.53850049446518), np.float64(0.5274860989171789), np.float64(0.4564102564102564), np.float64(0.5936793503816199)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 17:09:41,195] Trial 101 finished with value: 0.523478199495577 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.475616204328865e-05, 'optimizer': 'Adam', 'weight_decay': 1.2484353415166117e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.4663688833197958e-05, optimizer=Adam, weight_decay=2.3558471449036248e-06, batch_size=10,factor=1\n",
      "Trial 103, Fold 2: Test Accuracy = 0.5647\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 104, Fold 1: Test Accuracy = 0.4674\n",
      "Trial 102, Fold 4: Test Accuracy = 0.4473\n",
      "Trial 103, Fold 3: Test Accuracy = 0.5301\n",
      "Trial 104, Fold 2: Test Accuracy = 0.5724\n",
      "Trial 102, Fold 5: Test Accuracy = 0.6298\n",
      "Trial 102: Mean Accuracy = 0.5095, Fold Accuracies = [np.float64(0.4563981486089683), np.float64(0.510064758988101), np.float64(0.5039238790567288), np.float64(0.44726720647773277), np.float64(0.6298173886476218)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 17:25:01,190] Trial 102 finished with value: 0.5094942763558306 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.4762568214777568e-05, 'optimizer': 'Adam', 'weight_decay': 2.266623320246937e-06, 'batch_size': 10, 'label_smoothing': 0.19}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.407401484162111e-05, optimizer=Adam, weight_decay=1.4526493520859708e-06, batch_size=10,factor=1\n",
      "Trial 103, Fold 4: Test Accuracy = 0.4876\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 105, Fold 1: Test Accuracy = 0.4881\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 104, Fold 3: Test Accuracy = 0.5451\n",
      "Trial 103, Fold 5: Test Accuracy = 0.6197\n",
      "Trial 103: Mean Accuracy = 0.5420, Fold Accuracies = [np.float64(0.5079260107168334), np.float64(0.5647390499888346), np.float64(0.5301318484974508), np.float64(0.4875843454790823), np.float64(0.619715349625262)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 17:34:46,427] Trial 103 finished with value: 0.5420193208614925 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.3724258889929559e-05, 'optimizer': 'Adam', 'weight_decay': 2.371946395462843e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=5.11788998804414e-05, optimizer=Adam, weight_decay=3.2583414702343135e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 106, Fold 1: Test Accuracy = 0.5134\n",
      "Trial 105, Fold 2: Test Accuracy = 0.6020\n",
      "Trial 104, Fold 4: Test Accuracy = 0.4533\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 105, Fold 3: Test Accuracy = 0.5499\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 106, Fold 2: Test Accuracy = 0.5754\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 104, Fold 5: Test Accuracy = 0.6102\n",
      "Trial 104: Mean Accuracy = 0.5297, Fold Accuracies = [np.float64(0.46742801528448275), np.float64(0.5723527503961038), np.float64(0.5451257643670194), np.float64(0.453306342780027), np.float64(0.6102405409073917)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 17:46:35,612] Trial 104 finished with value: 0.5296906827470049 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.4663688833197958e-05, 'optimizer': 'Adam', 'weight_decay': 2.3558471449036248e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=4.898155624993735e-05, optimizer=Adam, weight_decay=3.224957111801335e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 106, Fold 3: Test Accuracy = 0.5416\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 105, Fold 4: Test Accuracy = 0.4503\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 107, Fold 1: Test Accuracy = 0.4700\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 106, Fold 4: Test Accuracy = 0.4292\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 107, Fold 2: Test Accuracy = 0.5389\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 105, Fold 5: Test Accuracy = 0.6199\n",
      "Trial 105: Mean Accuracy = 0.5420, Fold Accuracies = [np.float64(0.48811555157406944), np.float64(0.6019714805244522), np.float64(0.5498602190286955), np.float64(0.45033738191632927), np.float64(0.6198855103126207)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 17:58:10,447] Trial 105 finished with value: 0.5420340286712334 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.407401484162111e-05, 'optimizer': 'Adam', 'weight_decay': 1.4526493520859708e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=4.758768753539347e-05, optimizer=Adam, weight_decay=1.4466554258521613e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 106, Fold 5: Test Accuracy = 0.6018\n",
      "Trial 106: Mean Accuracy = 0.5323, Fold Accuracies = [np.float64(0.5134052876347207), np.float64(0.5754351825267703), np.float64(0.5416046701477134), np.float64(0.42918353576248314), np.float64(0.601762450255681)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 18:01:33,434] Trial 106 finished with value: 0.5322782252654737 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 5.11788998804414e-05, 'optimizer': 'Adam', 'weight_decay': 3.2583414702343135e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=6.658457299134199e-06, optimizer=Adam, weight_decay=1.974360654636552e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 108, Fold 1: Test Accuracy = 0.4474\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 107, Fold 3: Test Accuracy = 0.5652\n",
      "Trial 109, Fold 1: Test Accuracy = 0.4958\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 107, Fold 4: Test Accuracy = 0.4566\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 108, Fold 2: Test Accuracy = 0.5650\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 107, Fold 5: Test Accuracy = 0.6076\n",
      "Trial 107: Mean Accuracy = 0.5277, Fold Accuracies = [np.float64(0.4700331458716966), np.float64(0.5388713433502409), np.float64(0.56519322890193), np.float64(0.45664642375168696), np.float64(0.6075525362542655)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 18:15:00,707] Trial 107 finished with value: 0.527659335625964 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 4.898155624993735e-05, 'optimizer': 'Adam', 'weight_decay': 3.224957111801335e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=6.763306336456458e-06, optimizer=Adam, weight_decay=1.4153735222468084e-06, batch_size=10,factor=1\n",
      "Trial 109, Fold 2: Test Accuracy = 0.6012\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 108, Fold 3: Test Accuracy = 0.5368\n",
      "Trial 110, Fold 1: Test Accuracy = 0.4980\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 108, Fold 4: Test Accuracy = 0.4585\n",
      "Trial 109, Fold 3: Test Accuracy = 0.4956\n",
      "Trial 110, Fold 2: Test Accuracy = 0.5762\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 108, Fold 5: Test Accuracy = 0.6315\n",
      "Trial 108: Mean Accuracy = 0.5278, Fold Accuracies = [np.float64(0.44742993966942607), np.float64(0.5650381216703352), np.float64(0.536758159666066), np.float64(0.458468286099865), np.float64(0.6315412899865418)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 18:33:07,717] Trial 108 finished with value: 0.5278471594184468 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 4.758768753539347e-05, 'optimizer': 'Adam', 'weight_decay': 1.4466554258521613e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=32, num_transformer_blocks=2, learning_rate=6.624382240491889e-06, optimizer=Adam, weight_decay=1.9484326721033226e-06, batch_size=16,factor=1\n",
      "Trial 109, Fold 4: Test Accuracy = 0.4195\n",
      "Trial 111, Fold 1: Test Accuracy = 0.3700\n",
      "Trial 110, Fold 3: Test Accuracy = 0.5213\n",
      "Trial 109, Fold 5: Test Accuracy = 0.5540\n",
      "Trial 109: Mean Accuracy = 0.5132, Fold Accuracies = [np.float64(0.4957769483015091), np.float64(0.6012125030571772), np.float64(0.49560787394297856), np.float64(0.4195344129554656), np.float64(0.5540388400348463)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 18:43:45,353] Trial 109 finished with value: 0.5132341156583953 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 6.658457299134199e-06, 'optimizer': 'Adam', 'weight_decay': 1.974360654636552e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=8, num_transformer_blocks=2, learning_rate=9.874005647126574e-05, optimizer=Adam, weight_decay=1.029743835545186e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 111, Fold 2: Test Accuracy = 0.6050\n",
      "Trial 112, Fold 1: Test Accuracy = 0.4897\n",
      "Trial 110, Fold 4: Test Accuracy = 0.4440\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 112, Fold 2: Test Accuracy = 0.5587\n",
      "Trial 111, Fold 3: Test Accuracy = 0.4806\n",
      "Trial 110, Fold 5: Test Accuracy = 0.5919\n",
      "Trial 110: Mean Accuracy = 0.5263, Fold Accuracies = [np.float64(0.497996980706382), np.float64(0.5762114396911985), np.float64(0.5213114766723658), np.float64(0.44396086369770577), np.float64(0.5919260428413952)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 18:58:45,243] Trial 110 finished with value: 0.5262813607218095 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 6.763306336456458e-06, 'optimizer': 'Adam', 'weight_decay': 1.4153735222468084e-06, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=3.154267490327759e-05, optimizer=Adam, weight_decay=1.7362628584725401e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 112, Fold 3: Test Accuracy = 0.5334\n",
      "Trial 111, Fold 4: Test Accuracy = 0.3365\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 112, Fold 4: Test Accuracy = 0.4455\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 27\n",
      "Trial 112, Fold 5: Test Accuracy = 0.5646\n",
      "Trial 112: Mean Accuracy = 0.5184, Fold Accuracies = [np.float64(0.4896694371174041), np.float64(0.558669888665582), np.float64(0.533394251651957), np.float64(0.4454790823211876), np.float64(0.5646335983182696)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 19:07:08,216] Trial 112 finished with value: 0.5183692516148801 and parameters: {'num_heads': 8, 'num_transformer_blocks': 2, 'learning_rate': 9.874005647126574e-05, 'optimizer': 'Adam', 'weight_decay': 1.029743835545186e-06, 'batch_size': 10, 'label_smoothing': 0.15}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.9049197456746035e-05, optimizer=Adam, weight_decay=1.724598771824461e-06, batch_size=10,factor=1\n",
      "Trial 113, Fold 1: Test Accuracy = 0.5128\n",
      "Trial 111, Fold 5: Test Accuracy = 0.5456\n",
      "Trial 111: Mean Accuracy = 0.4675, Fold Accuracies = [np.float64(0.3699603067956226), np.float64(0.6049954275262918), np.float64(0.48056805754509185), np.float64(0.3365384615384615), np.float64(0.5455974327096225)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 19:07:22,009] Trial 111 finished with value: 0.4675319372230181 and parameters: {'num_heads': 32, 'num_transformer_blocks': 2, 'learning_rate': 6.624382240491889e-06, 'optimizer': 'Adam', 'weight_decay': 1.9484326721033226e-06, 'batch_size': 16, 'label_smoothing': 0.15}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.9751966534263565e-05, optimizer=Adam, weight_decay=1.6537464520782325e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 113, Fold 2: Test Accuracy = 0.5580\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 115, Fold 1: Test Accuracy = 0.4711\n",
      "Trial 114, Fold 1: Test Accuracy = 0.5025\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 113, Fold 3: Test Accuracy = 0.5426\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 114, Fold 2: Test Accuracy = 0.5541\n",
      "Trial 115, Fold 2: Test Accuracy = 0.5949\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 113, Fold 4: Test Accuracy = 0.4401\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 114, Fold 3: Test Accuracy = 0.5147\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 115, Fold 3: Test Accuracy = 0.5452\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 113, Fold 5: Test Accuracy = 0.6268\n",
      "Trial 113: Mean Accuracy = 0.5360, Fold Accuracies = [np.float64(0.5127905904212081), np.float64(0.5580066141363874), np.float64(0.5425799023458558), np.float64(0.440080971659919), np.float64(0.626755447263112)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 19:31:25,154] Trial 113 finished with value: 0.5360427051652964 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 3.154267490327759e-05, 'optimizer': 'Adam', 'weight_decay': 1.7362628584725401e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.9730413786944568e-05, optimizer=Adam, weight_decay=4.155931680937987e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 114, Fold 4: Test Accuracy = 0.5205\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 115, Fold 4: Test Accuracy = 0.4378\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 116, Fold 1: Test Accuracy = 0.5002\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 114, Fold 5: Test Accuracy = 0.5979\n",
      "Trial 114: Mean Accuracy = 0.5379, Fold Accuracies = [np.float64(0.5024791165524755), np.float64(0.5541279867291926), np.float64(0.5146866287756265), np.float64(0.5205128205128206), np.float64(0.5978679644030159)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 19:41:19,392] Trial 114 finished with value: 0.5379349033946262 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.9049197456746035e-05, 'optimizer': 'Adam', 'weight_decay': 1.724598771824461e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=2.1235279233929493e-05, optimizer=Adam, weight_decay=4.347328030585618e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 115, Fold 5: Test Accuracy = 0.6373\n",
      "Trial 115: Mean Accuracy = 0.5372, Fold Accuracies = [np.float64(0.47107642794339655), np.float64(0.5948987675588308), np.float64(0.5451731281672135), np.float64(0.4377867746288799), np.float64(0.6372775252587775)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 19:44:05,570] Trial 115 finished with value: 0.5372425247114195 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.9751966534263565e-05, 'optimizer': 'Adam', 'weight_decay': 1.6537464520782325e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=1.329420166269361e-05, optimizer=SGD, weight_decay=2.4724192150529774e-06, batch_size=10,factor=1\n",
      "Trial 116, Fold 2: Test Accuracy = 0.6020\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 117, Fold 1: Test Accuracy = 0.4676\n",
      "Trial 118, Fold 1: Test Accuracy = 0.3587\n",
      "Trial 116, Fold 3: Test Accuracy = 0.5220\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 117, Fold 2: Test Accuracy = 0.6232\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 118, Fold 2: Test Accuracy = 0.3552\n",
      "Trial 116, Fold 4: Test Accuracy = 0.4489\n",
      "Trial 117, Fold 3: Test Accuracy = 0.5450\n",
      "Trial 118, Fold 3: Test Accuracy = 0.3573\n",
      "Trial 116, Fold 5: Test Accuracy = 0.5909\n",
      "Trial 116: Mean Accuracy = 0.5328, Fold Accuracies = [np.float64(0.5002271880201508), np.float64(0.6019581884497187), np.float64(0.521977650447453), np.float64(0.4488866396761133), np.float64(0.590889945604647)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 20:07:23,479] Trial 116 finished with value: 0.5327879224396165 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.9730413786944568e-05, 'optimizer': 'Adam', 'weight_decay': 4.155931680937987e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=1.483635606876198e-05, optimizer=SGD, weight_decay=1.4305956483535e-06, batch_size=10,factor=1\n",
      "Trial 117, Fold 4: Test Accuracy = 0.4669\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 119, Fold 1: Test Accuracy = 0.3210\n",
      "Trial 118, Fold 4: Test Accuracy = 0.3330\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 118, Fold 5: Test Accuracy = 0.3570\n",
      "Trial 118: Mean Accuracy = 0.3522, Fold Accuracies = [np.float64(0.3586832783114075), np.float64(0.3552081007220255), np.float64(0.35733099209833186), np.float64(0.3329622132253711), np.float64(0.35699589819877425)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 20:15:45,351] Trial 118 finished with value: 0.35223609651118204 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 1.329420166269361e-05, 'optimizer': 'SGD', 'weight_decay': 2.4724192150529774e-06, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=1.6472566216252616e-05, optimizer=Adam, weight_decay=1.4863638676079143e-06, batch_size=10,factor=1\n",
      "Trial 117, Fold 5: Test Accuracy = 0.5698\n",
      "Trial 117: Mean Accuracy = 0.5345, Fold Accuracies = [np.float64(0.46758422225540125), np.float64(0.6231630352718495), np.float64(0.544978898079263), np.float64(0.4669028340080972), np.float64(0.569780130097467)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 20:16:34,409] Trial 117 finished with value: 0.5344818239424155 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 2.1235279233929493e-05, 'optimizer': 'Adam', 'weight_decay': 4.347328030585618e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=1.3880530011552487e-05, optimizer=Adam, weight_decay=1.4259066581701147e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 120, Fold 1: Test Accuracy = 0.5419\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 119, Fold 2: Test Accuracy = 0.3202\n",
      "Trial 121, Fold 1: Test Accuracy = 0.4789\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 120, Fold 2: Test Accuracy = 0.5352\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 119, Fold 3: Test Accuracy = 0.3543\n",
      "Trial 121, Fold 2: Test Accuracy = 0.5958\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 120, Fold 3: Test Accuracy = 0.5177\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 119, Fold 4: Test Accuracy = 0.3329\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 10\n",
      "Trial 119, Fold 5: Test Accuracy = 0.3225\n",
      "Trial 119: Mean Accuracy = 0.3302, Fold Accuracies = [np.float64(0.32100178613889857), np.float64(0.32021804319392605), np.float64(0.3542672858617131), np.float64(0.3328947368421053), np.float64(0.32249218647626665)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 20:41:46,508] Trial 119 finished with value: 0.33017480770258195 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 1.483635606876198e-05, 'optimizer': 'SGD', 'weight_decay': 1.4305956483535e-06, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=4, learning_rate=1.6230210158896062e-06, optimizer=Adam, weight_decay=2.1085688960815073e-06, batch_size=16,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 120, Fold 4: Test Accuracy = 0.4220\n",
      "Trial 121, Fold 3: Test Accuracy = 0.5637\n",
      "Trial 122, Fold 1: Test Accuracy = 0.2765\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 120, Fold 5: Test Accuracy = 0.5881\n",
      "Trial 120: Mean Accuracy = 0.5210, Fold Accuracies = [np.float64(0.5419447126417712), np.float64(0.5351827926117331), np.float64(0.5177206845031807), np.float64(0.4219635627530365), np.float64(0.5881041463124236)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 20:51:08,515] Trial 120 finished with value: 0.520983179764429 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 1.6472566216252616e-05, 'optimizer': 'Adam', 'weight_decay': 1.4863638676079143e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=2, learning_rate=3.0018708146253204e-05, optimizer=Adam, weight_decay=1.900383992174991e-06, batch_size=10,factor=1\n",
      "Trial 122, Fold 2: Test Accuracy = 0.3995\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 123, Fold 1: Test Accuracy = 0.4980\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 121, Fold 4: Test Accuracy = 0.4271\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 123, Fold 2: Test Accuracy = 0.5680\n",
      "Trial 122, Fold 3: Test Accuracy = 0.4438\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 121, Fold 5: Test Accuracy = 0.6247\n",
      "Trial 121: Mean Accuracy = 0.5380, Fold Accuracies = [np.float64(0.47888328162931254), np.float64(0.5957800321136525), np.float64(0.5636798977249974), np.float64(0.42705802968960865), np.float64(0.624669972036158)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 21:07:46,591] Trial 121 finished with value: 0.5380142426387459 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 1.3880530011552487e-05, 'optimizer': 'Adam', 'weight_decay': 1.4259066581701147e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=1.20344433030941e-05, optimizer=Adam, weight_decay=2.1583216572078846e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 123, Fold 3: Test Accuracy = 0.5329\n",
      "Trial 122, Fold 4: Test Accuracy = 0.3374\n",
      "Trial 123, Fold 4: Test Accuracy = 0.4511\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 122, Fold 5: Test Accuracy = 0.3593\n",
      "Trial 122: Mean Accuracy = 0.3633, Fold Accuracies = [np.float64(0.2764924434711923), np.float64(0.3995185610531577), np.float64(0.44378509927144466), np.float64(0.33741565452091765), np.float64(0.3593499324227962)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 21:17:56,304] Trial 122 finished with value: 0.3633123381479017 and parameters: {'num_heads': 2, 'num_transformer_blocks': 4, 'learning_rate': 1.6230210158896062e-06, 'optimizer': 'Adam', 'weight_decay': 2.1085688960815073e-06, 'batch_size': 16, 'label_smoothing': 0.17}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=1.2024252454829069e-05, optimizer=Adam, weight_decay=6.333946794043769e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 124, Fold 1: Test Accuracy = 0.5023\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 123, Fold 5: Test Accuracy = 0.6188\n",
      "Trial 123: Mean Accuracy = 0.5338, Fold Accuracies = [np.float64(0.4980237451406515), np.float64(0.5680434597675482), np.float64(0.5329196894783051), np.float64(0.4510796221322537), np.float64(0.6188299550354704)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 21:19:52,951] Trial 123 finished with value: 0.5337792943108458 and parameters: {'num_heads': 2, 'num_transformer_blocks': 2, 'learning_rate': 3.0018708146253204e-05, 'optimizer': 'Adam', 'weight_decay': 1.900383992174991e-06, 'batch_size': 10, 'label_smoothing': 0.16}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=2, num_transformer_blocks=16, learning_rate=8.589288898959194e-06, optimizer=Adam, weight_decay=1.0020971359692609e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 124, Fold 2: Test Accuracy = 0.5664\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 126, Fold 1: Test Accuracy = 0.4986\n",
      "Trial 125, Fold 1: Test Accuracy = 0.4934\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 124, Fold 3: Test Accuracy = 0.5102\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 126, Fold 2: Test Accuracy = 0.5828\n",
      "Trial 124, Fold 4: Test Accuracy = 0.4866\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 25\n",
      "Trial 125, Fold 2: Test Accuracy = 0.5913\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 126, Fold 3: Test Accuracy = 0.5182\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 124, Fold 5: Test Accuracy = 0.5981\n",
      "Trial 124: Mean Accuracy = 0.5327, Fold Accuracies = [np.float64(0.5022855720897825), np.float64(0.5664165098201849), np.float64(0.5101781340973153), np.float64(0.48657219973009447), np.float64(0.5981198190900064)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 21:45:57,716] Trial 124 finished with value: 0.5327144469654768 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 1.20344433030941e-05, 'optimizer': 'Adam', 'weight_decay': 2.1583216572078846e-06, 'batch_size': 10, 'label_smoothing': 0.17}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=8.897150177110242e-06, optimizer=Adam, weight_decay=1.1676045364791909e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 125, Fold 3: Test Accuracy = 0.5277\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 29\n",
      "Trial 126, Fold 4: Test Accuracy = 0.4218\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 126, Fold 5: Test Accuracy = 0.5511\n",
      "Trial 126: Mean Accuracy = 0.5145, Fold Accuracies = [np.float64(0.49856310378960056), np.float64(0.5827763954020054), np.float64(0.5182262064291545), np.float64(0.4217948717948718), np.float64(0.5510585264942844)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 21:57:01,922] Trial 126 finished with value: 0.5144838207819833 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 8.589288898959194e-06, 'optimizer': 'Adam', 'weight_decay': 1.0020971359692609e-06, 'batch_size': 10, 'label_smoothing': 0.19}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=32, learning_rate=3.259055034760565e-06, optimizer=Adam, weight_decay=1.6292535335065366e-06, batch_size=10,factor=1\n",
      "Trial 127, Fold 1: Test Accuracy = 0.4857\n",
      "Trial 125, Fold 4: Test Accuracy = 0.4188\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 23\n",
      "Trial 125, Fold 5: Test Accuracy = 0.6248\n",
      "Trial 125: Mean Accuracy = 0.5312, Fold Accuracies = [np.float64(0.4934387099985069), np.float64(0.59132452866303), np.float64(0.5277018930117215), np.float64(0.41879217273954117), np.float64(0.6247911775177452)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 22:08:32,971] Trial 125 finished with value: 0.5312096963861089 and parameters: {'num_heads': 2, 'num_transformer_blocks': 16, 'learning_rate': 1.2024252454829069e-05, 'optimizer': 'Adam', 'weight_decay': 6.333946794043769e-06, 'batch_size': 10, 'label_smoothing': 0.18}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=4.029936860246839e-05, optimizer=Adam, weight_decay=1.146240811336436e-06, batch_size=10,factor=1\n",
      "Trial 127, Fold 2: Test Accuracy = 0.5917\n",
      "Trial 128, Fold 1: Test Accuracy = 0.4082\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 129, Fold 1: Test Accuracy = 0.5217\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 128, Fold 2: Test Accuracy = 0.3349\n",
      "Trial 129, Fold 2: Test Accuracy = 0.5907\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 129, Fold 3: Test Accuracy = 0.5420\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 28\n",
      "Trial 127, Fold 3: Test Accuracy = 0.5142\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 129, Fold 4: Test Accuracy = 0.4387\n",
      "Trial 128, Fold 3: Test Accuracy = 0.4725\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 129, Fold 5: Test Accuracy = 0.5938\n",
      "Trial 129: Mean Accuracy = 0.5374, Fold Accuracies = [np.float64(0.5216918661557093), np.float64(0.590691825905722), np.float64(0.5419593980561589), np.float64(0.438697705802969), np.float64(0.5937647160216843)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 22:32:59,743] Trial 129 finished with value: 0.5373611023884487 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 4.029936860246839e-05, 'optimizer': 'Adam', 'weight_decay': 1.146240811336436e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.00013614178016474758, optimizer=Adam, weight_decay=1.1629609811905326e-06, batch_size=10,factor=1\n",
      "Trial 127, Fold 4: Test Accuracy = 0.3466\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 130, Fold 1: Test Accuracy = 0.5324\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 130, Fold 2: Test Accuracy = 0.5554\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 130, Fold 3: Test Accuracy = 0.5374\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 24\n",
      "Trial 127, Fold 5: Test Accuracy = 0.5987\n",
      "Trial 127: Mean Accuracy = 0.5074, Fold Accuracies = [np.float64(0.4857454171436155), np.float64(0.5917139864527174), np.float64(0.5141691696317175), np.float64(0.34659244264507416), np.float64(0.5986614025927768)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 22:45:23,051] Trial 127 finished with value: 0.5073764836931802 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 8.897150177110242e-06, 'optimizer': 'Adam', 'weight_decay': 1.1676045364791909e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.00012529753340761099, optimizer=Adam, weight_decay=1.7576629947368491e-06, batch_size=10,factor=1\n",
      "Trial 128, Fold 4: Test Accuracy = 0.3481\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 130, Fold 4: Test Accuracy = 0.5144\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 131, Fold 1: Test Accuracy = 0.4938\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 130, Fold 5: Test Accuracy = 0.5828\n",
      "Trial 130: Mean Accuracy = 0.5445, Fold Accuracies = [np.float64(0.5323874649546277), np.float64(0.5553761125466552), np.float64(0.5374059270212405), np.float64(0.5144062078272605), np.float64(0.582758858051685)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 22:49:46,283] Trial 130 finished with value: 0.5444669140802938 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.00013614178016474758, 'optimizer': 'Adam', 'weight_decay': 1.1629609811905326e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.000142228634537685, optimizer=Adam, weight_decay=1.3210296419016843e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 131, Fold 2: Test Accuracy = 0.5752\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 132, Fold 1: Test Accuracy = 0.5036\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 131, Fold 3: Test Accuracy = 0.5439\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 132, Fold 2: Test Accuracy = 0.4911\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 18\n",
      "Trial 131, Fold 4: Test Accuracy = 0.4411\n",
      "Trial 128, Fold 5: Test Accuracy = 0.5440\n",
      "Trial 128: Mean Accuracy = 0.4216, Fold Accuracies = [np.float64(0.40821110724022186), np.float64(0.3349483204134367), np.float64(0.47250912619564717), np.float64(0.34814439946018894), np.float64(0.5439896759425039)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 22:59:21,193] Trial 128 finished with value: 0.4215605258503997 and parameters: {'num_heads': 16, 'num_transformer_blocks': 32, 'learning_rate': 3.259055034760565e-06, 'optimizer': 'Adam', 'weight_decay': 1.6292535335065366e-06, 'batch_size': 10, 'label_smoothing': 0.12}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.00012771315175564576, optimizer=Adam, weight_decay=1.3852738619108427e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 132, Fold 3: Test Accuracy = 0.5454\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 131, Fold 5: Test Accuracy = 0.5713\n",
      "Trial 131: Mean Accuracy = 0.5251, Fold Accuracies = [np.float64(0.4938393359765976), np.float64(0.5752464350655565), np.float64(0.5439277297721917), np.float64(0.44109311740890694), np.float64(0.5713442902691754)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:01:59,705] Trial 131 finished with value: 0.5250901816984858 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.00012529753340761099, 'optimizer': 'Adam', 'weight_decay': 1.7576629947368491e-06, 'batch_size': 10, 'label_smoothing': 0.1}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=7.686573697574408e-05, optimizer=Adam, weight_decay=1.3203576152291836e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 133, Fold 1: Test Accuracy = 0.5261\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 134, Fold 1: Test Accuracy = 0.4724\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 132, Fold 4: Test Accuracy = 0.4864\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 133, Fold 2: Test Accuracy = 0.5458\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 132, Fold 5: Test Accuracy = 0.5705\n",
      "Trial 132: Mean Accuracy = 0.5194, Fold Accuracies = [np.float64(0.5036072042778856), np.float64(0.491054433704448), np.float64(0.5454104862683486), np.float64(0.48640350877192984), np.float64(0.5705485614388742)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:10:53,407] Trial 132 finished with value: 0.5194048388922973 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.000142228634537685, 'optimizer': 'Adam', 'weight_decay': 1.3210296419016843e-06, 'batch_size': 10, 'label_smoothing': 0.1}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=7.747277657326559e-05, optimizer=Adam, weight_decay=1.1266076895341462e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 134, Fold 2: Test Accuracy = 0.5753\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 19\n",
      "Trial 133, Fold 3: Test Accuracy = 0.6095\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 135, Fold 1: Test Accuracy = 0.4876\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 134, Fold 3: Test Accuracy = 0.4870\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 133, Fold 4: Test Accuracy = 0.4287\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 13\n",
      "Trial 135, Fold 2: Test Accuracy = 0.5557\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 133, Fold 5: Test Accuracy = 0.5924\n",
      "Trial 133: Mean Accuracy = 0.5405, Fold Accuracies = [np.float64(0.5260673424133335), np.float64(0.5457872098340085), np.float64(0.6094632872787765), np.float64(0.42874493927125507), np.float64(0.5923559638199198)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:22:26,249] Trial 133 finished with value: 0.5404837485234587 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.00012771315175564576, 'optimizer': 'Adam', 'weight_decay': 1.3852738619108427e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.00021508398500403553, optimizer=Adam, weight_decay=1.1638099708851568e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 21\n",
      "Trial 134, Fold 4: Test Accuracy = 0.4405\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 22\n",
      "Trial 135, Fold 3: Test Accuracy = 0.5521\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 136, Fold 1: Test Accuracy = 0.5013\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 134, Fold 5: Test Accuracy = 0.5655\n",
      "Trial 134: Mean Accuracy = 0.5081, Fold Accuracies = [np.float64(0.4724129685849688), np.float64(0.5753381503812167), np.float64(0.4869632487716218), np.float64(0.4404520917678812), np.float64(0.5655334313859796)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:28:23,006] Trial 134 finished with value: 0.5081399781783336 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 7.686573697574408e-05, 'optimizer': 'Adam', 'weight_decay': 1.3203576152291836e-06, 'batch_size': 10, 'label_smoothing': 0.11}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=2, learning_rate=0.00023045015629036885, optimizer=Adam, weight_decay=1.1523205860116413e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 20\n",
      "Trial 135, Fold 4: Test Accuracy = 0.4495\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 136, Fold 2: Test Accuracy = 0.5138\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 137, Fold 1: Test Accuracy = 0.5277\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 135, Fold 5: Test Accuracy = 0.6113\n",
      "Trial 135: Mean Accuracy = 0.5312, Fold Accuracies = [np.float64(0.48755743570176463), np.float64(0.5556818302655225), np.float64(0.5521282134220538), np.float64(0.44949392712550607), np.float64(0.6112906383406553)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:34:14,761] Trial 135 finished with value: 0.5312304089711004 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 7.747277657326559e-05, 'optimizer': 'Adam', 'weight_decay': 1.1266076895341462e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=0.00020904212330170825, optimizer=Adam, weight_decay=1.0068861174231567e-06, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 136, Fold 3: Test Accuracy = 0.5289\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 137, Fold 2: Test Accuracy = 0.4536\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 138, Fold 1: Test Accuracy = 0.5063\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 136, Fold 4: Test Accuracy = 0.4761\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 137, Fold 3: Test Accuracy = 0.5690\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 138, Fold 2: Test Accuracy = 0.5159\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 11\n",
      "Trial 137, Fold 4: Test Accuracy = 0.5103\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 136, Fold 5: Test Accuracy = 0.5488\n",
      "Trial 136: Mean Accuracy = 0.5138, Fold Accuracies = [np.float64(0.5013132047092133), np.float64(0.5137785646686021), np.float64(0.5289122498960307), np.float64(0.4761133603238867), np.float64(0.5487894609762259)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:43:48,862] Trial 136 finished with value: 0.5137813681147916 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.00021508398500403553, 'optimizer': 'Adam', 'weight_decay': 1.1638099708851568e-06, 'batch_size': 10, 'label_smoothing': 0.14}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=5.813158155810114e-05, optimizer=Adam, weight_decay=0.0007318031906742477, batch_size=10,factor=1\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 15\n",
      "Trial 138, Fold 3: Test Accuracy = 0.5499\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 137, Fold 5: Test Accuracy = 0.5118\n",
      "Trial 137: Mean Accuracy = 0.5145, Fold Accuracies = [np.float64(0.5276947969718587), np.float64(0.4535893918609968), np.float64(0.5690110592548095), np.float64(0.5102564102564103), np.float64(0.5118170176134711)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:47:34,265] Trial 137 finished with value: 0.5144737351915094 and parameters: {'num_heads': 16, 'num_transformer_blocks': 2, 'learning_rate': 0.00023045015629036885, 'optimizer': 'Adam', 'weight_decay': 1.1523205860116413e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: num_heads=16, num_transformer_blocks=4, learning_rate=3.791748745220864e-05, optimizer=Adam, weight_decay=1.4234382831671557e-06, batch_size=10,factor=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-01 23:48:42,613] Trial 140 failed with parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 3.791748745220864e-05, 'optimizer': 'Adam', 'weight_decay': 1.4234382831671557e-06, 'batch_size': 10, 'label_smoothing': 0.15} because of the following error: RuntimeError('DataLoader worker (pid(s) 28076) exited unexpectedly').\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1251, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\queue.py\", line 212, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Gabriel\\AppData\\Local\\Temp\\ipykernel_29172\\348537963.py\", line 82, in objective\n",
      "    for inputs, labels in train_loader:\n",
      "                          ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1458, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1410, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1264, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "        f\"DataLoader worker (pid(s) {pids_str}) exited unexpectedly\"\n",
      "    ) from e\n",
      "RuntimeError: DataLoader worker (pid(s) 28076) exited unexpectedly\n",
      "[W 2025-05-01 23:48:42,738] Trial 140 failed with value None.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 139, Fold 1: Test Accuracy = 0.5030\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 16\n",
      "Trial 138, Fold 4: Test Accuracy = 0.4558\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 14\n",
      "Trial 139, Fold 2: Test Accuracy = 0.5882\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 138, Fold 5: Test Accuracy = 0.5218\n",
      "Trial 138: Mean Accuracy = 0.5100, Fold Accuracies = [np.float64(0.506323727998142), np.float64(0.5159451728501399), np.float64(0.5499333826224913), np.float64(0.45580296896086364), np.float64(0.5217721827490555)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 23:54:01,709] Trial 138 finished with value: 0.5099554870361385 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 0.00020904212330170825, 'optimizer': 'Adam', 'weight_decay': 1.0068861174231567e-06, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 12\n",
      "Trial 139, Fold 3: Test Accuracy = 0.5589\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 26\n",
      "Trial 139, Fold 4: Test Accuracy = 0.4476\n",
      "Divergence detected. Stopping training after 10 epochs.\n",
      "Early stopping at epoch 17\n",
      "Trial 139, Fold 5: Test Accuracy = 0.5494\n",
      "Trial 139: Mean Accuracy = 0.5294, Fold Accuracies = [np.float64(0.5029877735197996), np.float64(0.5881503812167034), np.float64(0.5588571846649107), np.float64(0.44757085020242915), np.float64(0.5493960672463922)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 00:00:26,797] Trial 139 finished with value: 0.529392451370047 and parameters: {'num_heads': 16, 'num_transformer_blocks': 4, 'learning_rate': 5.813158155810114e-05, 'optimizer': 'Adam', 'weight_decay': 0.0007318031906742477, 'batch_size': 10, 'label_smoothing': 0.13}. Best is trial 86 with value: 0.5553539263859596.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 28076) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1251\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\queue.py:212\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait(remaining)\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 143\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Start Optuna Study\u001b[39;00m\n\u001b[0;32m    137\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(\n\u001b[0;32m    138\u001b[0m     direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    139\u001b[0m     sampler\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(),\n\u001b[0;32m    140\u001b[0m     pruner\u001b[38;5;241m=\u001b[39moptuna\u001b[38;5;241m.\u001b[39mpruners\u001b[38;5;241m.\u001b[39mMedianPruner(n_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    141\u001b[0m )\n\u001b[1;32m--> 143\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;66;03m# Best result\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:100\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[38;5;66;03m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n\u001b[1;32m--> 100\u001b[0m                         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m                 futures\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    103\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[0;32m    104\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m                     )\n\u001b[0;32m    116\u001b[0m                 )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\concurrent\\futures\\thread.py:59\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    247\u001b[0m ):\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[8], line 82\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     81\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 82\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1410\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m   1409\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[1;32m-> 1410\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1412\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Gabriel\\anaconda3\\envs\\cudaenv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1264\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1263\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1265\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1266\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 28076) exited unexpectedly"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8, 16, 32])\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [2, 4, 8, 16, 32])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    label_smoothing = trial.suggest_float(\"label_smoothing\", 0.0, 0.3, step=0.01)\n",
    "    factor = 1\n",
    "\n",
    "    print(f\"Hyperparameters: num_heads={num_heads}, num_transformer_blocks={num_transformer_blocks}, \"\n",
    "          f\"learning_rate={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size},factor={factor}\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "\n",
    "    for test_fold_idx in range(5):\n",
    "        \n",
    "        #clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        test_fold = fold_indices[test_fold_idx]\n",
    "        remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "        val_fold_idx = test_fold_idx % 4\n",
    "        val_fold = remaining_folds[val_fold_idx]\n",
    "        train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "\n",
    "        test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "        test_labels = labels_folds[test_fold]\n",
    "\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                      torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                    torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), \n",
    "                                     torch.tensor(test_labels, dtype=torch.long))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TRANS_CNN(input_shape=input_shape, num_transformer_blocks=num_transformer_blocks, \n",
    "                          num_heads=num_heads, num_classes=num_classes, embed_dim=128).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "        optimizer = optimizer_cls[optimizer_name](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Trial {trial.number}: Mean Accuracy = {mean_accuracy:.4f}, Fold Accuracies = {fold_accuracies}\")\n",
    "    \n",
    "    trial.set_user_attr(\"fold_accuracies\", fold_accuracies)\n",
    "    trial.report(mean_accuracy, step=0)  # Single report after all folds\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=300, n_jobs=3)       \n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
