{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d84bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = r\"G:\\CODING\\py\\data\\numpy\\melunfiltered\\concatenatedspectrograms\"\n",
    "\n",
    "# Load the numpy files into the respective arrays with the correct capitalized naming\n",
    "eeg_fold_1 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_1.npy'))\n",
    "labels_fold_1 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_1.npy'))\n",
    "patients_fold_1 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_1.npy'))\n",
    "\n",
    "eeg_fold_2 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_2.npy'))\n",
    "labels_fold_2 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_2.npy'))\n",
    "patients_fold_2 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_2.npy'))\n",
    "\n",
    "eeg_fold_3 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_3.npy'))\n",
    "labels_fold_3 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_3.npy'))\n",
    "patients_fold_3 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_3.npy'))\n",
    "\n",
    "eeg_fold_4 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_4.npy'))\n",
    "labels_fold_4 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_4.npy'))\n",
    "patients_fold_4 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_4.npy'))\n",
    "\n",
    "eeg_fold_5 = np.load(os.path.join(folder_path, 'MEL_DATA_FOLD_fold_5.npy'))\n",
    "labels_fold_5 = np.load(os.path.join(folder_path, 'MEL_LABELS_FOLD_fold_5.npy'))\n",
    "patients_fold_5 = np.load(os.path.join(folder_path, 'MEL_PATIENTS_FOLD_fold_5.npy'))\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9831e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497bd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "debug_mode_flag = False\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "debug_mode_flag = False\n",
    "class CustomCnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=20,\n",
    "                      out_channels=40,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=40,\n",
    "                      out_channels=80,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(80),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "        self.block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=80,\n",
    "                      out_channels=160,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(160),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=160,\n",
    "                      out_channels=320,\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=1),\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2),\n",
    "                         stride=(2, 2))\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if debug_mode_flag: print(f\"{x.shape}\")\n",
    "        \n",
    "        x = self.block_1(x)\n",
    "        if debug_mode_flag: print(f\"block 1-{x.shape}\")\n",
    "        \n",
    "        x = self.block_2(x)\n",
    "        if debug_mode_flag: print(f\"block 2-{x.shape}\")\n",
    "\n",
    "        \n",
    "        return x \n",
    "    \n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.att(x, x, x)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "class TRANS_CNN(nn.Module):\n",
    "    def __init__(self, input_shape, num_classes, embed_dim=512, num_heads=2, ff_dim=64, num_transformer_blocks=4):\n",
    "        \n",
    "        super(TRANS_CNN,self).__init__()\n",
    "        \n",
    "        self.num_transformer_blocks = num_transformer_blocks\n",
    "        self.cnn_extractor = CustomCnn()\n",
    "        \n",
    "        self.projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoder(embed_dim,num_heads,ff_dim) for _ in range(num_transformer_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.precls = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls2 = nn.Linear(embed_dim,embed_dim)\n",
    "        self.precls3 = nn.Linear(embed_dim,embed_dim//4)\n",
    "        \n",
    "        self.clf = nn.Linear(embed_dim//4,num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x = self.cnn_extractor(x)\n",
    "        if debug_mode_flag: print(f\"x shape after cnn extraction = {x.shape}\")\n",
    "        \n",
    "        B,C,H,W = x.shape\n",
    "        \n",
    "        x = x.view(B,H*W,C)\n",
    "        if debug_mode_flag: print(f\"x shape after changing view= {x.shape}\")\n",
    "        \n",
    "        # x = self.projection(x)\n",
    "        # if debug_mode_flag: print(f\"x shape after projection= {x.shape}\")\n",
    "        \n",
    "        for encoderblock in self.encoder:\n",
    "            x = encoderblock(x)\n",
    "            \n",
    "        if debug_mode_flag: print(f\"x shape after passing thru encoder= {x.shape}\")\n",
    "        \n",
    "        x = x.permute(1,0,2)\n",
    "        if debug_mode_flag: print(f\"x shape after permuting{x.shape}\")\n",
    "        \n",
    "        x = self.precls3(x)\n",
    "        if debug_mode_flag: print(f\"precls3 {x.shape}\")\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.mean(dim=0)  # Global average pooling over sequence (9 tokens → 1 token)\n",
    "        if debug_mode_flag: print(f\"x shape after average pooling {x.shape}\")\n",
    "\n",
    "        x = self.clf(x)  #they see me rolling\n",
    "        if debug_mode_flag: print(f\"cls {x.shape}\")\n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "model = TRANS_CNN(input_shape=(224,224,20),num_classes=3,num_transformer_blocks=4,embed_dim=320)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "input_shape = (20,224,224)\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f665b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [2, 4, 8, 16, 32])\n",
    "    num_transformer_blocks = trial.suggest_categorical(\"num_transformer_blocks\", [2, 4, 8, 16, 32])\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"AdamW\", \"SGD\"])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    label_smoothing = 0.3\n",
    "    factor = trial.suggest_int(\"factor\", 1, 5)\n",
    "\n",
    "    print(f\"Hyperparameters: num_heads={num_heads}, num_transformer_blocks={num_transformer_blocks}, \"\n",
    "          f\"learning_rate={learning_rate}, optimizer={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size}, factor={factor}\")\n",
    "    \n",
    "    fold_accuracies = []\n",
    "\n",
    "    for test_fold_idx in range(5):\n",
    "        test_fold = fold_indices[test_fold_idx]\n",
    "        remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "        val_fold_idx = test_fold_idx % 4\n",
    "        val_fold = remaining_folds[val_fold_idx]\n",
    "        train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds]).transpose(0, 3, 1, 2)\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "        val_data = eeg_folds[val_fold].transpose(0, 3, 1, 2)\n",
    "        val_labels = labels_folds[val_fold]\n",
    "\n",
    "        test_data = eeg_folds[test_fold].transpose(0, 3, 1, 2)\n",
    "        test_labels = labels_folds[test_fold]\n",
    "\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32), \n",
    "                                      torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32), \n",
    "                                    torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32), \n",
    "                                     torch.tensor(test_labels, dtype=torch.long))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = TRANS_CNN(input_shape=input_shape, num_transformer_blocks=num_transformer_blocks, \n",
    "                          num_heads=num_heads, num_classes=num_classes, embed_dim=320).to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "        optimizer = optimizer_cls[optimizer_name](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Trial {trial.number}: Mean Accuracy = {mean_accuracy:.4f}, Fold Accuracies = {fold_accuracies}\")\n",
    "    \n",
    "    trial.set_user_attr(\"fold_accuracies\", fold_accuracies)\n",
    "    trial.report(mean_accuracy, step=0)  # Single report after all folds\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    \n",
    "    return mean_accuracy\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=300, n_jobs=3)\n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba9f72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
