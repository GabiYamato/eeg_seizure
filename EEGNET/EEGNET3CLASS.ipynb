{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bac77148",
   "metadata": {
    "id": "bac77148"
   },
   "source": [
    "<div style=\"background: linear-gradient(to right,rgb(225, 231, 134),rgb(176, 238, 148),rgb(150, 232, 238)); padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 10px 20px rgba(0,0,0,0.19), 0 6px 6px rgba(0,0,0,0.23);\">\n",
    "    <span style=\"font-family: 'Montserrat', sans-serif; font-weight: 800; font-size: 2.5em; color: white; text-shadow: 2px 2px 4px #000;\">✨ IMPORTS ✨</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e145729",
   "metadata": {
    "id": "1e145729"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "from torchinfo import summary\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47be2b6",
   "metadata": {},
   "source": [
    "SPLITTING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be42d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, labels, patients from pkl files\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_data(data_path, labels_path, patients_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    with open(labels_path, 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "    with open(patients_path, 'rb') as f:\n",
    "        patients = pickle.load(f)\n",
    "    return data, labels, patients\n",
    "\n",
    "data,labels, patients = load_data('data.pkl', 'labels.pkl', 'patients.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e7a38a",
   "metadata": {
    "id": "d7e7a38a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def separate_data_by_class(labels,data, patients, class_no):\n",
    "    class_indices = np.where(labels == int(class_no))[0]\n",
    "    data_class = data[class_indices]\n",
    "    labels_class = labels[class_indices]\n",
    "    patients_class = patients[class_indices]\n",
    "    return class_indices, data_class, labels_class, patients_class\n",
    "\n",
    "\n",
    "def create_patient_folds(patients, n_splits=5):\n",
    "    all_unique_patients = np.unique(patients)\n",
    "    print(\"No. of unique patients is:\", len(all_unique_patients))\n",
    "    np.random.shuffle(all_unique_patients)\n",
    "    patient_splits = np.array_split(all_unique_patients, n_splits)\n",
    "\n",
    "    patient_fold_mapping = {}\n",
    "    for fold, patient_group in enumerate(patient_splits):\n",
    "        for patient in patient_group:\n",
    "            patient_fold_mapping[patient] = fold\n",
    "\n",
    "    return patient_fold_mapping\n",
    "\n",
    "def split_data_by_patients_across_classes(eeg_data, labels, patients, patient_fold_mapping, n_splits=5):\n",
    "    eeg_data_splits = [[] for _ in range(n_splits)]\n",
    "    labels_splits = [[] for _ in range(n_splits)]\n",
    "    patients_splits = [[] for _ in range(n_splits)]\n",
    "    for i in range(len(patients)):\n",
    "        patient_id = patients[i]\n",
    "        fold = patient_fold_mapping[patient_id]  # Get the fold number from the mapping\n",
    "        eeg_data_splits[fold].append(eeg_data[i])\n",
    "        labels_splits[fold].append(labels[i])\n",
    "        patients_splits[fold].append(patients[i])\n",
    "        \n",
    "    eeg_data_splits = [np.array(split) for split in eeg_data_splits]\n",
    "    labels_splits = [np.array(split) for split in labels_splits]\n",
    "    patients_splits = [np.array(split) for split in patients_splits]\n",
    "\n",
    "    return eeg_data_splits, labels_splits, patients_splits\n",
    "\n",
    "\n",
    "def shuffle_fold(eeg_fold, labels_fold, patients_fold):\n",
    "    indices = np.arange(len(eeg_fold))  \n",
    "    np.random.shuffle(indices)          \n",
    "    eeg_fold_shuffled = eeg_fold[indices]\n",
    "    labels_fold_shuffled = labels_fold[indices]\n",
    "    patients_fold_shuffled = patients_fold[indices]\n",
    "\n",
    "    return eeg_fold_shuffled, labels_fold_shuffled, patients_fold_shuffled\n",
    "\n",
    "\n",
    "c0_indices, data_c0, labels_c0, patients_c0 = separate_data_by_class(labels, data, patients, 0)\n",
    "\n",
    "c1_indices, data_c1, labels_c1, patients_c1 = separate_data_by_class(labels, data, patients, 1)\n",
    "\n",
    "c2_indices, data_c2, labels_c2, patients_c2 = separate_data_by_class(labels, data, patients, 2)\n",
    "\n",
    "\n",
    "print(f\"Class 0: {data_c0.shape}, {labels_c0.shape}, {len(np.unique(patients_c0))}\")\n",
    "print(f\"Class 1: {data_c1.shape}, {labels_c1.shape}, {len(np.unique(patients_c1))}\")\n",
    "print(f\"Class 2: {data_c2.shape}, {labels_c2.shape}, {len(np.unique(patients_c2))}\")\n",
    "\n",
    "\n",
    "patients_set_c0 = set(patients_c0)\n",
    "patients_set_c1 = set(patients_c1)\n",
    "patients_set_c2 = set(patients_c2)\n",
    "\n",
    "common_patients_all = patients_set_c0.intersection(patients_set_c1, patients_set_c2)\n",
    "common_patients_all = list(common_patients_all)\n",
    "print('No. of common patients between all classes: ', len(common_patients_all))\n",
    "\n",
    "def extract_common_data(patients, data, labels, common_patient):\n",
    "    common_indices = [i for i, patient in enumerate(patients) if patient in common_patient]\n",
    "    common_data = data[common_indices]\n",
    "    common_labels = labels[common_indices]\n",
    "    common_patients = patients[common_indices]\n",
    "    return common_data, common_labels, common_patients\n",
    "\n",
    "common_data_c0, common_labels_c0, common_patients_c0 = extract_common_data(patients_c0, data_c0, labels_c0, common_patients_all)\n",
    "common_data_c1, common_labels_c1, common_patients_c1 = extract_common_data(patients_c1, data_c1, labels_c1, common_patients_all)\n",
    "common_data_c2, common_labels_c2, common_patients_c2 = extract_common_data(patients_c2, data_c2, labels_c2, common_patients_all)\n",
    "\n",
    "print(np.unique(common_patients_c0).shape)\n",
    "\n",
    "common_allpatient_fold_mapping = create_patient_folds(common_patients_all, n_splits=5)\n",
    "eeg_012_c0, labels_012_c0, patients_012_c0 = split_data_by_patients_across_classes(\n",
    "    common_data_c0, common_labels_c0, common_patients_c0, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "eeg_012_c1, labels_012_c1, patients_012_c1 = split_data_by_patients_across_classes(\n",
    "    common_data_c1, common_labels_c1, common_patients_c1, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "eeg_012_c2, labels_012_c2, patients_012_c2 = split_data_by_patients_across_classes(\n",
    "    common_data_c2, common_labels_c2, common_patients_c2, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([eeg_012_c0[i], eeg_012_c1[i], eeg_012_c2[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([labels_012_c0[i], labels_012_c1[i], labels_012_c2[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([patients_012_c0[i], patients_012_c1[i], patients_012_c2[i]]).shape}\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {eeg_012_c0[i].shape}, \"\n",
    "          f\"Labels shape 0: {labels_012_c0[i].shape}, \"\n",
    "          f\"Patients shape 0: {patients_012_c0[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {eeg_012_c1[i].shape}, \"\n",
    "      f\"Labels shape 1: {labels_012_c1[i].shape}, \"\n",
    "      f\"Patients shape 1: {patients_012_c1[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 2: {eeg_012_c2[i].shape}, \"\n",
    "      f\"Labels shape 2: {labels_012_c2[i].shape}, \"\n",
    "      f\"Patients shape 2: {patients_012_c2[i].shape}\")\n",
    "    \n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(patients_012_c2[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(patients_012_c1[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(patients_012_c0[i]))}\")\n",
    "    \n",
    "    # Find exclusive patients for each class\n",
    "exclusive_patients_c0 = list(patients_set_c0 - set(common_patients_all))\n",
    "exclusive_patients_c1 = list(patients_set_c1 - set(common_patients_all))\n",
    "exclusive_patients_c2 = list(patients_set_c2 - set(common_patients_all))\n",
    "\n",
    "print(len(exclusive_patients_c0))\n",
    "print(len(exclusive_patients_c1))\n",
    "print(len(exclusive_patients_c2))\n",
    "\n",
    "exclusive_c2 = set(exclusive_patients_c2)\n",
    "exclusive_c0 = set(exclusive_patients_c0)\n",
    "common_c2_c0 = exclusive_c0.intersection(exclusive_c2)\n",
    "common_patient_c2_c0 = list(common_c2_c0)\n",
    "\n",
    "print(len(common_patient_c2_c0))\n",
    "\n",
    "\n",
    "\n",
    "common_data_c02, common_labels_c02, common_patients_c02 = extract_common_data(patients_c0, data_c0, labels_c0, common_patient_c2_c0)\n",
    "common_data_c22, common_labels_c22, common_patients_c22 = extract_common_data(patients_c2, data_c2, labels_c2, common_patient_c2_c0)\n",
    "\n",
    "print(np.unique(common_patients_c02).shape)\n",
    "print(np.unique(common_patients_c22).shape)\n",
    "\n",
    "c20_mapping = create_patient_folds(common_patient_c2_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c02, common_labels_splits_c02, common_patients_splits_c02 = split_data_by_patients_across_classes(\n",
    "    common_data_c02, common_labels_c02, common_patients_c02, c20_mapping, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c22, common_labels_splits_c22, common_patients_splits_c22 = split_data_by_patients_across_classes(\n",
    "    common_data_c22, common_labels_c22, common_patients_c22, c20_mapping, n_splits=5)\n",
    "\n",
    "\n",
    "# Check the shape of each fold for verification\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c02[i], common_eeg_splits_c22[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c02[i], common_labels_splits_c22[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c02[i], common_patients_splits_c22[i]]).shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {common_eeg_splits_c02[i].shape}, \"\n",
    "          f\"Labels shape 0: {common_labels_splits_c02[i].shape}, \"\n",
    "          f\"Patients shape 0: {common_patients_splits_c02[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {common_eeg_splits_c22[i].shape}, \"\n",
    "      f\"Labels shape 1: {common_labels_splits_c22[i].shape}, \"\n",
    "      f\"Patients shape 1: {common_patients_splits_c22[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c02[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(common_patients_splits_c22[i]))}\")\n",
    "\n",
    "\n",
    "# Find exclusive patients for each class\n",
    "exclusive_patients_c0_c2 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0))\n",
    "exclusive_patients_c1_c2 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0))\n",
    "exclusive_patients_c2_c2 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0))\n",
    "\n",
    "print(len(exclusive_patients_c0_c2))\n",
    "print(len(exclusive_patients_c1_c2))\n",
    "print(len(exclusive_patients_c2_c2))\n",
    "\n",
    "exclusive_c2 = set(exclusive_patients_c2_c2)\n",
    "exclusive_c1 = set(exclusive_patients_c1_c2)\n",
    "common_c2_c1 = exclusive_c1.intersection(exclusive_c2)\n",
    "common_patient_c2_c1 = list(common_c2_c1)\n",
    "\n",
    "print(len(common_patient_c2_c1))\n",
    "\n",
    "\n",
    "\n",
    "common_data_c12, common_labels_c12, common_patients_c12 = extract_common_data(patients_c1, data_c1, labels_c1, common_patient_c2_c1)\n",
    "common_data_1_c22, common_labels_1_c22, common_patients_1_c22 = extract_common_data(patients_c2, data_c2, labels_c2, common_patient_c2_c1)\n",
    "\n",
    "print(np.unique(common_patients_c12).shape)\n",
    "print(np.unique(common_patients_1_c22).shape)\n",
    "\n",
    "c21_mapping = create_patient_folds(common_patient_c2_c1, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c12, common_labels_splits_c12, common_patients_splits_c12 = split_data_by_patients_across_classes(\n",
    "    common_data_c12, common_labels_c12, common_patients_c12, c21_mapping, n_splits=5)\n",
    "\n",
    "common_eeg_splits_1_c22, common_labels_splits_1_c22, common_patients_splits_1_c22 = split_data_by_patients_across_classes(\n",
    "    common_data_1_c22, common_labels_1_c22, common_patients_1_c22, c21_mapping, n_splits=5)\n",
    "\n",
    "\n",
    "# Check the shape of each fold for verification\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c12[i], common_eeg_splits_1_c22[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c12[i], common_labels_splits_1_c22[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c12[i], common_patients_splits_1_c22[i]]).shape}\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c12[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(common_patients_splits_1_c22[i]))}\")\n",
    "    \n",
    "\n",
    "exclusive_patients_c0_c2_c1 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1))\n",
    "exclusive_patients_c1_c2_c1 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1))\n",
    "exclusive_patients_c2_c2_c1 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0)- set(common_c2_c1))\n",
    "\n",
    "print(len(exclusive_patients_c0_c2_c1))\n",
    "print(len(exclusive_patients_c1_c2_c1))\n",
    "print(len(exclusive_patients_c2_c2_c1))\n",
    "\n",
    "only_c2 = set(exclusive_patients_c2_c2_c1)\n",
    "only_c2= list(only_c2)\n",
    "data_only_c2, labels_only_c2,patients_only_c2 = extract_common_data(patients_c2, data_c2, labels_c2, only_c2)\n",
    "print(np.unique(patients_only_c2).shape)\n",
    "\n",
    "c2_mapping = create_patient_folds(only_c2, n_splits=5)\n",
    "\n",
    "eeg_splits_c2, labels_splits_c2, patients_splits_c2 = split_data_by_patients_across_classes(\n",
    "    data_only_c2, labels_c2, patients_only_c2, c2_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(patients_splits_c2[i]))}\")\n",
    "    \n",
    "    # Find exclusive patients for each class\n",
    "exclusive_patients_all_c02 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "exclusive_patients_all_c12 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "exclusive_patients_all_c22 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "\n",
    "print(len(exclusive_patients_all_c02))\n",
    "print(len(exclusive_patients_all_c12))\n",
    "print(len(exclusive_patients_all_c22))\n",
    "\n",
    "\n",
    "exc_c1_c11 = set(exclusive_patients_all_c12)\n",
    "exc_c1_c01 = set(exclusive_patients_all_c02)\n",
    "common_patient_c1_c0 = exc_c1_c01.intersection(exc_c1_c11)\n",
    "common_patient_c1_c0 = list(common_patient_c1_c0)\n",
    "\n",
    "print(len(common_patient_c1_c0))\n",
    "\n",
    "common_data_c1_c01, common_labels_c1_c01, common_patients_c1_c01 = extract_common_data(patients_c0, data_c0, labels_c0, common_patient_c1_c0)\n",
    "common_data_c1_c11, common_labels_c1_c11, common_patients_c1_c11 = extract_common_data(patients_c1, data_c1, labels_c1, common_patient_c1_c0)\n",
    "\n",
    "print(np.unique(common_patients_c1_c01).shape)\n",
    "\n",
    "mapping_c1_c0 = create_patient_folds(common_patient_c1_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c1_c01, common_labels_splits_c1_c01, common_patients_splits_c1_c01= split_data_by_patients_across_classes(\n",
    "    common_data_c1_c01, common_labels_c1_c01, common_patients_c1_c01, mapping_c1_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c1_c11, common_labels_splits_c1_c11, common_patients_splits_c1_c11 = split_data_by_patients_across_classes(\n",
    "    common_data_c1_c11, common_labels_c1_c11, common_patients_c1_c11, mapping_c1_c0, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c1_c01[i], common_eeg_splits_c1_c11[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c1_c01[i], common_labels_splits_c1_c11[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c1_c01[i], common_patients_splits_c1_c11[i]]).shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {common_eeg_splits_c1_c01[i].shape}, \"\n",
    "          f\"Labels shape 0: {common_labels_splits_c1_c01[i].shape}, \"\n",
    "          f\"Patients shape 0: {common_patients_splits_c1_c01[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {common_eeg_splits_c1_c11[i].shape}, \"\n",
    "      f\"Labels shape 1: {common_labels_splits_c1_c11[i].shape}, \"\n",
    "      f\"Patients shape 1: {common_patients_splits_c1_c11[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c1_c01[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(common_patients_splits_c1_c11[i]))}\")\n",
    "    \n",
    "exclusive_patients_c1_c01 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0))\n",
    "exclusive_patients_c1_c11 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0))\n",
    "exclusive_patients_c1_c21 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0))\n",
    "\n",
    "print(len(exclusive_patients_c1_c01))\n",
    "print(len(exclusive_patients_c1_c11))\n",
    "print(len(exclusive_patients_c1_c21))\n",
    "\n",
    "\n",
    "\n",
    "print(\"CLASS 1\")\n",
    "\n",
    "only_c1 = set(exclusive_patients_c1_c11)\n",
    "only_c1= list(only_c1)\n",
    "data_only_c1, labels_only_c1,patients_only_c1 = extract_common_data(patients_c1, data_c1, labels_c1, only_c1)\n",
    "print(np.unique(patients_only_c1).shape)\n",
    "\n",
    "c1_mapping = create_patient_folds(only_c1, n_splits=5)\n",
    "\n",
    "eeg_splits_c1, labels_splits_c1, patients_splits_c1 = split_data_by_patients_across_classes(\n",
    "    data_only_c1, labels_c1, patients_only_c1, c1_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(patients_splits_c1[i]))}\")\n",
    "    \n",
    "exclusive_patients_all1_c01 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0)- set(only_c1))\n",
    "exclusive_patients_all1_c11 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1))\n",
    "exclusive_patients_all1_c21 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1))\n",
    "\n",
    "print(len(exclusive_patients_all1_c01))\n",
    "print(len(exclusive_patients_all1_c11))\n",
    "print(len(exclusive_patients_all1_c21))\n",
    "\n",
    "print(\"CLASS 0\")\n",
    "only_c0 = set(exclusive_patients_all1_c01)\n",
    "only_c0= list(only_c0)\n",
    "data_only_c0, labels_only_c0,patients_only_c0 = extract_common_data(patients_c0, data_c0, labels_c0, only_c0)\n",
    "print(np.unique(patients_only_c0).shape)\n",
    "\n",
    "c0_mapping = create_patient_folds(only_c0, n_splits=5)\n",
    "\n",
    "eeg_splits_c0, labels_splits_c0, patients_splits_c0 = split_data_by_patients_across_classes(\n",
    "    data_only_c0, labels_c0, patients_only_c0, c0_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(patients_splits_c0[i]))}\")\n",
    "\n",
    "final_c0 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0)- set(only_c1) - set(only_c0))\n",
    "final_c1 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1)- set(only_c0))\n",
    "final_c2 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1)- set(only_c0))\n",
    "\n",
    "print(len(final_c0))\n",
    "print(len(final_c1))\n",
    "print(len(final_c2))\n",
    "\n",
    "eeg_fold_1 = np.concatenate([eeg_012_c0[0], eeg_012_c1[0], eeg_012_c2[0], common_eeg_splits_c02[0], common_eeg_splits_c22[0], common_eeg_splits_c12[4], common_eeg_splits_1_c22[4],eeg_splits_c2[0],common_eeg_splits_c1_c01[0], common_eeg_splits_c1_c11[0],eeg_splits_c1[0],eeg_splits_c0[4]])\n",
    "labels_fold_1 = np.concatenate([labels_012_c0[0], labels_012_c1[0], labels_012_c2[0],common_labels_splits_c02[0], common_labels_splits_c22[0],common_labels_splits_c12[4], common_labels_splits_1_c22[4],labels_splits_c2[0],common_labels_splits_c1_c01[0], common_labels_splits_c1_c11[0],labels_splits_c1[0],labels_splits_c0[4] ])\n",
    "patients_fold_1 = np.concatenate([patients_012_c0[0], patients_012_c1[0], patients_012_c2[0],common_patients_splits_c02[0], common_patients_splits_c22[0],common_patients_splits_c12[4], common_patients_splits_1_c22[4],patients_splits_c2[0],common_patients_splits_c1_c01[0], common_patients_splits_c1_c11[0],patients_splits_c1[0],patients_splits_c0[4]])\n",
    "\n",
    "eeg_fold_2 = np.concatenate([eeg_012_c0[1], eeg_012_c1[1], eeg_012_c2[1], common_eeg_splits_c02[1], common_eeg_splits_c22[1], common_eeg_splits_c12[3], common_eeg_splits_1_c22[3],eeg_splits_c2[1],common_eeg_splits_c1_c01[1], common_eeg_splits_c1_c11[1],eeg_splits_c1[1],eeg_splits_c0[3]])\n",
    "labels_fold_2 = np.concatenate([labels_012_c0[1], labels_012_c1[1], labels_012_c2[1],common_labels_splits_c02[1], common_labels_splits_c22[1],common_labels_splits_c12[3], common_labels_splits_1_c22[3],labels_splits_c2[1],common_labels_splits_c1_c01[1], common_labels_splits_c1_c11[1],labels_splits_c1[1],labels_splits_c0[3] ])\n",
    "patients_fold_2 = np.concatenate([patients_012_c0[1], patients_012_c1[1], patients_012_c2[1],common_patients_splits_c02[1], common_patients_splits_c22[1],common_patients_splits_c12[3], common_patients_splits_1_c22[3],patients_splits_c2[1],common_patients_splits_c1_c01[1], common_patients_splits_c1_c11[1],patients_splits_c1[1],patients_splits_c0[3]])\n",
    "\n",
    "eeg_fold_3 = np.concatenate([eeg_012_c0[2], eeg_012_c1[2], eeg_012_c2[2], common_eeg_splits_c02[2], common_eeg_splits_c22[2], common_eeg_splits_c12[2], common_eeg_splits_1_c22[2],eeg_splits_c2[2],common_eeg_splits_c1_c01[2], common_eeg_splits_c1_c11[2],eeg_splits_c1[2],eeg_splits_c0[2]])\n",
    "labels_fold_3 = np.concatenate([labels_012_c0[2], labels_012_c1[2], labels_012_c2[2],common_labels_splits_c02[2], common_labels_splits_c22[2],common_labels_splits_c12[2], common_labels_splits_1_c22[2],labels_splits_c2[2],common_labels_splits_c1_c01[2], common_labels_splits_c1_c11[2],labels_splits_c1[2],labels_splits_c0[2] ])\n",
    "patients_fold_3 = np.concatenate([patients_012_c0[2], patients_012_c1[2], patients_012_c2[2],common_patients_splits_c02[2], common_patients_splits_c22[2],common_patients_splits_c12[2], common_patients_splits_1_c22[2],patients_splits_c2[2],common_patients_splits_c1_c01[2], common_patients_splits_c1_c11[2],patients_splits_c1[2],patients_splits_c0[2]])\n",
    "\n",
    "eeg_fold_4 = np.concatenate([eeg_012_c0[3], eeg_012_c1[3], eeg_012_c2[3], common_eeg_splits_c02[3], common_eeg_splits_c22[3], common_eeg_splits_c12[1], common_eeg_splits_1_c22[1],eeg_splits_c2[3],common_eeg_splits_c1_c01[3], common_eeg_splits_c1_c11[3],eeg_splits_c1[3],eeg_splits_c0[1]])\n",
    "labels_fold_4 = np.concatenate([labels_012_c0[3], labels_012_c1[3], labels_012_c2[3],common_labels_splits_c02[3], common_labels_splits_c22[3],common_labels_splits_c12[1], common_labels_splits_1_c22[1],labels_splits_c2[3],common_labels_splits_c1_c01[3], common_labels_splits_c1_c11[3],labels_splits_c1[3],labels_splits_c0[1] ])\n",
    "patients_fold_4 = np.concatenate([patients_012_c0[3], patients_012_c1[3], patients_012_c2[3],common_patients_splits_c02[3], common_patients_splits_c22[3],common_patients_splits_c12[1], common_patients_splits_1_c22[1],patients_splits_c2[3],common_patients_splits_c1_c01[3], common_patients_splits_c1_c11[3],patients_splits_c1[3],patients_splits_c0[1]])\n",
    "\n",
    "eeg_fold_5 = np.concatenate([eeg_012_c0[4], eeg_012_c1[4], eeg_012_c2[4], common_eeg_splits_c02[4], common_eeg_splits_c22[4], common_eeg_splits_c12[0], common_eeg_splits_1_c22[0],eeg_splits_c2[4],common_eeg_splits_c1_c01[4], common_eeg_splits_c1_c11[4],eeg_splits_c1[4],eeg_splits_c0[0]])\n",
    "labels_fold_5 = np.concatenate([labels_012_c0[4], labels_012_c1[4], labels_012_c2[4],common_labels_splits_c02[4], common_labels_splits_c22[4],common_labels_splits_c12[0], common_labels_splits_1_c22[0],labels_splits_c2[4],common_labels_splits_c1_c01[4], common_labels_splits_c1_c11[4],labels_splits_c1[4],labels_splits_c0[0] ])\n",
    "patients_fold_5 = np.concatenate([patients_012_c0[4], patients_012_c1[4], patients_012_c2[4],common_patients_splits_c02[4], common_patients_splits_c22[4],common_patients_splits_c12[0], common_patients_splits_1_c22[0],patients_splits_c2[4],common_patients_splits_c1_c01[4], common_patients_splits_c1_c11[4],patients_splits_c1[4],patients_splits_c0[0]])\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1GXnZcR9-t8",
   "metadata": {
    "id": "d1GXnZcR9-t8"
   },
   "source": [
    "DATA BALANCER AND EARLY STOPPING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b9affa",
   "metadata": {
    "id": "d4b9affa"
   },
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb644b0",
   "metadata": {
    "id": "deb644b0"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8B_39jPU-L3Y",
   "metadata": {
    "id": "8B_39jPU-L3Y"
   },
   "source": [
    "results plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bzX-UOf8f-WM",
   "metadata": {
    "id": "bzX-UOf8f-WM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def plot_metrics(labels, predictions, n_classes=3):\n",
    "    \"\"\"\n",
    "    Plots and computes metrics for classification tasks.\n",
    "\n",
    "    Args:\n",
    "        labels (array-like): True labels.\n",
    "        predictions (array-like): Predicted probabilities or class predictions.\n",
    "        n_classes (int): Number of classes (default is 3 for multi-class classification).\n",
    "    \"\"\"\n",
    "    # If predictions are probabilities, convert to class predictions\n",
    "    if predictions.ndim > 1:\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predicted_classes = predictions\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(labels, predicted_classes)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "    # Class-wise Accuracy\n",
    "    print(\"\\nClass-wise Accuracy:\")\n",
    "    for d in range(n_classes):\n",
    "        correct_preds = cm[d][d]\n",
    "        total_true_samples = sum(cm[d])\n",
    "        ratio_correct = correct_preds / total_true_samples if total_true_samples != 0 else 0\n",
    "        print(f'Class {d}: Correct Predictions / Total True Samples = {correct_preds}/{total_true_samples} ({ratio_correct:.2%})')\n",
    "\n",
    "    # Precision-Recall Curves and AUPRC\n",
    "    print(\"\\nPrecision-Recall Curves:\")\n",
    "    labels_binarized = label_binarize(labels, classes=np.arange(n_classes))\n",
    "    auprcs = []\n",
    "    for class_idx in range(n_classes):\n",
    "        precision, recall, _ = precision_recall_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        auprcs.append(auprc)\n",
    "        plt.plot(recall, precision, label=f'Class {class_idx + 1} (AUPRC = {auprc:.2f})')\n",
    "\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curves and AUC\n",
    "    print(\"\\nROC Curves:\")\n",
    "    for class_idx in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(labels_binarized[:, class_idx], predictions[:, class_idx])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'Class {class_idx + 1} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves for each class')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Mean F1 Score\n",
    "    f1 = f1_score(labels, predicted_classes, average='macro')\n",
    "    print(f\"\\nMean F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Balanced Accuracy\n",
    "    balanced_acc = balanced_accuracy_score(labels, predicted_classes)\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    # Average AUPRC\n",
    "    mean_auprc = np.mean(auprcs)\n",
    "    print(f\"Average AUPRC: {mean_auprc:.4f}\")\n",
    "\n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predicted_classes))\n",
    "\n",
    "    return {\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"class_wise_accuracy\": [cm[d][d] / sum(cm[d]) if sum(cm[d]) != 0 else 0 for d in range(n_classes)],\n",
    "        \"mean_f1_score\": f1,\n",
    "        \"balanced_accuracy\": balanced_acc,\n",
    "        \"average_auprc\": mean_auprc,\n",
    "        \"auprc_per_class\": auprcs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LSLUvVxl-aAu",
   "metadata": {
    "id": "LSLUvVxl-aAu"
   },
   "source": [
    "model and testing with random data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "u0CZAPTe-ZXJ",
   "metadata": {
    "id": "u0CZAPTe-ZXJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# Debug mode flag\n",
    "DEBUG_MODE = True\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes = 3,num_channels = 20 , num_timepoints = 5120):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = num_timepoints\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, num_channels), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        self.fc1 = nn.Linear(2560, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if DEBUG_MODE: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        if DEBUG_MODE: print(f\"After conv1: {x.shape}\")\n",
    "        x = self.batchnorm1(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm1: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout1: {x.shape}\")\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        if DEBUG_MODE: print(f\"After permute: {x.shape}\")\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        if DEBUG_MODE: print(f\"After padding1: {x.shape}\")\n",
    "        x = F.elu(self.conv2(x))\n",
    "        if DEBUG_MODE: print(f\"After conv2: {x.shape}\")\n",
    "        x = self.batchnorm2(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm2: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout2: {x.shape}\")\n",
    "        x = self.pooling2(x)\n",
    "        if DEBUG_MODE: print(f\"After pooling2: {x.shape}\")\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        if DEBUG_MODE: print(f\"After padding2: {x.shape}\")\n",
    "        x = F.elu(self.conv3(x))\n",
    "        if DEBUG_MODE: print(f\"After conv3: {x.shape}\")\n",
    "        x = self.batchnorm3(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm3: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout3: {x.shape}\")\n",
    "        x = self.pooling3(x)\n",
    "        if DEBUG_MODE: print(f\"After pooling3: {x.shape}\")\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(-1, 4*2*x.size(3))\n",
    "        if DEBUG_MODE: print(f\"After flattening: {x.shape}\")\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "VnpIBK38-hND",
   "metadata": {
    "id": "VnpIBK38-hND"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 1, 5120, 20])\n",
      "After conv1: torch.Size([32, 16, 5120, 1])\n",
      "After batchnorm1: torch.Size([32, 16, 5120, 1])\n",
      "After dropout1: torch.Size([32, 16, 5120, 1])\n",
      "After permute: torch.Size([32, 1, 16, 5120])\n",
      "After padding1: torch.Size([32, 1, 17, 5153])\n",
      "After conv2: torch.Size([32, 4, 16, 5122])\n",
      "After batchnorm2: torch.Size([32, 4, 16, 5122])\n",
      "After dropout2: torch.Size([32, 4, 16, 5122])\n",
      "After pooling2: torch.Size([32, 4, 4, 1281])\n",
      "After padding2: torch.Size([32, 4, 11, 1284])\n",
      "After conv3: torch.Size([32, 4, 4, 1281])\n",
      "After batchnorm3: torch.Size([32, 4, 4, 1281])\n",
      "After dropout3: torch.Size([32, 4, 4, 1281])\n",
      "After pooling3: torch.Size([32, 4, 2, 320])\n",
      "After flattening: torch.Size([32, 2560])\n",
      "Output shape: torch.Size([32, 3])\n",
      "Output: tensor([[0.8952, 0.1927, 0.2391],\n",
      "        [0.9151, 0.5152, 0.2031],\n",
      "        [0.9263, 0.2506, 0.3068],\n",
      "        [0.8935, 0.3210, 0.2142],\n",
      "        [0.8907, 0.1687, 0.3080],\n",
      "        [0.9059, 0.4850, 0.4131],\n",
      "        [0.9130, 0.1579, 0.1975],\n",
      "        [0.9552, 0.3351, 0.1713],\n",
      "        [0.8440, 0.3932, 0.1329],\n",
      "        [0.8150, 0.6590, 0.3522],\n",
      "        [0.8318, 0.3727, 0.1322],\n",
      "        [0.8211, 0.4843, 0.1864],\n",
      "        [0.9187, 0.2693, 0.2845],\n",
      "        [0.8763, 0.3736, 0.2648],\n",
      "        [0.8328, 0.5242, 0.3271],\n",
      "        [0.8849, 0.2741, 0.2383],\n",
      "        [0.9260, 0.3673, 0.2465],\n",
      "        [0.8922, 0.2748, 0.1743],\n",
      "        [0.8687, 0.4436, 0.2543],\n",
      "        [0.8539, 0.3911, 0.2320],\n",
      "        [0.9040, 0.3438, 0.2596],\n",
      "        [0.8687, 0.3678, 0.2318],\n",
      "        [0.9184, 0.2121, 0.2063],\n",
      "        [0.9273, 0.4270, 0.1342],\n",
      "        [0.7896, 0.5657, 0.1861],\n",
      "        [0.8400, 0.2931, 0.2223],\n",
      "        [0.8553, 0.5446, 0.1761],\n",
      "        [0.8842, 0.3552, 0.2152],\n",
      "        [0.9144, 0.4085, 0.2485],\n",
      "        [0.9196, 0.5958, 0.1638],\n",
      "        [0.9078, 0.4483, 0.3203],\n",
      "        [0.8961, 0.4105, 0.3019]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "\n",
    "\n",
    "demo_input = torch.randn(32, 1, 5120,20)\n",
    "\n",
    "# Run forward pass\n",
    "output = model(demo_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb7d72dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cdb7cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape               Output Shape              Param #                   Kernel Shape              Mult-Adds\n",
       "=====================================================================================================================================================================\n",
       "EEGNet (EEGNet)                          [32, 1, 5120, 20]         [32, 3]                   --                        --                        --\n",
       "├─Conv2d (conv1)                         [32, 1, 5120, 20]         [32, 16, 5120, 1]         336                       [1, 20]                   55,050,240\n",
       "├─BatchNorm2d (batchnorm1)               [32, 16, 5120, 1]         [32, 16, 5120, 1]         32                        --                        1,024\n",
       "├─ZeroPad2d (padding1)                   [32, 1, 16, 5120]         [32, 1, 17, 5153]         --                        --                        --\n",
       "├─Conv2d (conv2)                         [32, 1, 17, 5153]         [32, 4, 16, 5122]         260                       [2, 32]                   681,840,640\n",
       "├─BatchNorm2d (batchnorm2)               [32, 4, 16, 5122]         [32, 4, 16, 5122]         8                         --                        256\n",
       "├─MaxPool2d (pooling2)                   [32, 4, 16, 5122]         [32, 4, 4, 1281]          --                        2                         --\n",
       "├─ZeroPad2d (padding2)                   [32, 4, 4, 1281]          [32, 4, 11, 1284]         --                        --                        --\n",
       "├─Conv2d (conv3)                         [32, 4, 11, 1284]         [32, 4, 4, 1281]          516                       [8, 4]                    84,607,488\n",
       "├─BatchNorm2d (batchnorm3)               [32, 4, 4, 1281]          [32, 4, 4, 1281]          8                         --                        256\n",
       "├─MaxPool2d (pooling3)                   [32, 4, 4, 1281]          [32, 4, 2, 320]           --                        [2, 4]                    --\n",
       "├─Linear (fc1)                           [32, 2560]                [32, 3]                   7,683                     --                        245,856\n",
       "=====================================================================================================================================================================\n",
       "Total params: 8,843\n",
       "Trainable params: 8,843\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 821.75\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 13.11\n",
       "Forward/backward pass size (MB): 220.28\n",
       "Params size (MB): 0.04\n",
       "Estimated Total Size (MB): 233.42\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# Print the model summary\n",
    "summary(model, input_size=(32, 1, 5120, 20), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9FTKeUG-oYP",
   "metadata": {
    "id": "b9FTKeUG-oYP"
   },
   "source": [
    "training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aKWY8bN3BwmP",
   "metadata": {
    "id": "aKWY8bN3BwmP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oVWTn0JY-pCi",
   "metadata": {
    "id": "oVWTn0JY-pCi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "# Fold indices and other configurations\n",
    "num_folds = 5\n",
    "fold_indices = np.random.permutation(np.arange(num_folds))\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "test_folds_chosen = []\n",
    "val_folds_chosen = []\n",
    "fold_confusion_matrices = []  # To store confusion matrices for each fold\n",
    "fold_accuracies = []  # To store balanced accuracy for each fold\n",
    "fold_auprcs = []  # To store AUPRC for each fold\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "num_classes = 3\n",
    "n_classes = 3\n",
    "epochs = 100\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "def save_results_to_csv(fold_accuracies, fold_auprcs, fold_confusion_matrices, model_info, csv_path=r\"F:\\CODING\\py\\newnotebooks\\results.csv\"):\n",
    "    \"\"\"\n",
    "    Save all results from the current experiment to a CSV file\n",
    "\n",
    "    Parameters:\n",
    "    - fold_accuracies: list of balanced accuracy scores for each fold\n",
    "    - fold_auprcs: list of AUPRC scores for each fold\n",
    "    - fold_confusion_matrices: list of confusion matrices for each fold\n",
    "    - model_info: string with model architecture description\n",
    "    - csv_path: path to the CSV file to save results\n",
    "    \"\"\"\n",
    "    # Current time for experiment identification\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    # Prepare data for the DataFrame\n",
    "    data = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"model_info\": model_info,\n",
    "        \"num_classes\": num_classes,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"epochs\": epochs,\n",
    "        \"mean_balanced_accuracy\": np.mean(fold_accuracies),\n",
    "        \"std_balanced_accuracy\": np.std(fold_accuracies),\n",
    "        \"mean_auprc\": np.mean(fold_auprcs),\n",
    "        \"std_auprc\": np.std(fold_auprcs),\n",
    "    }\n",
    "\n",
    "    # Add individual fold results\n",
    "    for i, (acc, auprc) in enumerate(zip(fold_accuracies, fold_auprcs)):\n",
    "        data[f\"fold_{i+1}_accuracy\"] = acc\n",
    "        data[f\"fold_{i+1}_auprc\"] = auprc\n",
    "\n",
    "    # Add confusion matrix info\n",
    "    for i, cm in enumerate(fold_confusion_matrices):\n",
    "        data[f\"fold_{i+1}_confusion_matrix\"] = str(cm)\n",
    "\n",
    "    # Create DataFrame and append to CSV\n",
    "    df = pd.DataFrame([data])\n",
    "\n",
    "    # Check if file exists\n",
    "    file_exists = os.path.isfile(csv_path)\n",
    "\n",
    "    # Save to CSV\n",
    "    if file_exists:\n",
    "        df.to_csv(csv_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(csv_path, mode='w', header=True, index=False)\n",
    "\n",
    "    print(f\"Results saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ahy892unBp5d",
   "metadata": {
    "id": "Ahy892unBp5d"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training loop for cross-validation\n",
    "for fold_idx in range(num_folds):\n",
    "    print(f'Fold No: {fold_idx + 1}')\n",
    "\n",
    "    # Initialize model, loss, and optimizer\n",
    "    fold_model = EEGNet(num_classes=num_classes).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "    optimizer = optim.Adam(fold_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Split data into train, validation, and test sets\n",
    "    test_fold = fold_indices[fold_idx]\n",
    "    val_fold = val_fold_indices[fold_idx]\n",
    "    train_folds = [fold for fold in fold_indices if fold != test_fold and fold != val_fold]\n",
    "\n",
    "    train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "    train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "\n",
    "\n",
    "    test_folds_chosen.append(test_fold)\n",
    "    val_folds_chosen.append(val_fold)\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=1)\n",
    "\n",
    "        train_dataset = TensorDataset(\n",
    "            torch.tensor(balanced_train_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(balanced_train_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "        fold_model.train()\n",
    "        running_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = fold_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        # Validation loop\n",
    "        val_data = eeg_folds[val_fold]\n",
    "        val_data = val_data\n",
    "        val_labels = labels_folds[val_fold]\n",
    "        val_dataset = TensorDataset(\n",
    "            torch.tensor(val_data, dtype=torch.float32).to(device),\n",
    "            torch.tensor(val_labels, dtype=torch.long).to(device)\n",
    "        )\n",
    "        val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        fold_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds = []\n",
    "        val_labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_loader:\n",
    "                val_outputs = fold_model(val_inputs)\n",
    "                loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, val_batch_preds = torch.max(val_outputs, 1)\n",
    "                val_preds.extend(val_batch_preds.cpu().numpy())\n",
    "                val_labels_list.extend(val_labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = balanced_accuracy_score(val_labels_list, val_preds)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        early_stopping(val_loss, fold_model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    early_stopping.load_best_model(fold_model)\n",
    "\n",
    "    # Test loop\n",
    "    test_data = eeg_folds[test_fold]\n",
    "    test_data = test_data\n",
    "    test_labels = labels_folds[test_fold]\n",
    "    test_dataset = TensorDataset(\n",
    "        torch.tensor(test_data, dtype=torch.float32).to(device),\n",
    "        torch.tensor(test_labels, dtype=torch.long).to(device)\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "    fold_model.eval()\n",
    "    test_probs = []\n",
    "    test_preds = []\n",
    "    test_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels in test_loader:\n",
    "            test_outputs = fold_model(test_inputs)\n",
    "            probabilities = torch.softmax(test_outputs, dim=1)\n",
    "            test_probs.extend(probabilities.cpu().numpy())\n",
    "            _, preds = torch.max(probabilities, 1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_labels_list.extend(test_labels.cpu().numpy())\n",
    "\n",
    "    # Compute metrics\n",
    "    test_acc = balanced_accuracy_score(test_labels_list, test_preds)\n",
    "    fold_accuracies.append(test_acc)\n",
    "\n",
    "    cm = confusion_matrix(test_labels_list, test_preds)\n",
    "    fold_confusion_matrices.append(cm)\n",
    "\n",
    "    test_labels_binarized = label_binarize(test_labels_list, classes=np.arange(num_classes))\n",
    "    test_auprcs = []\n",
    "    for class_idx in range(num_classes):\n",
    "        precision, recall, _ = precision_recall_curve(test_labels_binarized[:, class_idx], np.array(test_probs)[:, class_idx])\n",
    "        auprc = auc(recall, precision)\n",
    "        test_auprcs.append(auprc)\n",
    "\n",
    "    mean_test_auprc = np.mean(test_auprcs)\n",
    "    fold_auprcs.append(mean_test_auprc)\n",
    "\n",
    "    print(f'Test Fold {fold_idx + 1}, Mean AUPRC: {mean_test_auprc:.4f}, Balanced Accuracy: {test_acc:.4f}')\n",
    "\n",
    "    # Use the plot_metrics function to visualize metrics\n",
    "    plot_metrics(np.array(test_labels_list), np.array(test_probs), n_classes=num_classes)\n",
    "\n",
    "# Final metrics across all folds\n",
    "average_auprc = np.mean(fold_auprcs)\n",
    "mean_accuracy = np.mean(fold_accuracies)\n",
    "print(f'Accuracy for each fold: {fold_accuracies}')\n",
    "print(f'AUPRC for each fold: {fold_auprcs}')\n",
    "print(f'Average AUPRC across all folds: {average_auprc}')\n",
    "print(f'Average Balanced Accuracy across all folds: {mean_accuracy}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8J__nt7hEfFQ",
   "metadata": {
    "id": "8J__nt7hEfFQ"
   },
   "outputs": [],
   "source": [
    "#save results to results.csv\n",
    "save_path = r\"SAVE PATH HERE\"\n",
    "spect = \"MEL\"\n",
    "save_results_to_csv(fold_accuracies,fold_auprcs,fold_confusion_matrices,model_info=f\"EEGNET MODEL {num_classes}classes {spect} spectrogram\",csv_path=save_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
