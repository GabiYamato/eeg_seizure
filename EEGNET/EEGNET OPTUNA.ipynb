{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d84bd",
   "metadata": {
    "id": "626d84bd"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ad51f8",
   "metadata": {
    "id": "29ad51f8"
   },
   "outputs": [],
   "source": [
    "# load data, labels, patients from pkl files\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_data(data_path, labels_path, patients_path):\n",
    "    with open(data_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    with open(labels_path, 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "    with open(patients_path, 'rb') as f:\n",
    "        patients = pickle.load(f)\n",
    "    return data, labels, patients\n",
    "\n",
    "data,labels, patients = load_data('data.pkl', 'labels.pkl', 'patients.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc784f71",
   "metadata": {},
   "source": [
    "<h1> DATA SPLITTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb7475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def separate_data_by_class(labels,data, patients, class_no):\n",
    "    class_indices = np.where(labels == int(class_no))[0]\n",
    "    data_class = data[class_indices]\n",
    "    labels_class = labels[class_indices]\n",
    "    patients_class = patients[class_indices]\n",
    "    return class_indices, data_class, labels_class, patients_class\n",
    "\n",
    "\n",
    "def create_patient_folds(patients, n_splits=5):\n",
    "    all_unique_patients = np.unique(patients)\n",
    "    print(\"No. of unique patients is:\", len(all_unique_patients))\n",
    "    np.random.shuffle(all_unique_patients)\n",
    "    patient_splits = np.array_split(all_unique_patients, n_splits)\n",
    "\n",
    "    patient_fold_mapping = {}\n",
    "    for fold, patient_group in enumerate(patient_splits):\n",
    "        for patient in patient_group:\n",
    "            patient_fold_mapping[patient] = fold\n",
    "\n",
    "    return patient_fold_mapping\n",
    "\n",
    "def split_data_by_patients_across_classes(eeg_data, labels, patients, patient_fold_mapping, n_splits=5):\n",
    "    eeg_data_splits = [[] for _ in range(n_splits)]\n",
    "    labels_splits = [[] for _ in range(n_splits)]\n",
    "    patients_splits = [[] for _ in range(n_splits)]\n",
    "    for i in range(len(patients)):\n",
    "        patient_id = patients[i]\n",
    "        fold = patient_fold_mapping[patient_id]  # Get the fold number from the mapping\n",
    "        eeg_data_splits[fold].append(eeg_data[i])\n",
    "        labels_splits[fold].append(labels[i])\n",
    "        patients_splits[fold].append(patients[i])\n",
    "        \n",
    "    eeg_data_splits = [np.array(split) for split in eeg_data_splits]\n",
    "    labels_splits = [np.array(split) for split in labels_splits]\n",
    "    patients_splits = [np.array(split) for split in patients_splits]\n",
    "\n",
    "    return eeg_data_splits, labels_splits, patients_splits\n",
    "\n",
    "\n",
    "def shuffle_fold(eeg_fold, labels_fold, patients_fold):\n",
    "    indices = np.arange(len(eeg_fold))  \n",
    "    np.random.shuffle(indices)          \n",
    "    eeg_fold_shuffled = eeg_fold[indices]\n",
    "    labels_fold_shuffled = labels_fold[indices]\n",
    "    patients_fold_shuffled = patients_fold[indices]\n",
    "\n",
    "    return eeg_fold_shuffled, labels_fold_shuffled, patients_fold_shuffled\n",
    "\n",
    "\n",
    "c0_indices, data_c0, labels_c0, patients_c0 = separate_data_by_class(labels, data, patients, 0)\n",
    "\n",
    "c1_indices, data_c1, labels_c1, patients_c1 = separate_data_by_class(labels, data, patients, 1)\n",
    "\n",
    "c2_indices, data_c2, labels_c2, patients_c2 = separate_data_by_class(labels, data, patients, 2)\n",
    "\n",
    "\n",
    "print(f\"Class 0: {data_c0.shape}, {labels_c0.shape}, {len(np.unique(patients_c0))}\")\n",
    "print(f\"Class 1: {data_c1.shape}, {labels_c1.shape}, {len(np.unique(patients_c1))}\")\n",
    "print(f\"Class 2: {data_c2.shape}, {labels_c2.shape}, {len(np.unique(patients_c2))}\")\n",
    "\n",
    "\n",
    "patients_set_c0 = set(patients_c0)\n",
    "patients_set_c1 = set(patients_c1)\n",
    "patients_set_c2 = set(patients_c2)\n",
    "\n",
    "common_patients_all = patients_set_c0.intersection(patients_set_c1, patients_set_c2)\n",
    "common_patients_all = list(common_patients_all)\n",
    "print('No. of common patients between all classes: ', len(common_patients_all))\n",
    "\n",
    "def extract_common_data(patients, data, labels, common_patient):\n",
    "    common_indices = [i for i, patient in enumerate(patients) if patient in common_patient]\n",
    "    common_data = data[common_indices]\n",
    "    common_labels = labels[common_indices]\n",
    "    common_patients = patients[common_indices]\n",
    "    return common_data, common_labels, common_patients\n",
    "\n",
    "common_data_c0, common_labels_c0, common_patients_c0 = extract_common_data(patients_c0, data_c0, labels_c0, common_patients_all)\n",
    "common_data_c1, common_labels_c1, common_patients_c1 = extract_common_data(patients_c1, data_c1, labels_c1, common_patients_all)\n",
    "common_data_c2, common_labels_c2, common_patients_c2 = extract_common_data(patients_c2, data_c2, labels_c2, common_patients_all)\n",
    "\n",
    "print(np.unique(common_patients_c0).shape)\n",
    "\n",
    "common_allpatient_fold_mapping = create_patient_folds(common_patients_all, n_splits=5)\n",
    "eeg_012_c0, labels_012_c0, patients_012_c0 = split_data_by_patients_across_classes(\n",
    "    common_data_c0, common_labels_c0, common_patients_c0, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "eeg_012_c1, labels_012_c1, patients_012_c1 = split_data_by_patients_across_classes(\n",
    "    common_data_c1, common_labels_c1, common_patients_c1, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "eeg_012_c2, labels_012_c2, patients_012_c2 = split_data_by_patients_across_classes(\n",
    "    common_data_c2, common_labels_c2, common_patients_c2, common_allpatient_fold_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([eeg_012_c0[i], eeg_012_c1[i], eeg_012_c2[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([labels_012_c0[i], labels_012_c1[i], labels_012_c2[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([patients_012_c0[i], patients_012_c1[i], patients_012_c2[i]]).shape}\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {eeg_012_c0[i].shape}, \"\n",
    "          f\"Labels shape 0: {labels_012_c0[i].shape}, \"\n",
    "          f\"Patients shape 0: {patients_012_c0[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {eeg_012_c1[i].shape}, \"\n",
    "      f\"Labels shape 1: {labels_012_c1[i].shape}, \"\n",
    "      f\"Patients shape 1: {patients_012_c1[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 2: {eeg_012_c2[i].shape}, \"\n",
    "      f\"Labels shape 2: {labels_012_c2[i].shape}, \"\n",
    "      f\"Patients shape 2: {patients_012_c2[i].shape}\")\n",
    "    \n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(patients_012_c2[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(patients_012_c1[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(patients_012_c0[i]))}\")\n",
    "    \n",
    "    # Find exclusive patients for each class\n",
    "exclusive_patients_c0 = list(patients_set_c0 - set(common_patients_all))\n",
    "exclusive_patients_c1 = list(patients_set_c1 - set(common_patients_all))\n",
    "exclusive_patients_c2 = list(patients_set_c2 - set(common_patients_all))\n",
    "\n",
    "print(len(exclusive_patients_c0))\n",
    "print(len(exclusive_patients_c1))\n",
    "print(len(exclusive_patients_c2))\n",
    "\n",
    "exclusive_c2 = set(exclusive_patients_c2)\n",
    "exclusive_c0 = set(exclusive_patients_c0)\n",
    "common_c2_c0 = exclusive_c0.intersection(exclusive_c2)\n",
    "common_patient_c2_c0 = list(common_c2_c0)\n",
    "\n",
    "print(len(common_patient_c2_c0))\n",
    "\n",
    "\n",
    "\n",
    "common_data_c02, common_labels_c02, common_patients_c02 = extract_common_data(patients_c0, data_c0, labels_c0, common_patient_c2_c0)\n",
    "common_data_c22, common_labels_c22, common_patients_c22 = extract_common_data(patients_c2, data_c2, labels_c2, common_patient_c2_c0)\n",
    "\n",
    "print(np.unique(common_patients_c02).shape)\n",
    "print(np.unique(common_patients_c22).shape)\n",
    "\n",
    "c20_mapping = create_patient_folds(common_patient_c2_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c02, common_labels_splits_c02, common_patients_splits_c02 = split_data_by_patients_across_classes(\n",
    "    common_data_c02, common_labels_c02, common_patients_c02, c20_mapping, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c22, common_labels_splits_c22, common_patients_splits_c22 = split_data_by_patients_across_classes(\n",
    "    common_data_c22, common_labels_c22, common_patients_c22, c20_mapping, n_splits=5)\n",
    "\n",
    "\n",
    "# Check the shape of each fold for verification\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c02[i], common_eeg_splits_c22[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c02[i], common_labels_splits_c22[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c02[i], common_patients_splits_c22[i]]).shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {common_eeg_splits_c02[i].shape}, \"\n",
    "          f\"Labels shape 0: {common_labels_splits_c02[i].shape}, \"\n",
    "          f\"Patients shape 0: {common_patients_splits_c02[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {common_eeg_splits_c22[i].shape}, \"\n",
    "      f\"Labels shape 1: {common_labels_splits_c22[i].shape}, \"\n",
    "      f\"Patients shape 1: {common_patients_splits_c22[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c02[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(common_patients_splits_c22[i]))}\")\n",
    "\n",
    "\n",
    "# Find exclusive patients for each class\n",
    "exclusive_patients_c0_c2 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0))\n",
    "exclusive_patients_c1_c2 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0))\n",
    "exclusive_patients_c2_c2 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0))\n",
    "\n",
    "print(len(exclusive_patients_c0_c2))\n",
    "print(len(exclusive_patients_c1_c2))\n",
    "print(len(exclusive_patients_c2_c2))\n",
    "\n",
    "exclusive_c2 = set(exclusive_patients_c2_c2)\n",
    "exclusive_c1 = set(exclusive_patients_c1_c2)\n",
    "common_c2_c1 = exclusive_c1.intersection(exclusive_c2)\n",
    "common_patient_c2_c1 = list(common_c2_c1)\n",
    "\n",
    "print(len(common_patient_c2_c1))\n",
    "\n",
    "\n",
    "\n",
    "common_data_c12, common_labels_c12, common_patients_c12 = extract_common_data(patients_c1, data_c1, labels_c1, common_patient_c2_c1)\n",
    "common_data_1_c22, common_labels_1_c22, common_patients_1_c22 = extract_common_data(patients_c2, data_c2, labels_c2, common_patient_c2_c1)\n",
    "\n",
    "print(np.unique(common_patients_c12).shape)\n",
    "print(np.unique(common_patients_1_c22).shape)\n",
    "\n",
    "c21_mapping = create_patient_folds(common_patient_c2_c1, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c12, common_labels_splits_c12, common_patients_splits_c12 = split_data_by_patients_across_classes(\n",
    "    common_data_c12, common_labels_c12, common_patients_c12, c21_mapping, n_splits=5)\n",
    "\n",
    "common_eeg_splits_1_c22, common_labels_splits_1_c22, common_patients_splits_1_c22 = split_data_by_patients_across_classes(\n",
    "    common_data_1_c22, common_labels_1_c22, common_patients_1_c22, c21_mapping, n_splits=5)\n",
    "\n",
    "\n",
    "# Check the shape of each fold for verification\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c12[i], common_eeg_splits_1_c22[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c12[i], common_labels_splits_1_c22[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c12[i], common_patients_splits_1_c22[i]]).shape}\")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c12[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(common_patients_splits_1_c22[i]))}\")\n",
    "    \n",
    "\n",
    "exclusive_patients_c0_c2_c1 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1))\n",
    "exclusive_patients_c1_c2_c1 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1))\n",
    "exclusive_patients_c2_c2_c1 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0)- set(common_c2_c1))\n",
    "\n",
    "print(len(exclusive_patients_c0_c2_c1))\n",
    "print(len(exclusive_patients_c1_c2_c1))\n",
    "print(len(exclusive_patients_c2_c2_c1))\n",
    "\n",
    "only_c2 = set(exclusive_patients_c2_c2_c1)\n",
    "only_c2= list(only_c2)\n",
    "data_only_c2, labels_only_c2,patients_only_c2 = extract_common_data(patients_c2, data_c2, labels_c2, only_c2)\n",
    "print(np.unique(patients_only_c2).shape)\n",
    "\n",
    "c2_mapping = create_patient_folds(only_c2, n_splits=5)\n",
    "\n",
    "eeg_splits_c2, labels_splits_c2, patients_splits_c2 = split_data_by_patients_across_classes(\n",
    "    data_only_c2, labels_c2, patients_only_c2, c2_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 2 is {len(np.unique(patients_splits_c2[i]))}\")\n",
    "    \n",
    "    # Find exclusive patients for each class\n",
    "exclusive_patients_all_c02 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "exclusive_patients_all_c12 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "exclusive_patients_all_c22 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2))\n",
    "\n",
    "print(len(exclusive_patients_all_c02))\n",
    "print(len(exclusive_patients_all_c12))\n",
    "print(len(exclusive_patients_all_c22))\n",
    "\n",
    "\n",
    "exc_c1_c11 = set(exclusive_patients_all_c12)\n",
    "exc_c1_c01 = set(exclusive_patients_all_c02)\n",
    "common_patient_c1_c0 = exc_c1_c01.intersection(exc_c1_c11)\n",
    "common_patient_c1_c0 = list(common_patient_c1_c0)\n",
    "\n",
    "print(len(common_patient_c1_c0))\n",
    "\n",
    "common_data_c1_c01, common_labels_c1_c01, common_patients_c1_c01 = extract_common_data(patients_c0, data_c0, labels_c0, common_patient_c1_c0)\n",
    "common_data_c1_c11, common_labels_c1_c11, common_patients_c1_c11 = extract_common_data(patients_c1, data_c1, labels_c1, common_patient_c1_c0)\n",
    "\n",
    "print(np.unique(common_patients_c1_c01).shape)\n",
    "\n",
    "mapping_c1_c0 = create_patient_folds(common_patient_c1_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c1_c01, common_labels_splits_c1_c01, common_patients_splits_c1_c01= split_data_by_patients_across_classes(\n",
    "    common_data_c1_c01, common_labels_c1_c01, common_patients_c1_c01, mapping_c1_c0, n_splits=5)\n",
    "\n",
    "common_eeg_splits_c1_c11, common_labels_splits_c1_c11, common_patients_splits_c1_c11 = split_data_by_patients_across_classes(\n",
    "    common_data_c1_c11, common_labels_c1_c11, common_patients_c1_c11, mapping_c1_c0, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape: {np.concatenate([common_eeg_splits_c1_c01[i], common_eeg_splits_c1_c11[i]]).shape}, \"\n",
    "          f\"Labels shape: {np.concatenate([common_labels_splits_c1_c01[i], common_labels_splits_c1_c11[i]]).shape}, \"\n",
    "          f\"Patients shape: {np.concatenate([common_patients_splits_c1_c01[i], common_patients_splits_c1_c11[i]]).shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 0: {common_eeg_splits_c1_c01[i].shape}, \"\n",
    "          f\"Labels shape 0: {common_labels_splits_c1_c01[i].shape}, \"\n",
    "          f\"Patients shape 0: {common_patients_splits_c1_c01[i].shape}\")\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"Fold {i+1}: EEG shape 1: {common_eeg_splits_c1_c11[i].shape}, \"\n",
    "      f\"Labels shape 1: {common_labels_splits_c1_c11[i].shape}, \"\n",
    "      f\"Patients shape 1: {common_patients_splits_c1_c11[i].shape}\")\n",
    "\n",
    "print(\"__________________________________\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(common_patients_splits_c1_c01[i]))}\")\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(common_patients_splits_c1_c11[i]))}\")\n",
    "    \n",
    "exclusive_patients_c1_c01 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0))\n",
    "exclusive_patients_c1_c11 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0))\n",
    "exclusive_patients_c1_c21 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0))\n",
    "\n",
    "print(len(exclusive_patients_c1_c01))\n",
    "print(len(exclusive_patients_c1_c11))\n",
    "print(len(exclusive_patients_c1_c21))\n",
    "\n",
    "\n",
    "\n",
    "print(\"CLASS 1\")\n",
    "\n",
    "only_c1 = set(exclusive_patients_c1_c11)\n",
    "only_c1= list(only_c1)\n",
    "data_only_c1, labels_only_c1,patients_only_c1 = extract_common_data(patients_c1, data_c1, labels_c1, only_c1)\n",
    "print(np.unique(patients_only_c1).shape)\n",
    "\n",
    "c1_mapping = create_patient_folds(only_c1, n_splits=5)\n",
    "\n",
    "eeg_splits_c1, labels_splits_c1, patients_splits_c1 = split_data_by_patients_across_classes(\n",
    "    data_only_c1, labels_c1, patients_only_c1, c1_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 1 is {len(np.unique(patients_splits_c1[i]))}\")\n",
    "    \n",
    "exclusive_patients_all1_c01 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0)- set(only_c1))\n",
    "exclusive_patients_all1_c11 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1))\n",
    "exclusive_patients_all1_c21 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1))\n",
    "\n",
    "print(len(exclusive_patients_all1_c01))\n",
    "print(len(exclusive_patients_all1_c11))\n",
    "print(len(exclusive_patients_all1_c21))\n",
    "\n",
    "print(\"CLASS 0\")\n",
    "only_c0 = set(exclusive_patients_all1_c01)\n",
    "only_c0= list(only_c0)\n",
    "data_only_c0, labels_only_c0,patients_only_c0 = extract_common_data(patients_c0, data_c0, labels_c0, only_c0)\n",
    "print(np.unique(patients_only_c0).shape)\n",
    "\n",
    "c0_mapping = create_patient_folds(only_c0, n_splits=5)\n",
    "\n",
    "eeg_splits_c0, labels_splits_c0, patients_splits_c0 = split_data_by_patients_across_classes(\n",
    "    data_only_c0, labels_c0, patients_only_c0, c0_mapping, n_splits=5)\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"No. of patients in class 0 is {len(np.unique(patients_splits_c0[i]))}\")\n",
    "\n",
    "final_c0 = list(patients_set_c0 - set(common_patients_all) - set(common_c2_c0) - set(common_c2_c1)- set(only_c2) - set(common_patient_c1_c0)- set(only_c1) - set(only_c0))\n",
    "final_c1 = list(patients_set_c1 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1)- set(only_c0))\n",
    "final_c2 = list(patients_set_c2 - set(common_patients_all)- set(common_c2_c0) - set(common_c2_c1)- set(only_c2)- set(common_patient_c1_c0)- set(only_c1)- set(only_c0))\n",
    "\n",
    "print(len(final_c0))\n",
    "print(len(final_c1))\n",
    "print(len(final_c2))\n",
    "\n",
    "eeg_fold_1 = np.concatenate([eeg_012_c0[0], eeg_012_c1[0], eeg_012_c2[0], common_eeg_splits_c02[0], common_eeg_splits_c22[0], common_eeg_splits_c12[4], common_eeg_splits_1_c22[4],eeg_splits_c2[0],common_eeg_splits_c1_c01[0], common_eeg_splits_c1_c11[0],eeg_splits_c1[0],eeg_splits_c0[4]])\n",
    "labels_fold_1 = np.concatenate([labels_012_c0[0], labels_012_c1[0], labels_012_c2[0],common_labels_splits_c02[0], common_labels_splits_c22[0],common_labels_splits_c12[4], common_labels_splits_1_c22[4],labels_splits_c2[0],common_labels_splits_c1_c01[0], common_labels_splits_c1_c11[0],labels_splits_c1[0],labels_splits_c0[4] ])\n",
    "patients_fold_1 = np.concatenate([patients_012_c0[0], patients_012_c1[0], patients_012_c2[0],common_patients_splits_c02[0], common_patients_splits_c22[0],common_patients_splits_c12[4], common_patients_splits_1_c22[4],patients_splits_c2[0],common_patients_splits_c1_c01[0], common_patients_splits_c1_c11[0],patients_splits_c1[0],patients_splits_c0[4]])\n",
    "\n",
    "eeg_fold_2 = np.concatenate([eeg_012_c0[1], eeg_012_c1[1], eeg_012_c2[1], common_eeg_splits_c02[1], common_eeg_splits_c22[1], common_eeg_splits_c12[3], common_eeg_splits_1_c22[3],eeg_splits_c2[1],common_eeg_splits_c1_c01[1], common_eeg_splits_c1_c11[1],eeg_splits_c1[1],eeg_splits_c0[3]])\n",
    "labels_fold_2 = np.concatenate([labels_012_c0[1], labels_012_c1[1], labels_012_c2[1],common_labels_splits_c02[1], common_labels_splits_c22[1],common_labels_splits_c12[3], common_labels_splits_1_c22[3],labels_splits_c2[1],common_labels_splits_c1_c01[1], common_labels_splits_c1_c11[1],labels_splits_c1[1],labels_splits_c0[3] ])\n",
    "patients_fold_2 = np.concatenate([patients_012_c0[1], patients_012_c1[1], patients_012_c2[1],common_patients_splits_c02[1], common_patients_splits_c22[1],common_patients_splits_c12[3], common_patients_splits_1_c22[3],patients_splits_c2[1],common_patients_splits_c1_c01[1], common_patients_splits_c1_c11[1],patients_splits_c1[1],patients_splits_c0[3]])\n",
    "\n",
    "eeg_fold_3 = np.concatenate([eeg_012_c0[2], eeg_012_c1[2], eeg_012_c2[2], common_eeg_splits_c02[2], common_eeg_splits_c22[2], common_eeg_splits_c12[2], common_eeg_splits_1_c22[2],eeg_splits_c2[2],common_eeg_splits_c1_c01[2], common_eeg_splits_c1_c11[2],eeg_splits_c1[2],eeg_splits_c0[2]])\n",
    "labels_fold_3 = np.concatenate([labels_012_c0[2], labels_012_c1[2], labels_012_c2[2],common_labels_splits_c02[2], common_labels_splits_c22[2],common_labels_splits_c12[2], common_labels_splits_1_c22[2],labels_splits_c2[2],common_labels_splits_c1_c01[2], common_labels_splits_c1_c11[2],labels_splits_c1[2],labels_splits_c0[2] ])\n",
    "patients_fold_3 = np.concatenate([patients_012_c0[2], patients_012_c1[2], patients_012_c2[2],common_patients_splits_c02[2], common_patients_splits_c22[2],common_patients_splits_c12[2], common_patients_splits_1_c22[2],patients_splits_c2[2],common_patients_splits_c1_c01[2], common_patients_splits_c1_c11[2],patients_splits_c1[2],patients_splits_c0[2]])\n",
    "\n",
    "eeg_fold_4 = np.concatenate([eeg_012_c0[3], eeg_012_c1[3], eeg_012_c2[3], common_eeg_splits_c02[3], common_eeg_splits_c22[3], common_eeg_splits_c12[1], common_eeg_splits_1_c22[1],eeg_splits_c2[3],common_eeg_splits_c1_c01[3], common_eeg_splits_c1_c11[3],eeg_splits_c1[3],eeg_splits_c0[1]])\n",
    "labels_fold_4 = np.concatenate([labels_012_c0[3], labels_012_c1[3], labels_012_c2[3],common_labels_splits_c02[3], common_labels_splits_c22[3],common_labels_splits_c12[1], common_labels_splits_1_c22[1],labels_splits_c2[3],common_labels_splits_c1_c01[3], common_labels_splits_c1_c11[3],labels_splits_c1[3],labels_splits_c0[1] ])\n",
    "patients_fold_4 = np.concatenate([patients_012_c0[3], patients_012_c1[3], patients_012_c2[3],common_patients_splits_c02[3], common_patients_splits_c22[3],common_patients_splits_c12[1], common_patients_splits_1_c22[1],patients_splits_c2[3],common_patients_splits_c1_c01[3], common_patients_splits_c1_c11[3],patients_splits_c1[3],patients_splits_c0[1]])\n",
    "\n",
    "eeg_fold_5 = np.concatenate([eeg_012_c0[4], eeg_012_c1[4], eeg_012_c2[4], common_eeg_splits_c02[4], common_eeg_splits_c22[4], common_eeg_splits_c12[0], common_eeg_splits_1_c22[0],eeg_splits_c2[4],common_eeg_splits_c1_c01[4], common_eeg_splits_c1_c11[4],eeg_splits_c1[4],eeg_splits_c0[0]])\n",
    "labels_fold_5 = np.concatenate([labels_012_c0[4], labels_012_c1[4], labels_012_c2[4],common_labels_splits_c02[4], common_labels_splits_c22[4],common_labels_splits_c12[0], common_labels_splits_1_c22[0],labels_splits_c2[4],common_labels_splits_c1_c01[4], common_labels_splits_c1_c11[4],labels_splits_c1[4],labels_splits_c0[0] ])\n",
    "patients_fold_5 = np.concatenate([patients_012_c0[4], patients_012_c1[4], patients_012_c2[4],common_patients_splits_c02[4], common_patients_splits_c22[4],common_patients_splits_c12[0], common_patients_splits_1_c22[0],patients_splits_c2[4],common_patients_splits_c1_c01[4], common_patients_splits_c1_c11[4],patients_splits_c1[4],patients_splits_c0[0]])\n",
    "\n",
    "eeg_folds = [eeg_fold_1, eeg_fold_2, eeg_fold_3, eeg_fold_4, eeg_fold_5]\n",
    "labels_folds = [labels_fold_1, labels_fold_2, labels_fold_3, labels_fold_4, labels_fold_5]\n",
    "patients_folds = [patients_fold_1, patients_fold_2, patients_fold_3, patients_fold_4, patients_fold_5]\n",
    "\n",
    "for i in range(len(eeg_folds)):\n",
    "    eeg_folds[i] = eeg_folds[i].astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd3e9ab",
   "metadata": {},
   "source": [
    "<h1> DATA BALANCER AND EARLYSTOPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9831e02",
   "metadata": {
    "id": "f9831e02"
   },
   "outputs": [],
   "source": [
    "def data_balancer(data, labels, factor):\n",
    "    # Count the number of samples in each class\n",
    "    num_class_0 = np.sum(labels == 0)\n",
    "    num_class_1 = np.sum(labels == 1)\n",
    "    num_class_2 = np.sum(labels == 2)\n",
    "\n",
    "    # Find the minimum number of samples across all classes\n",
    "    min_samples = min(num_class_0, num_class_1, num_class_2)\n",
    "\n",
    "    # Calculate the number of samples to take from each class\n",
    "    samples_per_class = min_samples // factor\n",
    "\n",
    "    # Randomly sample 'samples_per_class' from each class\n",
    "    class_0_indices = np.random.choice(np.where(labels == 0)[0], samples_per_class, replace=False)\n",
    "    class_1_indices = np.random.choice(np.where(labels == 1)[0], samples_per_class, replace=False)\n",
    "    class_2_indices = np.random.choice(np.where(labels == 2)[0], samples_per_class, replace=False)\n",
    "\n",
    "    # Combine balanced indices\n",
    "    balanced_indices = np.concatenate((class_0_indices, class_1_indices, class_2_indices))\n",
    "\n",
    "    # Shuffle the balanced indices\n",
    "    np.random.shuffle(balanced_indices)\n",
    "\n",
    "    # Create balanced training data and labels\n",
    "    balanced_data = data[balanced_indices]\n",
    "    balanced_labels = labels[balanced_indices]\n",
    "\n",
    "    return balanced_data, balanced_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497bd4e",
   "metadata": {
    "id": "0497bd4e"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5):\n",
    "        \"\"\"\n",
    "        Initializes the early stopping mechanism based on divergence detection.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of consecutive epochs with increasing validation loss\n",
    "                            before stopping.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Checks if the validation loss is diverging and updates the state accordingly.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_loss is None or val_loss < self.best_loss:\n",
    "            # Improvement detected\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            # Validation loss increased\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                print(f\"Divergence detected. Stopping training after {self.counter} epochs.\")\n",
    "                self.early_stop = True\n",
    "\n",
    "    def load_best_model(self, model):\n",
    "        \"\"\"\n",
    "        Restores the model to the state with the lowest validation loss.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model to restore.\n",
    "        \"\"\"\n",
    "        model.load_state_dict(self.best_model_state)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9bba8",
   "metadata": {
    "id": "04c9bba8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "debug_mode_flag = False\n",
    "\n",
    "# Debug mode flag\n",
    "DEBUG_MODE = False\n",
    "\n",
    "\n",
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes = 3,num_channels = 20 , num_timepoints = 5120):\n",
    "        super(EEGNet, self).__init__()\n",
    "        self.T = num_timepoints\n",
    "        \n",
    "        # Layer 1\n",
    "        self.conv1 = nn.Conv2d(1, 16, (1, num_channels), padding = 0)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
    "        self.conv2 = nn.Conv2d(1, 4, (2, 32))\n",
    "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
    "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
    "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
    "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
    "        \n",
    "        # FC Layer\n",
    "        # NOTE: This dimension will depend on the number of timestamps per sample in your data.\n",
    "        self.fc1 = nn.Linear(2560, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if DEBUG_MODE: print(f\"Input shape: {x.shape}\")\n",
    "        \n",
    "        # Layer 1\n",
    "        x = F.elu(self.conv1(x))\n",
    "        if DEBUG_MODE: print(f\"After conv1: {x.shape}\")\n",
    "        x = self.batchnorm1(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm1: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout1: {x.shape}\")\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        if DEBUG_MODE: print(f\"After permute: {x.shape}\")\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.padding1(x)\n",
    "        if DEBUG_MODE: print(f\"After padding1: {x.shape}\")\n",
    "        x = F.elu(self.conv2(x))\n",
    "        if DEBUG_MODE: print(f\"After conv2: {x.shape}\")\n",
    "        x = self.batchnorm2(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm2: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout2: {x.shape}\")\n",
    "        x = self.pooling2(x)\n",
    "        if DEBUG_MODE: print(f\"After pooling2: {x.shape}\")\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.padding2(x)\n",
    "        if DEBUG_MODE: print(f\"After padding2: {x.shape}\")\n",
    "        x = F.elu(self.conv3(x))\n",
    "        if DEBUG_MODE: print(f\"After conv3: {x.shape}\")\n",
    "        x = self.batchnorm3(x)\n",
    "        if DEBUG_MODE: print(f\"After batchnorm3: {x.shape}\")\n",
    "        x = F.dropout(x, 0.25)\n",
    "        if DEBUG_MODE: print(f\"After dropout3: {x.shape}\")\n",
    "        x = self.pooling3(x)\n",
    "        if DEBUG_MODE: print(f\"After pooling3: {x.shape}\")\n",
    "        \n",
    "        # FC Layer\n",
    "        x = x.reshape(-1, 4*2*x.size(3))\n",
    "        if DEBUG_MODE: print(f\"After flattening: {x.shape}\")\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53c211",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1747776598293,
     "user": {
      "displayName": "Ryan Gabriel",
      "userId": "15533354653069607353"
     },
     "user_tz": -330
    },
    "id": "eb53c211",
    "outputId": "ba1ef9f9-c6c9-48e4-d709-e615639718b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3])\n"
     ]
    }
   ],
   "source": [
    "model = EEGNet()\n",
    "\n",
    "\n",
    "demo_input = torch.randn(32, 1, 5120,20)\n",
    "\n",
    "# Run forward pass\n",
    "output = model(demo_input)\n",
    "\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\", output)\n",
    "from torchinfo import summary\n",
    "# Print the model summary\n",
    "summary(model, input_size=(32, 1, 5120, 20), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"], row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "psh7bqJLmK1u",
   "metadata": {
    "id": "psh7bqJLmK1u"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pca = sklearn.decomposition.PCA(3)\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "fold_indices = np.arange(5)\n",
    "fold_indices = np.random.permutation(fold_indices)\n",
    "val_fold_indices = np.roll(fold_indices, 1)\n",
    "\n",
    "\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f665b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a5f665b8",
    "outputId": "72b7174f-1d92-4492-8392-ccccf613c130"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-20 21:30:01,632] A new study created in memory with name: no-name-b7caaa91-ba18-46ec-b10b-637faa3b9b9f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size: 1024, num_layers: 4, learning_rate: 0.00018671377065131162, optimizer_name: Adam, weight_decay: 1.4865027075569687e-06, batch_size: 16, label_smoothing: 0.3, factor: 1hidden_size: 128, num_layers: 1, learning_rate: 4.520190118492533e-06, optimizer_name: Adam, weight_decay: 0.0003089835017969355, batch_size: 32, label_smoothing: 0.3, factor: 1\n",
      "hidden_size: 1024, num_layers: 3, learning_rate: 3.5092887725468008e-06, optimizer_name: Adam, weight_decay: 4.3742946042079865e-06, batch_size: 16, label_smoothing: 0.3, factor: 1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-05-20 21:30:22,210] Trial 2 failed with parameters: {'hidden_size': 128, 'num_layers': 1, 'dropout': 0.4, 'learning_rate': 4.520190118492533e-06, 'weight_decay': 0.0003089835017969355, 'batch_size': 32} because of the following error: RuntimeError('DataLoader worker (pid(s) 1374) exited unexpectedly').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1251, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 114, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"<ipython-input-9-1476bc4554bc>\", line 82, in objective\n",
      "    for inputs, labels in train_loader:\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1458, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1420, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1264, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: DataLoader worker (pid(s) 1374) exited unexpectedly\n",
      "[W 2025-05-20 21:30:22,221] Trial 2 failed with value None.\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*step is already reported.*\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    \n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-8, 1e-2, log=True)\n",
    "    optimizer_name = \"Adam\"\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [10, 16, 32])\n",
    "    label_smoothing = 0.3\n",
    "    factor = 1\n",
    "\n",
    "    #print all parameters\n",
    "    print(f\"Trial {trial.number}: learning_rate={learning_rate}, optimizer_name={optimizer_name}, weight_decay={weight_decay}, batch_size={batch_size}, label_smoothing={label_smoothing}\")\n",
    "    fold_accuracies = []\n",
    "\n",
    "    for test_fold_idx in range(5):\n",
    "        test_fold = fold_indices[test_fold_idx]\n",
    "        remaining_folds = [fold_indices[i] for i in range(5) if i != test_fold_idx]\n",
    "        val_fold_idx = test_fold_idx % 4\n",
    "        val_fold = remaining_folds[val_fold_idx]\n",
    "        train_folds = [fold for fold in remaining_folds if fold != val_fold]\n",
    "\n",
    "        train_data = np.concatenate([eeg_folds[j] for j in train_folds])\n",
    "        train_data = train_data\n",
    "        train_labels = np.concatenate([labels_folds[j] for j in train_folds])\n",
    "\n",
    "        val_data = eeg_folds[val_fold]\n",
    "        val_data = val_data\n",
    "        val_labels = labels_folds[val_fold]\n",
    "\n",
    "        test_data = eeg_folds[test_fold]\n",
    "        test_data = test_data\n",
    "        test_labels = labels_folds[test_fold]\n",
    "\n",
    "        balanced_train_data, balanced_train_labels = data_balancer(train_data, train_labels, factor=factor)\n",
    "\n",
    "        train_dataset = TensorDataset(torch.tensor(balanced_train_data, dtype=torch.float32),\n",
    "                                      torch.tensor(balanced_train_labels, dtype=torch.long))\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "        val_dataset = TensorDataset(torch.tensor(val_data, dtype=torch.float32),\n",
    "                                    torch.tensor(val_labels, dtype=torch.long))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "        test_dataset = TensorDataset(torch.tensor(test_data, dtype=torch.float32),\n",
    "                                     torch.tensor(test_labels, dtype=torch.long))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = EEGNet(num_classes=num_classes, num_channels=20, num_timepoints=5120).to(device)\n",
    "        criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "        optimizer_cls = {\"Adam\": optim.Adam, \"AdamW\": optim.AdamW, \"SGD\": optim.SGD}\n",
    "        optimizer = optimizer_cls[optimizer_name](model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        epochs = 30\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_labels in val_loader:\n",
    "                    val_inputs, val_labels = val_inputs.to(device, non_blocking=True), val_labels.to(device, non_blocking=True)\n",
    "                    val_outputs = model(val_inputs)\n",
    "                    loss = criterion(val_outputs, val_labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "            early_stopping(val_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "\n",
    "        early_stopping.load_best_model(model)\n",
    "\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        fold_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        fold_accuracies.append(fold_acc)\n",
    "        print(f\"Trial {trial.number}, Fold {test_fold_idx+1}: Test Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    mean_accuracy = np.mean(fold_accuracies)\n",
    "    print(f\"Trial {trial.number}: Mean Accuracy = {mean_accuracy:.4f}, Fold Accuracies = {fold_accuracies}\")\n",
    "\n",
    "    trial.set_user_attr(\"fold_accuracies\", fold_accuracies)\n",
    "    trial.report(mean_accuracy, step=0)  # Single report after all folds\n",
    "\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "\n",
    "    return mean_accuracy\n",
    "\n",
    "# Start Optuna Study\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=300, n_jobs=3)\n",
    "\n",
    "# Best result\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1wsVLiiebraTQaPsj9O9sap_UyjGLTS4y",
     "timestamp": 1747774865523
    }
   ]
  },
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
